\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{ifthen}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage[document]{ragged2e}
\usepackage{listings}
\setlist{nosep}
\usepackage{subfig}


% for including images
\graphicspath{ {./images/} }


\pdfinfo{
  /Title (CS3211.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Pei Cheng Yi)
  /Subject (CS3211)
  /Keywords (CS3211, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

\newenvironment{tightcenter}{%
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
}{%
  \end{center}
}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}%
\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}
%  convenient absolute value symbol
\newcommand{\abs}[1]{\vert #1 \vert}
%  convenient floor and ceiling
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
%  convenient modulo
\newcommand{\Mod}[1]{\ \mathrm{mod}\ #1}
%  for logical not operator, iff symbol, convenient "if/then"
\renewcommand{\lnot}{\mathord{\sim}}
\let\then\Rightarrow
\let\Then\Rightarrow
%  vectors
\newcommand{\vv}[1]{\boldsymbol{#1}}
\newcommand{\VV}[1]{\overrightarrow{#1}}
%  column vector
\newcommand{\cvv}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}
\newcommand{\code}[1]{\textcolor{myblue}{\texttt{#1}}}
\newcommand\bggreen{\cellcolor{green!10}}

\makeatother
\definecolor{myblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{myblue}}
% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% this changes all items (enumerate and itemize)
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.4cm}
\setlength{\leftmarginiii}{0.5cm}
\setlist[enumerate,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,3]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{4}


% multicol parameters
% These lengths are set only within the two main columns
\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \fbox{%
        \parbox{0.8\linewidth}{\centering \textcolor{black}{
            {\Large\textbf{CS3211}}
            \\ \normalsize{AY22/23 Sem 2}}
            \\ {\footnotesize \textcolor{myblue}{github.com/SeekSaveServe}}
        }%
    }
\end{center}

% LECTURES

\section{Lectures}
\subsection{L1: Introduction to Concurrency}
\textbf{Concurrency} \\ 
\begin{itemize}
    \item Concurency is pervasive when modern computers have several cores and types of memory
    \item $\ge$ 2 activities making progress at the same time (overlaping time periods)
    \item Involves interleaving of instructions from different activities
\end{itemize}

\textbf{Parallelism} \\
\begin{itemize}
    \item $\ge$ 2 processes executing and making progress \textit{simultaneously} 
    \item Hardware dependent: requires hyperthreading (SMT), or multi-core and hardware threads
\end{itemize}

\textbf{Processes Vs threads} \\
\begin{itemize}
    \item Independent Vs Shared memory (address) space 
    \item Both use independet stack 
    \item Expensive Vs Cheap context switch 
    \item OS facilitated vs Non OS facilitated inter-process/thread Communication
    \item Expensive (copy on write mediates this somewhat) Vs Cheap creation
\end{itemize}

\textbf{Interrupts} \\
\begin{itemize}
    \item Asynchronous (independent to program execution) 
    \item Used by OS to interact with the programme
    \item Triggered by external events (e.g. I/O, timer, hardware failure)
\end{itemize}

\textbf{Exceptions} \\ 
\begin{itemize}
    \item Synchronous (dependent on program execution)
    \item Used by process to interact with the OS
    \item Triggered by process error (e.g. underflow, overflow)
\end{itemize}

\textbf{User thread} \\ 
\begin{itemize}
    \item Library created, linked to one kernel thread
\end{itemize}

\textbf{Race condition} \\
\begin{itemize}
    \item Outcome depends on reltive ordering of operations on $ge$ 2 Threads
    \item a flaw that occurs when the timing or ordering of events affects a program's correctness
\end{itemize}

\textbf{Data Race} \\ 
\begin{enumerate}
    \item $\ge$ 2 concurrent threads concurrently access a shared resource without Synchronisation / fixed ordering
    \item At least one modifies shared resource
    \item Causes undefined behaviour
\end{enumerate}

\textbf{Mutex} \\ 
\begin{itemize}
    \item Creates critical section can be treated as a large atomic blocks
    \item Only one thread at a a time
    \item Supported by a hardware instruction (CAS, test and set etc)
    \item \textbf{Properties:} Mutex, progres, bounded wait, performance 
    \item Provides \textbf{serialisation} (less concurrency)
\end{itemize}

\textbf{Critical section} \\ 
\begin{itemize}
    \item Safety: nothing bad happens
    \item Liveness: Something good (progress) happens 
    \item Performance: depends on aggregate performance of all threads
\end{itemize}

\textbf{Locks} \\ 
\begin{itemize}
    \item Primitive that is provided b the hardware, minimal semantic 
    \item E.g. Test and set 
\end{itemize}

\textbf{Deadlock iff} \\ 
\begin{enumerate}
    \item Mutex: One resource held in a non-shareable state
    \item Hold and wait: One proces holding one resource and waiting for another resource 
    \item No-preemption: Resource and critical section cannot be aborted externally 
    \item Circular wait
    \item Note: Lock free can deadlock
\end{enumerate}

\textbf{Dealing with deadlock} \\
\begin{itemize}
    \item Prevention: Eliminate one of the above conditions (E.g. hold all locks at the start)
    \item Detection and recovery: Look for cycles in dependencies (E.g. wait for graph)
    \item Avoidance: Control allocation of resources
\end{itemize}

\textbf{Starvation} \\ 
\begin{itemize}
    \item One process cannot progress becausee another process is holding on a resource it needs
    \item Side effect of scheduling algorithm 
    \item Wait-die and wound-wait are possible solutions, if priority of processes is preserved
\end{itemize}

\textbf{Advantages of concurrency} \\ 
\begin{itemize}
    \item Performance
    \item Separation of concerns
\end{itemize}

\textbf{Disadvanageous of concurrency} \\ 
\begin{itemize}
    \item Maintenance and debugging
\end{itemize}

\textbf{Task parallelism} \\ 
\begin{enumerate}
    \item Do the \textbf{same type of} work faster
    \item Task dependency graph can be parallel
    \item Make tasks specialists: Same type of tasks are assigned to the same thread
    \item Divide a sequence of tasks among threads to solve complexed task 
    \item \textbf{Pipeline:} 1 type of thread for one phase of execution
\end{enumerate}

\textbf{Data parallelism} \\ 
\begin{enumerate}
    \item Do \textbf{more work} in the same amount of time
    \item Divide data to chunks and execute by different threads
    \item Embarasingly parallel tasks
\end{enumerate}

\textbf{Challenges of concurrency} \\
\begin{enumerate}
    \item Finding enough parallelism: Amadahl's law
    \item Granularity of tasks
    \item Locality
    \item Coordination and Synchronisation
    \item debugging
    \item Performance and monitroring
\end{enumerate}

\subsection*{L2: Tasks, threads, synchronisation in modern C++}
\textbf{History of CPP} \\
\begin{itemize}
    \item 1998: No support for multithreading
    \begin{enumerate}
        \item Effects of language model are assumed to be sequential and there are no established memory model
        \item Different libraries used different memory models
        \item Execution threads were not acknowledged
    \end{enumerate}
    \item 2011: C++11
    \begin{enumerate}
        \item Standard threads are implemented
        \item Thread aware memory model.Do not rely on platform specific extensions to guarantee behaviour
        \item Atomic operations library, class to manage threads, protected shared data etc. 
    \end{enumerate}
\end{itemize}

\textbf{Four ways to manage threads} \\ 
\begin{enumerate}
    \item Declare a function that returns a thread

    \begin{lstlisting}[language=c++,breaklines=true, breakatwhitespace=true]
void hello() {
    std::cout << "Hello Concurrent World\n";
}
int main() {
    std::thread t(hello);
    t.join();// existing thread waits for t to finish
}
    \end{lstlisting}

    \item Thread with a function object 
    \begin{lstlisting}[language=c++, breaklines=true, breakatwhitespace=true]
class background_task {
public: 
    void operator()() const {
        do_something();
        do_something_else();
    }
};
/* Calleable object */
background_task f;
std::thread my_thread(f);
    \end{lstlisting}
    \begin{itemize}
        \item  $std:: my\_thread(background\_task())$ declares a function that takes a single parameter (type *f() $\rightarrow$ object)
        \item This is not the same as using a function object!
    \end{itemize}
\item Threads with a lambda expression (local fn instead of a calleable object) 
\begin{lstlisting}[language=c++, breaklines=true, breakatwhitespace=true]
std::thread my_thread([]{
    do_something();
    do_something_else();
});
\end{lstlisting}
\end{enumerate}

\textbf{Wait} \\ 
\begin{itemize}
    \item Uses join() on the thread instance exactly once 
    \item Use joinable to check
    \item Local variables do not go out of scope 
    \item Blocking
\end{itemize}

\textbf{Detach()} \\ 
\begin{itemize}
    \item Local variable passed might go out of scope and 'disappear' during runtime, causing invalid access for the detached thread 
    \item Example 
    \begin{lstlisting}[language=c++, breaklines=true, breakatwhitespace=true]
void oops(){
    int local_state = 0;
    /* Reference passed might become invalid */
    func my_func(local_state);
    std::thread my_thread(my_func);
    my_thread.detach();
} /*oops ends here and local_state will be destroyed */
    \end{lstlisting}
    \item Not blocking 
\end{itemize}


\textbf{Passing arguments} \\
\begin{enumerate}
    \item by value $std::thread t(f, 3, "hello")$
    \item by reference $std::thread t(f, 3, buffer)$
    \begin{itemize}
        \item Buffer is a charbuffer that only gets converted to str when we call f
        \item Hence it is possible for buffer to go out of scope 
        \item Fix: Use explicit cast $std::thread(f, 3, std::string(buffer))$ 
        \item \textbf{Major issue} with passing by reference is that threads outside of the scope can use it in \textbf{unsafe} ways. E.g. Not using mutex on shared data, deletion etc 
    \end{itemize}

    \item by copy
    \begin{lstlisting}[language=c++, breaklines=true, breakatwhitespace=true]
void update_data_for_widget(widget_id w, widget_data& data);
void oops_again(widget_id w) {
    widget_data data;
    /* a copy of data is passed */
    std::thread t(update_data_for_widget, w, data);
    display_status();
    t.join();
    /* changes made to the copy is not reflected to other threads */
    process_widget_data(data);
}
    \end{lstlisting}
    \begin{itemize}
        \item Fix: use reference $std::thread t(update_data_for_widget, w, std::ref(data))$
    \end{itemize} 
\end{enumerate}



\textbf{Ownership in C++} \\ 
\begin{itemize}
    \item Owner is an object containing a pointer to an object allocated by $new$ for which the owner is responsible for deleting
    \item Every object on free store (heap, dynamic store) must have \textbf{exactly one} owner
\end{itemize}

\textbf{C++ Resource Management} \\ 
\begin{itemize}
    \item For scoped objects, desctructor is implicit at scope exit 
    \item Free store objects (created using $new$) requires explicit delete
\end{itemize}


\textbf{RAII} \\ 
\begin{itemize}
    \item Binds the lifetime of a resource that must be acquired before use to the lifetime of an object
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
/* Handle interrupts using RAII */
void enqueue(Job job) {
    std::unique_lock lock{mut}; // constructor locks mutex
    jobs.push(job);// destructor unlocks mutex
}
\end{lstlisting}

\textbf{Lifetime} \\
\begin{itemize}
    \item Lifetime begins when storage is obtained and its initialization is complete (except std::allocator::allocate)   
    \item Lifetime ends when :
    \begin{itemize}
        \item Non-class type (int): destroyed
        \item Class type: When destructor is called 
        \item Reference: begins with initialisation and ends when destroyed. A dangling reference is possible.
    \end{itemize}
\end{itemize}

\textbf{Ownership of thread} \\ 
\begin{itemize}
    \item Moveable but not copyable
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
void some_function();
void some_other_function();
std::thread t1(some_function);
/* t1 no longer references the thread */
std::thread t2 = std::move(t1); 
/* t1 now owns a new thread */
t1 = std::thread(some_other_function);
std::thread t3;
/* t3 owns the thread running some function */
t3 = std::move(t2);
/* t1 already owns a thread, this will trigger a runtime error */
t1 = std::move(t3); 
    \end{lstlisting}
    \begin{itemize}
        \item C++ compiler cannot catch this
    \end{itemize}
    \item Ownerhship can be moved out of a function and moved into another function
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]

/* Transferrring out of a function */
std::thread g() {
    void some_function();
    std::thread t(some_function);
    return t; // ownership transferred out of g()
}

/* Transferring into a function */
void f(std::thread t);
void g() {
    void some_other_function();
    std::thread t(some_other_function);
    f(std::move(t)); // ownership transferred into f()
}
    \end{lstlisting}
\end{itemize}


\textbf{Mutex in C++} \\
\begin{itemize}
    \item $std::lock_guard$ locks the mutex upon initialisation, unlocks upon destruction
    \item $std::lock_guard<std::mutex>{some_mutex};$ 
    \item Group mutex and protected data together in a class rather than use global variables
    \item Never pass data or pointers (via returns, sotring in externally visible memory, as input to functions etc) when their usage is not guaranteed to be safe
\end{itemize}


\textbf{Types of lock guards}
\begin{itemize}
    \item \textbf{lock guard} no manual lock, can lock many or one mutex at once without deadlock
    \item \textbf{Scoped lock} accepts and locks a list of mutexes. Can be unintentionally initialized without a mutex
    \item \textbf{unique lock} manual unlock, defers locking using $std::defer_lock$, only single mutex. 
\end{itemize}

\textbf{Condition Variable} \\
\begin{itemize}
    \item Use condition variables to wait for a an event to be triggered by another thread
    \item Avoids busy waiting 
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
std::condition_variable.wait(lock, []{return predicate});
    \end{lstlisting}
    \item if condition is satisfied, returns
    \item unlocks the mutex and places the thread in block state if condition is not satisfied
    \item $std::condition_variable.nofiy_one();$ to notify one thread waiting on the cond
\end{itemize}


\textbf{Spurious Wake} \\ 
\begin{itemize}
    \item Thread wakes up from waiting, but is blocked again as the resource required is not available
    \item Leads to unnecessary context switching
    \item Use conditionals to prevent spurious wake
\end{itemize}

\end{itemize}

\textbf{Shared DS - Invariants} \\ 
\begin{itemize}
    \item Often broken during an update 
    \item \textbf{Case study: Doubly LL}
\includegraphics*[width=7cm]{dll.png}
    \item  invariant is temporarily broken during an update, and we need to prevent objects from accessing the DS during this time
\end{itemize}

\subsection{L3: Atomics and Memory Model in C++} 

\textbf{Reordering of operations} \\ 
\begin{itemize}
    \item Compiler may reorder (potentially conflicting) actions for performance 
    \item Not visible to programmers
\end{itemize}

\textbf{As-if rule} \\
\begin{itemize}
    \item Reordering is allowed as long as:
    \begin{enumerate}
        \item At termination, data written to files is exactly as if the program was executed as written (same final state)
        \item Prompting text that is sent to interactive devices will be shown before the program waits for input
        \item Programs with undefined behaviour is exempted from these rules.
    \end{enumerate}
\end{itemize}

\textbf{Multi-threading aware memory model} \\ 
\begin{itemize}
    \item Using synchronisation constructs (mutexes, barriers etc) should preclude the need for a memory model since they serialise threads 
    \item Memory model gives us more flexibility and speed by getting us closer to the machine
\end{itemize}

\textbf{Stucture of memory model} \\
\begin{itemize}
    \item Every object has a memory location, some occupy exactly one, some occupy many 
    \item The changes in memory location / what is stored there affects other threads
\end{itemize}

\textbf{Modification Order} \\ 
\begin{itemize}
    \item Compose of all writes to an object from all threads in the program
    \item MO varies between runs, each object has their own MO
    \item The programmer is responsible that threads agree on the MO (if not, race condition happens)  
\end{itemize}

\textbf{MO - Requirements} \\ 
\begin{itemize}
    \item The MO of each object is monotonic within a thread 
    \item But the relative ordering of MO of different objects is not guaranteed
\end{itemize}

\textbf{MO - Building Blocks} \\
\begin{itemize}
    \item \textbf{Sequenced-before (SB)}
    \begin{itemize}
        \item Each lines of code in a thread is sequenced before the next line
        \item There is \textbf{NO} sequenced before in a statement with many function calls 
    \end{itemize}
    \item \textbf{Synchronises-with (SW)}
    \begin{itemize}
        \item Established by a $load$ from $T_i$ reading $T_j$'s $store$ 
        \item Both $T_i$ and $T_j$ are synced with respect to the common value in the MO
    \end{itemize}
    \item \textbf{Happens-before (HB)} When an operation happens before another operation due to SW or SB
    \item \textbf{Interthread Happens Before (IHB)}
    \begin{itemize}
        \item When a $store$ in $T_i$ established a sequenced before a $load$ in $T_j$, $sotre_i$ happens before $load_j$ 
        \item IHB $\subseteq$ HB
    \end{itemize}
    \item \textbf{Visible Side effects} 
    \begin{itemize}
        \item Side effect of write A on O is visible to a read B on O if:
        \begin{enumerate}
            \item A HB B
            \item There is no other side effects to O that happens between A and B
        \end{enumerate}
        \item If the side effect of A is visible to B then the longest contiguous subset of the side-effects to O (that B does not HB) is known as the visible sequence of side effects 
        \item Do not think of ordering, think in terms of side effects that are visible
    \end{itemize}
    \item \textbf{Modification Order}
\end{itemize}

\textbf{MO - Seq Const} \\
\begin{itemize}
    \item The default
    \item All threads must see the same ordering of operation
    \item Synchronises with a sequentially consistent load of the same variable that reads the value loaded 
    \item Does not apply to atomic operations with relaxed ordering
    \item Performance penalty when working with weakly ordered machine instructions (common) 
    \item Essentially a serailised monoversion - global total order enforced    
    \item Only guaranteed for data-race free programs (which is difficult since C++ is not as safe as Rust)
\end{itemize}

\textbf{MO - Relaxed} \\ 
\begin{itemize}
    \item Atomic operations don't conform with SW relationships
    \item Happens before still applies within the thread $\rightarrow$ monotonicity and SB within the thread is preserved
    \item No HB between load and store, different store operations from T1 can be viewed out of order by reads in T2
    \item T1: $x=1, y=0$. T2 can see y=0 without seeing x=1 since there is no SW between the two threads even though x=1 HB y=1 in T1.  
\end{itemize}


\textbf{MO - Acquire Release} \\
\begin{itemize}
    \item No total modification order, but there is a partial order
    \item Read - acquire updates about the memory order, load - release updates about the memory order
    \item A link between acquire and release acts like a barrier 
\end{itemize}


\textbf{MO - Mixing Models} \\
\begin{itemize}
    \item Seq const and Release Acquire: load and store of seq const behaves similar to release acquite 
    \item any MO and relaxed: Relaxed behaves like relaxed but is bounded by the other more limiting MO 
    \begin{lstlisting}[language=C++]
// T1
x.store(true, std::memory_order_relaxed);
y.store(true, std::memory_order_release);
// T2
while (!y.load(std::memory_order_acquire) );
/* Never fires because acquire and release */
/* x.store HB y.store  & y.store SW y.load */
assert(x.load(std::memory_order_relaxed)); 
    \end{lstlisting}
\end{itemize}

\textbf{Atomic Operations} \\ 
\begin{itemize}
    \item Compiler ensures necesary synchronisation is in place and enforces MO 
    \item Atomic ops are indivisible
    \item Atomic load loads either the initial value or the value stored by one of the modifications (cannot be half-done)
    \item Can be lock free or be implemented using mutex (which wipes off performance gains)
    \item Not necessarily race free
\end{itemize}


\subsection*{L4: Testing and debugging in C++} 
\textbf{Concurrency related bugs}\\
\begin{itemize}
    \item Unwanted blocking: Deadlock, livelock, blocking while waiting on I/O
    \item Race conditions:  
    \begin{enumerate}
        \item Data races: Undefined behaviour due to unsynchronised access to a shared memory locaiton. Obseravable.
        \item Broken Invariants:
        \begin{itemize}
            \item Dangling pointers: another thread deleted the data being accessed
            \item Random memory corruption: Inconsitent values being read due to partial updates
            \item Double free: Two threads pop the same value from a queue / deleting the same memory address twice - causes possible memory leak
            \item Use After Free - Reading or writing after a memory has been freed
            \item Uinitialised variables - Reading from a variable that has not been initialised
        \end{itemize}
        \item Lifetime issues:
        \begin{itemize}
            \item Thread outlives data
            \item Call to join skipped doe to an exception thrown
        \end{itemize}
    \end{enumerate}
\end{itemize}

\textbf{Techniques to locate concurrency bugs} \\ 
\begin{itemize}
    \item Look at the code 
    \item Testing: Difficult to reproduce, Tests do not always fail (Heisenbug)
\end{itemize}

\textbf{Guidelines for testing} \\ 
\begin{itemize}
    \item Run the smallest amount of code that could potentially demonstrate the bug to locate faulty code 
    \item Do single threaded tests to verify the bug is concurrency related 
    \item Run on sinlge core system to identify issues with interleavings
\end{itemize}

\textbf{Test environment} \\
\begin{itemize}
    \item Number of threads: More threads increases the chance of deadlock (at least 2), contention (blocking while contending for shared resources, degrades performance), overhead
    \item Architecture 
    \item Number of cores 
    \item Having memory fences and barriers to sync threads
\end{itemize}


\textbf{Designs for testability}\\
\begin{itemize}
    \item Responsiblity for each function and thread should be clear 
    \item Check that library calls are thread safe (e.g. do they use internal states to ensure correctness?)
\end{itemize}

\textbf{Techniques for testing} \\
\begin{enumerate}
    \item Stress testing
    \item Use special implementation of synchronisation primitives (e.g. log when mutexes are locked, unlocked)
    \item \textbf{Scalability} does the speedup scale when the number of threads increase? Contention to 
\end{enumerate}

\textbf{Debugging Tools} \\
\begin{itemize}
    \item Identify bugs: 
    \begin{enumerate}
        \item Valgrind (dynamic instrumentation)
        \begin{itemize}
            \item Shadow memory: track and store information on the memory that is used by a program during its execution. A bits and V bits must match for valid.
            \item 20x slowdown
            \item Provides more details than sanitizers despite its performance overhead
        \end{itemize}
        \includegraphics*[width=6cm,height=2.5cm]{valgrind.png}

        \item Helgrind (dynamic instrumentation)
        \begin{itemize}
            \item 100x slow down
            \item intercepts function calls to functions and instruments
            \item Detects: misuses of POSIX threads API, potential deadlocks (checking cyclic lock acquisition), data races (checks existence of HB between memory accesses)
        \end{itemize}

        \item Sanitizers (compilation-based approach)
        \begin{itemize}
            \item 5-10x overhead (lmao just use rust)
            \item $-fsanitize=address$ or $-fsanitize=thread$ or $-fsanitize=memory$ (to catch unitialised memory)
            \item Output is verbose (unlike rust compiler)
        \end{itemize}

        \item Adress Sanitizers (compilation-based approach)
        \begin{itemize}
            \item 2x slowdown, 3x overhead
            \item More efficient shadowing than valgrind 
        \end{itemize}
        \includegraphics*[width=6cm,height=2cm]{asan.png}

        \item Thread Sanitizer 
        \begin{itemize}
            \item 5-10x slow down, 5-15x overhead
            \item Function entry/exit and memory access are logged
            \item An 8-byte shadow cell represents one memory access
            \item Epoch: time of access
            \item Pos: location accessed
            \item If there is an overlap in Pos, we check epoch for evidence of HB. If no HB, then there might be a data race
            \item Use graph-based deadlock detection
        \end{itemize}
        \includegraphics*[width=6cm,height=2.5cm]{tsan_1.png}

    \end{enumerate}

\end{itemize}


\subsection*{L5: Concurrent DS in modern C++}
\textbf{Goal} \\
\begin{itemize}
    \item Multiple threads can access the same DS concurrently
    \item The scope of concurrency (all operations, some operations, one operation) depends
    \item Each thread has a self-consistent view of the DS 
    \item \textbf{Broken invariants} should not be visible E.g. delete() of DLL
    \item Avoid race conditions
    \item Handle exceptions, prevent exceptions from exposing broken invariants. E.g. UAF when a thread aborts after freeing
    \item \textbf{Thread safe}
    \begin{itemize}
        \item No data is lost or corrupted
        \item All invariants are kept 
        \item No problematic race conditions
    \end{itemize}
\end{itemize}

\textbf{Problems with mutex} \\
\begin{itemize}
    \item Prevents true concurrent access to DS - Serialisation
    \item Possible for deadlocks (well, same for lock-free but harder)
\end{itemize}

\textbf{Concurrency while calling functions} \\
\begin{itemize}
    \item Constructors and destructors require exclusive access to the DS - users should not access DS before construction is complete and after desctruction
    \item Swap(), assignment, copy(): Can they be used concurrently with other operations in the DS?
\end{itemize}

\textbf{Design Principles} \\
\begin{itemize}
    \item Smaller the protected region, the better - fewer serialised region
    \item Provide opportunities for concurrency to threads accessing a thread safe DS - what can be called concurrently?
    \item One type of operation can be performing one type of operation exclusively from another type of operation (reader writer) - consider shared mutex
    \item Or allow different types of operations to happen concurrently but disallow the same type to be used by concurrent threads
\end{itemize}

\textbf{Maximising concurrency} \\
\begin{itemize}
    \item Use different mutexes to protect different parts 
    \item Give more concurrency to more frequent operations
\end{itemize}


\textbf{Example: threadsafe stack} \\
\begin{enumerate}
    \item Exception Safety - safe!
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
std::shared_ptr<T> pop() {
    std::lock_guard<std::mutex> lock(m);
    /* This is safe */
    if (data.empty()) throw empty_stack();
    /* Potential out of memory error */
    /* This is fine since the mutex will be unlocked during exception */
    std::shared_ptr<T> const res(
std::make_shared<T>(std::move(data.top()));
    )
    value = std::move(data.top());
    data.pop(); 
    return res;
}
\end{lstlisting}

    \item Work is serialized for the DS - low concurrency
    \item Should use a monitor to allow waiting for an item to be added
\end{enumerate}

\textbf{Example: threadsafe Queue} \\
\begin{enumerate}
    \item Exception Safety: Suppose a thread is woken up by the monitor and that an exception occuring after this point will cause nothing to be popped from the queue.
    \item And other threads waiting on the condition variable is not able to be notified of the non-empty queue
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
void wait_and_pop(T & value) {
    std::unique_lock<std::mutex> lock(m);
    data_cond.wait(lock, [this]{return !data.empty();});
    std::shared_ptr<T> res(
/* Exception here is problematic */
std::make_shared<T>(std::move(data.top()))
    );
    value = std::move(data.top());
    /* Important to pop after moving */
    data.pop();
    return res;
}
    \end{lstlisting}
\item Fix 1 Notify All: Works ... but cause spurious wakes 
\item Fix 2: Put the shared pointer in the queue directly, so we can obtain a shared pointer directly by popping from the queue
\item Shared pointer helps us to handle deallocation

\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
class threadsafe_queue {
    private:
    std::mutex m;
    std::queue<std::shared_ptr<T>> data_queue;
    std::condition_variable data_cond;
    public:
    ... 
void push(T new_value) {
    /* Creation of new data takes place outside mutex */
    /* This is exception safe */
    std::shared_ptr<T> data(
std::make_shared<T>(std::move(new_value))
    );
    std::lock_guard<std::mutex> lock(m);
    data_queue.push(data);
    data_cond.notify_one();
}
}
        \end{lstlisting}
\item Share-pointers are exception safe
\item The creation now takes place outside the mutex - improves performance 
\item However, using the standard container and mutex limits concurrency as the queue is either protected or not 
\item For a more fine-grained locking, we need to write a customised DS
\end{enumerate}

\textbf{Example: threadsafe stack with fine-grained locks} \\
\includegraphics*[width=7cm,height=1.5cm]{finegrained_queue.png}
\begin{itemize}
    \item \textbf{Node}
    \begin{enumerate}
        \item Pointers to nodes should only be modified by one thread, so we use an unique poitner 
        \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
struct node {
    std::shared_ptr<T> data;
    /* Only one copy of next */
    std::unique_ptr<node> next;
    node(T data_):
            data(std::move(data_))
    {}
};
        \end{lstlisting}
    \end{enumerate}
    \item \textbf{Push - attempt 1} \\
    \begin{enumerate}
        \item Modify the of the queue if it is empty
        \item Else we modify the back
        \item Using a lock for checking front and back is problematic as we need to lock both mutexes if the queue is initially empty (front=back, potential for deadlock if two threads tries to push concurrently). This also serialises the queue 
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
void push(T new_value) {
    std::unique_ptr<node> p(new node(std::move(new_value)));
    node* const new_back = p.get();
    std::lock_guard<std::mutex> lk(mut);
    if (back) {
        back->next = std::move(p);
    } else {
        front = std::move(p);
    }
    back = new_back;
}
\end{lstlisting}
    \end{enumerate}

\item \textbf{Push - attempt 2} \\
\begin{enumerate}
    \item Ensure that there's always $\ge$1 node in the queue to separate the node being accessed at the front from the node being accessed at the back 
    \item Empty queue: front and back point at a dummy node 
    \item No race on front.next  and back.next (but with additional layer of indirection) \\
    \includegraphics*[width=6cm,height=3cm]{dummy2.png}
\end{enumerate}

\item With the use of dummy nodes, we can do more fine grained locking since we can now lock the front and back separately without the chance of Deadlock 
\item Pop and push can now occur concurrently\\
\includegraphics*[width=7cm,height=5.5cm]{queue1.png}
\includegraphics*[width=7cm,height=5cm]{queue2.png} \\
\item This can be even more fine grained by having a per-node mutex, which establishes a SW between push and pop threads
\item However this doesn't really make sense (unless we are supporting search etc) as pop and push only accesses the front and back of the queue 
\item We also have to use raw pointers since the mutex is in the Node and we cannot delete the Node before unlocking (otherwise UAF)
\end{itemize}

\textbf{Lock-free concurrent DS} \\
\begin{itemize}
    \item Non-blocking Mutexes: Spinning lock with test and set 
    \item A blocked thread is suspended by the OS
    \item Non-blocking DS: 
    \begin{enumerate}
        \item \textbf{Obstruction-free:} if all threads paused, then any thread will complete in a bounded number of threads
        \item \textbf{Lock-free:} if multiple threads are operating on a DS, then after a bounded number of steps, one of them will complete (note: deadlock  still possible)
        \begin{itemize}
            \item \textbf{Pros}
            \item Enable max concurrency; some thread makes progress with every step
            \item Robust: One thread dying doesn't affect the other threads using the DS, threads will not be excluded from accessing the DS 
            \item Need to ensure that the invariants are upheld, and that changes become visible to other threads in the correct order
            \item \textbf{Cons}
            \item live locks are possible (changes made by one thread requires the others to restart) 
            \item Decrease overall performance even though individual waiting time is reduced
            \item Atomic operations can be expensive
            \item Hardware need to use more expensive operations to synchronise data
        \end{itemize}
        \item \textbf{Wait-free:} every thread will complete in a bounded number of steps, even if other threads are operating on the DS (note: starvation still possible)
        \begin{itemize}
            \item Lock free with loops (CAS) can result in one being subjected to starvation 
            \item E.g. A spins, but B progresses and sets the condition to false again 
        \end{itemize}
        \item wait-free $subseteq$ lock-free $subseteq$ obstruction-free
        \item lock-free is not always wait-free
    \end{enumerate}
\end{itemize}

\textbf{Contention and Cache Ping Pong} \\
\begin{itemize}
    \item Change made by one thread takes time to propagate to the other cores
    \item This contention makes other threads wait for the change to propagate across memory
    \item Accessing data from the cache line with multiple threads can cause false sharing (hard to catch) 
    \item 
\end{itemize}

\subsection*{L6: Concurrency in GO} 

\textbf{Task Concurency Graph} \\
\begin{itemize}
    \item Node: Task, value is the expected execution time 
    \item Edge: Represent control dependency between tasks (v wait-for u)
    \item Critical path length: slowest completion time 
    \item Degree of concurrency: Total work / critical path length 
    \includegraphics*[width=7cm,height=2.5cm]{taskconcurrency.png}\\
\end{itemize}

\textbf{Speed up gained from concurrency} \\
$S_p(n) = \frac{T_{best seq}(n)}{T_{parallel}(n)}$, we aim to gain a $S_p$ of p, where p is the number of processors \\

\textbf{Amadahl's Law} \\
\begin{itemize}
    \item Speed up of parallel execution is limited by the fraction of the algorithm that cannot be parallelised
    \item f($0\le f \le 1)$ is the sequential fraction
    \includegraphics*[width=7cm,height=2.5cm]{amdahl.png}\\
\end{itemize}


\textbf{About GO} \\
\begin{itemize}
    \item Statically typed 
    \item Compiled PL 
    \item More memory safety, garbage collection, CSP style concurrency than C
\end{itemize}

\textbf{Go routeins} \\
\begin{itemize}
    \item Runs on OS threads - any type of threads the OS provides (go routines are tasks, not threads)
    \item A function running independently
    \item Cheaper than threads (a few kbs, 3 instructions to create) + lightweight    
    \item Follow the fork-join model
    \item Not garbage collected - programmers to prevent leaks
    \item Goroutines are multiplexed to OS threads: allows scheduling and goroutines to be multiplexed to threads.
    \item Decouples concurrency from parallelism
    \item Go routine blocks $\rightarrow$ thread blocks. But other goroutines can still run
\end{itemize}

\textbf{Communicating Sequential Processes (CSP)} \\
\begin{itemize}
    \item Concurrency: structure a program by breaking it into pieces that can be executed indepedently
    \item Go Routines:
    \begin{itemize}
        \item A function running independently
    \end{itemize}
    \item Communication: Coordinate the independent executions
    \item Channels:
    \begin{itemize}
        \item Read: <- Channel
        \item Write: channel <-
    \end{itemize}
    \begin{itemize}
        \item 
    \end{itemize}
\end{itemize}

\textbf{Wait groups} \\
\begin{itemize}
    \item A blocking call that waits for n goroutines to finish (n can be incremented)
\begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
var wg sync.WaitGroup
for _, salutations := range []string{"hello", "greetings", "good day"} {
    wg.Add(1)
    go func(salutation string) {
        # called when scope ends
        defer wg.Done() # decrements counter of wg
        # Salutation is passed by value
        fmt.Println(salutation)
    }(salutations)
}
wg.Wait()
\end{lstlisting}
\end{itemize}

\textbf{Shared memory} \\ 
\begin{itemize}
    \item Similar to C++, need to synchronise access to shared memory 
    \item Use channels instead of modifying shared memory
\end{itemize}

\textbf{Channels} \\ 
\begin{itemize}
    \item Channel is a reference to a place in memory where the channel resides 
    \item Typed 
    \item Bi directional or unidirectional
    \item Blocking 
    \begin{itemize}
        \item Writing to full Channels
        \item Reading from empty channel 
        \item can cause deadlock
        \item Reading from closed channel returns default value; nonblocking
    \end{itemize}
    \item Declaration:
    \begin{itemize}
        \item Bi directional 
        \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
// Unbuffered Channels
var c chan int
// Buffered Channels
datastream := make(chan int, 5)    
        \end{lstlisting}
        \item unidirectional
        \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
var writer chan<- int // write only
var reader <-chan int // read only
datastream := make(chan int)
writer = datastream
reader = datastream
        \end{lstlisting}
    \end{itemize}
\end{itemize}

\textbf{Synchronising using channels} \\
\begin{itemize}
    \item Ranging over channel, channel closes when loop ends
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
for i := range intStream {
    // do something
}
    \end{lstlisting}
    \item Close unbuffered channel 
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
begin := make(chan interface{})
var wg sync.WaitGroup 
for i := 0; i < 5; i++ {
    wg.Add(1)
    go func(i int) {
        defer wg.Done()
        <-begin
        fmt.Printf("%v has begun\n", i)
    }(i)
}
fmt.println("Unblocking goroutines...")
// This allows default values to be printed
close(begin)
wg.Wait()
    \end{lstlisting}
\end{itemize}

\textbf{Channel behaviour} \\
\includegraphics*[width=7cm, height =5cm]{gobehaviour.png}


\textbf{Buffered channels} \\
\begin{itemize}
    \item Delays the problem of a blocking (full/empty) channel 
\end{itemize}

\textbf{Guidelines: Ownership of a channel (NOT checked by compiler)} \\
\begin{itemize}
    \item Owner is the goroutine that instantiates, writes and closes 
    \item (unidirectional) Owner writes, utilisers read
    \includegraphics*[width=7cm, height =2cm]{channelownership.png}
    \item \textbf{Increases safety}
    \begin{enumerate}
        \item The owner will not deadlock by writing to a nill channel 
        \item The owner will not panic by closing a closed channel 
        \item The owner will not panic by writing to a closed channel since the owner closes the channel 
        \item The owner will not close the channel more than once because it closes the channel 
        \item Since only owner writes, the type checker can prevent improper writes at compile time 
    \end{enumerate}
\end{itemize}

\textbf{Select} \\
\begin{itemize}
    \item Blocks until a branch becomes available, all branches are considered simultaneously
    \item Use cases:
    \begin{enumerate}
        \item Multiple channels have something to read
        \begin{itemize}
            \item Peudo-randomly selects a channel to read from. Supposedly uniform
            \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
c1 := make(chan interface{}); close(c1) // nonblocking
c2 := make(chan interface{}); close(c2) // nonblocking
var c1Count, c2Count int
select {
    case <-ch1:
        c1Count ++
    case <-ch2:
        c2Count ++
}
            \end{lstlisting}
        \end{itemize}
        \item Channels are not ready
        \begin{itemize}
            \item Timeout channels that are never ready
            \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
var c1 <- chan int 
select {
    case <- c:
    // times out after 1 second
    case <- time.After(1 * time.Second): 
        fmt.Println("Timed out")
}
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
    \item For-select loop 
    \begin{itemize}
        \item Allows a Go routine to make progress while waiting for other go routines 
        \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
done := make(chan interface{})
go func () {
    for {
        select {
            // work is done!
            case <- done:
                return
            default:
        }
        // do non-preemptable work
    }
}() // this () calls the function immediately after defining
        \end{lstlisting}
    \end{itemize}
\end{itemize}

\textbf{Sync package} \\
\begin{itemize}
    \item Do not communicate by sharing memory. Instead, share memory by communicating.
    \includegraphics*[width=7cm, height =4cm]{syncpackage.png}
\end{itemize}

\textbf{Memory Model - GO} \\
\begin{itemize}
    \item Similar to C++, except theres no atomics
    \item Synchronised before:
    \begin{enumerate}
        \item Send is synced before a corresponding receive from another thread 
        \item Closing of a channel is synced before reads from the closed channel (value 0)
        \item Receive from unbuffered channel is synced before the send on that channel completes
        \item The kth receive on a channel with capacity C is synced before the (k+C)th send on the same channel
        \item $Go$ statement that starts a gorountine is synced before the go routine's execution 
        \item The exit of a gopher is \textbf{NOT} guaranteed to be synced before any code after the go statement
    \end{enumerate}
\end{itemize}

\subsection*{L7 Concurrency Patterns in GO}

\textbf{Patterns - Confinement} \\
\begin{itemize}
    \item Achieve safe operation through synchronisation (mutex / channels)
    \item Safe concurrency with good performance - immutable data, data protected by confinement 
    \item \textbf{Ad hoc confinement}
    \begin{itemize}
        \item Data is modified only from one gopher, even though the data is accessible by multiple 
        \item Needs static analysis to ensure safety (Exclusive ownership)
        \item Easily broken 
    \end{itemize}
    \item \textbf{Lexical confinement}
    \begin{itemize}
        \item Restric access to shared location
        \item Achieved by exposing only the reading or writing handle to the producer/consumer 
        \item Expose only a slice of the DS (e.g. array)
    \end{itemize}
\end{itemize}

\textbf{Pattern - For-select loop} \\
\begin{itemize}
    \item Context - sending iteration variables out on a channel
    \item Looping and waiting to be stopped (by context or done channel)
\end{itemize}

\textbf{Goroutine leakage} \\
\begin{itemize}
    \item Goroutines are detached and not garbage collected
    \item Creator of goroutines should also destroy them 
    \item Passing 'nil' as a channel will cause the gopher to run indefinitely
    \item Solutions:
    \begin{itemize}
        \item Pass a done channel, close it in the creator to signal the gopher to stop
    \end{itemize}
\end{itemize}

\textbf{Pattern - Error handling} \\
\begin{itemize}
    \item State goroutine maintains complete state of the program 
    \item All goroutines send their error to a state goroutine that handles this 
    \item Couple the potential error with potential result 
    \item E.g. Http handling 
\begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
type Result struct {
    Error error
    Response *http.Response
}

for _, url := range urls {
    result := Result{Error: nil, Response: nil}
    go func() {
        resp, err := http.Get(url)
        result = Result{Error: err, Response: resp}
        response <- result
    }()
    select {
        case <-done:
            return
        /* Owner of results will handle this */
        case results <- result:
        default:
    }
}    
\end{lstlisting}
\end{itemize}

\textbf{Pattern - Pipeline} \\
\begin{itemize}
    \item A series of stages connected by channels, each stage performs the stage function
    \item Inbound and outbound channels passes data from one channel to the next
    \item The first stage only has an outbound channel
    \item The last stage only has an inbound channel 
    \item Intermediate stages have multiple inbound and outbound channels
    \item Aims to achieve separation of concerns
\end{itemize}

\textbf{Pipleine - Efficiency} \\
\begin{itemize}
    \item Work and resources should be divided among stages s.t they all take the same time to complete their tasks 
    \item Fan-out to decrease the processing time for a stage
    \item Use I/O and multiple CPUs to process different streams of data 
    \item Not obviously faster than task pool unless optimised 
    \item Pipeline is better if there is a cap on a specific resource that is needed by all task pool (at different times)
    \item E.g 
    \includegraphics*[width=6.5cm, height = 2cm]{pipeline.png}
    \begin{itemize}
        \item generator, multiply etc are functions written using the for-select paradigm
    \end{itemize}
\end{itemize}

\textbf{Pattern - Fan-out, Fan-in} \\
\begin{itemize}
    \item Problem: Stages in pipeline might be slower than the other and they might benefit from parallelism 
    \item Fan-out: Start multiple gophers to handle inputs
    \begin{itemize}
        \item The stage should not rely on values that has been calculated before in preceding stages 
        \item The order of the concurent copies does not matter (is not maintained)
        \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
numFinders = runtime.NumCPU() // Max level of concurrency
// Channel of writer channels
finders := make([] <- chan int, numFinders)
for i := 0; i < numFinders; i++ {
    go func() {
        for i := range numFinders {
            // Fan-out
            finders[i] = primeFinder(done, inputStream)
        }
    }
}
        \end{lstlisting}
    \end{itemize}
    \item Fan-in: Combine multiple results into one output channel
    \begin{itemize}
        \item Multiplexing/joining multiple streams of data into a single data
        \item Order will not be maintained 
    \end{itemize}
\end{itemize}

\textbf{Load balancer} \\ 
\includegraphics*[width=4cm, height=2cm]{lb1.png}\\
\begin{itemize}
    \item Balance load among all workers based on the amount of request in the queue of each worker 
    \item The sending back of results is not done through the load balancer; a separate channel
    \item Requst 
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
type Request struct {
    fn funct() int // The operation to perform 
    result chan int // The channel to return the result
}
    \end{lstlisting}
    \item Requester 
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
func requester(work chan<- Request) {
    c := make(chan int) // read result from c
    for { // while loop 
    /* For the balancer to proess */
        case work <- Request{workFn, c}:
        result := <-c // wait for worker
        furtherProcess(result)
    }
}
    \end{lstlisting}
    \item Worker 
    \begin{itemize}
        \item 
    \end{itemize}
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
/* Receiver of the method, basically this is a fn of the worker struct */
func (w *worker) work(done chan *Worker) {
    for {
        req := <-w.requests
        req.c <- req.fn() // call fn and send result
        done <- w // we've finished this request
    }
}
    \end{lstlisting}
    \item Balancer - DS
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
type Pool []*worker // track the workers

type Balancer struct {
    pool Pool
    done chan *worker // shared among all workers
}
    \end{lstlisting}
    \item Dispatch - a min heap of channels 
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
func (p Pool) Less(i, j int) bool {
    return len(p[i].requests) < len(p[j].requests)
}

// Send request to worker 
func (b *Balancer) dispatch(req Request) {
    // Grab the least loaded worker
    w := heap.Pop(&b.pool).(*worker)
    // Send request to worker
    w.requests <- req
    // Put worker back into the pool
    heap.Push(&b.pool, w)
}
    \end{lstlisting}
    \item Completes
    \begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
func (b *Balancer) completed(w *worker) {
    w.pending -- 
    // Remove worker from pool
    heap.Remove(&b.pool, w.index)
    // Put it into heap 
    heap.Push(&b.pool, w)
}
    \end{lstlisting}
\item Balance (can be pipelined, see T6 3.2)
\begin{lstlisting}[language=Go, breaklines=true, breakatwhitespace=true]
func (b *Balancer) balance(work chan Request) {
    for {
        select {
            case req := <-work:
                b.dispatch(req)
            case w := <-b.done:
                b.completed(w)
        }
    }
}
\end{lstlisting}
\end{itemize}

\textbf{Concurrent Balancer} \\ 
\begin{itemize}
    \item Use pipelining to make the balancer more concurrent 
    \item Some requests can be starting at the dispatch stage 
    \item Othe requests can be waiting at the completed stage 
    \item Use a for-select pattern for this
\end{itemize}

\textbf{Work Stealing} \\
\begin{itemize}
    \item Steal work from the queue of the slowest (longest queue) worker 
    \item We steal from the back of the queue (in case there is dependence on the requests)
    \item 
\end{itemize}

\textbf{GO Runtime - work stealing } \\ 
\begin{itemize}
    \item (Fork Point) Add tasks to the tail of a deque of a thread 
    \item If the thread is idle, steal the last job added to the deque associated with some other random thread 
    \item (Join Point) At a join point that cannot be realised yet (waiting for some forked thread to complete), pop work off the tail of the thread's own deque (and process that)
    \item If the deque is empty, steal work from head of a random thread's deque
    \item Work stealing does stall the join, sometimes GO does continuation stealing instead
\end{itemize}

\subsection*{L8: Classical Synchronisation Problem in C++ and GO}
\includegraphics*[keyvals]{imagefile}

\pagebreak
% TUTORIALS
\section{Tutorials}
\subsection*{T1: Threads and Synchronisation}
\textbf{Why mutexes work - standard argument}\\ 
\begin{itemize}
    \item Define a critical section that contains all acesses to the shared resource 
    \item Argue that mutex guarantees mutual exclusivity of threads - removes interleaving, data race precluded
\end{itemize}

\textbf{Why mutexes work - theoretical argument}\\
\begin{itemize}
    \item Lock and unlock appears in a single total order
    \item Only one thread owns the lock at any pointer
    \item Unlock happens after lock, creating a synchronises with relationship between processes and \textbf{serialises the interleaving} - no concurrent access
\end{itemize}


\textbf{Monitor} \\
\begin{itemize}
    \item Allows us to block until a condition becomes true
    \item The monitor has: 
    \begin{enumerate}
        \item A mutex on the critical section
        \item A condition variable
        \item A condition to wait for 
    \end{enumerate}
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
std::condition_variable cond;

Job dequeue() {
    std::unique_lock lock{mut};
    /* wait until there is a job */
    cond.wait(lock, [this]() { return !jobs.empty(); });
    Job job = jobs.front();
    jobs.pop();
    return job;
    }

void enqueue(Job job) {
    {
        std::unique_lock lock{mut};
        jobs.push(job);
    }
    /* notify one thread waiting on the condition variable */
    cond.notify_one();
    }
    \end{lstlisting}
\end{itemize}


\subsection*{T2: Atomics in C++}

\textbf{Data races and undefined behaviour} \\
\begin{itemize}
    \item For undefined behaviour, the C++ compiler \textbf{will allow it to be compiled}
    \item Compilers are free to do any reordering and the answer cannot be predicted. 
    \item E.g. GCC and Clang will return different runtime outputs as they compile it differently
    \item See snipet 1
\end{itemize}

\textbf{Mutexes Vs Atomics} \\
\begin{itemize}
    \item Consider the following implementation of a counter
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
int counter = 0
std::mutex m;
void t1 {
    for (int i = 0; i < 1000000; i++) {
        std::lock_guard lock{mut};
        counter++;
    }
}

std::atomic<int> atomic_counter = 0
void t2 {
    for (int i = 0; i < 1000000; i++) {
        atomic_counter.fetch_add(1, std::memory_order_seq_cst);
    }
}
    \end{lstlisting}
    \item The atomic version is much faser than the mutex version
    \item Mutex calls lock and unlock multiple times around the add operation, which compiles to more instructions and may cause the threads to sleep / fight for access
    \item The atomic version is a single instruction $lock$ $addl$ $\$1$, which is much faster
\end{itemize}

\textbf{Memory order and performance} \\ 
\begin{itemize}
    \item Seq cst $\ge$ Acquire-Release $\ge$ Relaxed
    \item Seq cst's store uses $xchg$ (requires the processor to have exclusive access to shared mem) while the other orders allows x86 to use a simple $mov$ instruction
\end{itemize}

\textbf{Forcing ordering with $std::atomic<int>$} \\
\begin{itemize}
    \item Compiler reorders instructions to preserve visible side effects and optimise 
    \item We can use atomics to force the compiler to preserve ordering
    \item But this also forces the compiler to use the more expensive $xchg$ instruction, since atomics use seq cst by default
\end{itemize}

\textbf{Memory order - atomics} \\
\begin{itemize}
    \item Note: data races \textbf{is not possible} when atomics are used. If load and store are not atomic, then the output can theoretically be anything (segfault, garbage value etc)
    \includegraphics*[width=7cm,height=2.2cm]{t2_1.png}
    \item The first cout can print 1,3,5. The secon cout can print 3,5. They will never print 0 due to the acquire release.
\end{itemize}

\textbf{Fences} \\
\begin{itemize}
    \item Enforces memory order without modifying the data 
    \item Can be used with Relaxed MO to enforce ordering while preserving concurrency 
    \item \textbf{Memory Barrier} puts a line that certain operations cannot pass 
    \item \textbf{Atomic fence} with acquire release MO prevents all preceding read and writes from moving past all subsequent stores
\end{itemize}

\subsection*{T3: Debugging Concurrent C++ Programs}

\textbf{Protecting shared resource with unique$\_$ptr} \\
\begin{itemize}
    \item std::unique$\_$ptr is a smart pointer that owns and manages another object through a pointer and disposes of that object when the unique$\_$ptr goes out of scope.
    \item Destroys object with the provided $deleter$
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
{ 
std::unique_ptr<int> foo = std::make_unique<int>(5);
/* custom deleter for File */
auto deleter = [](FILE* f) { fclose(f); };
auto bar = std::unique_ptr<FILE, decltype(deleter)>(fopen("file.txt", "w"), deleter);
}
/* Foo and bar are destroyed here */
    \end{lstlisting}
\end{itemize}

\textbf{Protecting against shared lifetimes} \\
\begin{itemize}
    \item std::shared$\_$ptr is a smart pointer that retains shared ownership of an object through a pointer.
    \item When shared ptr is copied, we increase the count by 1. When it is destroyed, we decrease the count by 1. When count is 0, we delete the object
    \item While deleting shared ptr is thread safe (since it just deceremnts the counter), the object wrapped by shared ptr may not be (data races can still happen)
\end{itemize}

\textbf{Possible bugs with shared$\_$ptr} \\
\begin{itemize}
    \item Unsynchronised access to managed object - WR, WW can still happen if exclusive lock is not used 
    \item Data race on ptr (UAF): Overwrites to ptr deletes the old value and make a new one. But the reader may still be using the old ptr. Do not pass shared ptr by reference. 
    \item Circular reference: A points to B, B points to A. Both will never be deleted since the reference count will be at least 1. E.g. A.prev = B, B.next = A. 
    \begin{itemize}
        \item Using DLLNode* prev and DLLNode next. Only the next pointer copies the shared pointer, so there is no cycle.
    \end{itemize}
\end{itemize}

\textbf{Implementing shared ptr} \\
\begin{itemize}
    \item The semantics of a shared ptr is that it manages when to destroy a shared resource.
    \item This means that we need to keep a shared count, that is stored as a reference
    \item Since this is shared, we need to use a mutex (see 3.4) or atomic operations (see 3.5) to protect it
    \item During deletion (count == 0), we cannot delete the mutex in the critical section since the std::unique lock will call unlock when exiting the CS
    \item So we store the decremented value of counter in a temp var and check $temp==0$ outside of the critical section
    \item This is correct since $temp==0$ implies no thread can modify the counter so no data race
    \item Note: Use an atomic int for the atomic implementation
\end{itemize}

\textbf{Extra overhead of shared pointer} \\
\begin{itemize}
    \item Ambiguous ownership: Most resources should have a single owner using std::unique ptr and use a singleton pattern
    \item Memory leaks: Cyclic references, as shown above, can cause memory leaks. Std::unique ptr does not face this. 
    \item Performance overhead: Maintaining reference counts result in unnecessary synchronisation. Initialising memory on the heap instead of the stack (since shared ptr is shared) may slow down the program and reduce cache locality
\end{itemize}

\subsection*{T4 - lock free programming} 
This tutorial implements the threadsafe queue in L5 using atomics.\\

\textbf{Lock free} \\
\begin{verbatim}
    If multiple threads are operating on a DS, 
    then after a bounded number of steps one 
    of them must complete its operation
\end{verbatim}

\textbf{Problems faced by producers} \\
\begin{itemize}
    \item  Producers may data race by overwritting each other's job
    \begin{enumerate}
        \item Use queue$\_$back.exchange(new dummy), which returns the current work node to the producer to edit while setting the back to be a new dummy node (contains null)
    \end{enumerate}
    \item A producer may not be synchronised with consumers, causing consumers to read an invalid state
    \begin{enumerate}
        \item Use acquire release to update the $next$ pointers 
    \end{enumerate}
\end{itemize}

\textbf{Problems faced by consumers} \\
\begin{enumerate}
    \item Consumer may race wuth each other (by consuming a node twice, consuming another consumer's node, deleting the node (UAF))
    \begin{itemize}
        \item Doing queue$\_$front.exchange(queue$\_$front.next) is not good enough as ew only want to exchange if the current node does not contain null 
        \item A \textbf{CAS} is needed
    \end{itemize}
    \item Consumers may not be synced with producers 
    \begin{itemize}
        \item Use acquire release to update the $next$ pointers
    \end{itemize}
\end{enumerate}

\textbf{CAS pattern} \\
\begin{itemize}
    \item Performs a comparison on the memory location, checking if the current value is the same as the expected value. Exchange if true
    \item If the side effect is not monotonic, we might face \textbf{ABA} problem
    \item \textbf{Weak CAS} is conducted in a sequence of instructions. It is possible for it to fail spuriously. It is cheaper and should be used if CAS is used in a loop and each loop is cheap.
    \item \textbf{Strong CAS} is conducted in a single $cmpxchng$ instruction. It is more expensive but will not fail spuriously.
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
/* Ensures that the value read gets more updated with each loop */
Node* old_front_next = old_front.node -> next.load(stdmo::acquire);
... 
/* Can be relaxed because of the previous acuire */
m_queue_front.compare_exchange_weak(old_front, old_front_next, std::memory_order_relaxed)
    \end{lstlisting}
\end{itemize}

\textbf{ABA problem} \\
\begin{itemize}
    \item The next pointer in a Node is obviously non-monotonic (and so are the values stored in the nodes)
    \item It is possible for an old front to be consumed, and then re-inserted with the same value (but not necessarily at the front). A thread that read the old front before it was consumed will not be able to detect this, and CAS succeeds \\
    \includegraphics*[width=7cm, height = 2.5cm]{aba1.png}

    \item But the new old front may not contain the same next pointer as the value read by the thread from the old old front.\\
    \includegraphics*[width=7cm, height =2cm]{aba2.png}
\end{itemize}

\textbf{Solving ABA with generation-counted pointers} \\ 
\begin{itemize}
    \item Include a generation counter alongside the data we are performing CAS on  to differentiate between versions
    \begin{lstlisting}[language=c++,breaklines=true, breakatwhitespace=true]
struct alignas(16) GenNodePtr
{
    Node* node;
    uintptr_t gen;
};

static_assert(
std::atomic<GenNodePtr>::is_always_lock_free
);

alignas(64) std::atomic<Node*> m_queue_back;       // producer end
alignas(64) std::atomic<GenNodePtr> m_queue_front; // consumer end
    \end{lstlisting}
    \item When we initialise the new front, we set its gen to be old front.gen + 1.
    \item (Of course, overflow is possible so it is not fool proof)
\end{itemize}

\textbf{UAF} \\ 
\begin{itemize}
    \item gen ptr ensures that CAS fails when other consumers have changed the queue, but it is possible to dereference a pointer froma possible stale value of m$\_$queue$\_$front
    \item Then, that node may be allocated again on the same address. And the constructor of std::atomic will raise with the load()
    \item \textbf{Solutions:} 
    \begin{enumerate}
        \item Never free anything (let the memory leak )
        \item Mark nodes for deletion, and delay the deletion when the entire queue is free (no other threads acccessing queue)
        \item Use reference counting (atomic::shared ptr) to know when there are no more remaining references. This is not lock free.
        \item Hazard pointers to track which threads have references to which objects
    \end{enumerate}
\end{itemize}

\textbf{Recycling center} \\
\begin{itemize}
    \item Suppose we go with solution 2, we can set up a recycling center that maintains a concurrent stack of free nodes (see 4.1 for implementation details) so we can reuse deleted nodes.
    \item Push will obtain the node from the recycling center if it is not empty, or allocate a new node if it is empty
    \item Since nothing is freed, we cannot get UAF
\end{itemize}

\textbf{Data race in recycling stack} \\
\includegraphics*[width=7cm,height=2.5cm]{recycling1.png}

\begin{itemize}
    \item The race happens because consumer tries to read from the work$\_$node before the producer has written to it
    \item This is possible since htere is no synchronisation between T1 and T3 
    \item To fix this, we can make the load store of node.next acquire release such that T1 and T2 (by transitive T3 as well) are in sync
\end{itemize}
\includegraphics*[width=7cm,height=2.5cm]{recycling2.png} \\

\textbf{Internal Data Race} \\
\includegraphics*[width=7cm,height=2.5cm]{recycling3.png}
\begin{itemize}
    \item By allowing consumers to overtake producers threads (use yield), we can have a seemingly-impossible data race
    \item The fix is to slap acquire release on the load store of m$\_$queue$\_$front
\end{itemize}
\includegraphics*[width=7cm,height=2.5cm]{recycling4.png}\\

\textbf{Benchmarking} \\
\begin{itemize}
    \item Some common metrices are: CPU cycles, real clock time, MFLOPs (floating point operations per second) 
    \item There are 4 possible set ups to test for 
    \begin{enumerate}
        \item Single Producer, Single Consumer (SPSC)
        \item Single Producer, Multiple Consumers (SPMC)
        \item Multiple Producers, Single Consumer (MPSC)
        \item Multiple Producers, Multiple Consumers (MPMC)
    \end{enumerate}
    \item Use barier to ensure that all producers and consumer threads arrive at the the start of the benchmark before starting the timer and commencing the execution
\end{itemize}

\subsection*{T5: Goroutines and channels}

\textbf{Silly mistakes} \\
\begin{itemize}
    \item Not using wait group or waiting for channels to finish and "join"
    \item Allowing data race by not granting exclusive access by passing objects using channels s.t only 1 owner exists at any point in time
    \item Creating a deadlock using cyclic dependencies between channels
    \item Using buffered channel to resolve deadlock - bad idea since there are now multiple owners to values and a buffered channel becomes effectively unbuffered when it is full + more interleavings possible
    \item Closing the channel too early and resulting in a panic
    \item Not closing the channel and never stops reading 
\end{itemize}

\textbf{MPSC Queue using Channels} \\
\begin{itemize}
    \item For associative and identity operations, we can simply have the producers update a local copy from 0 and send it back to the consumer once done 
    \item This can be done either by having a channel of channels with one channel for each producer (1.2), or by having a single channel and using a for loop to read exactly N times from the channel (1.1).
\end{itemize}

\textbf{MPMC Queue using Channels} \\

\begin{itemize}
    \item The default MPMC GO provides contains mutex so it is not lock free
    \item We can implement a non-blocking, lock-free MPMC queue using the select-default idiom (See 2.2) (Note: default is a nexist for when all cases are blocked) 
\end{itemize}

\textbf{context.Context} \\
\begin{itemize}
    \item To canccel an Asynchronous task, we can use context.Context

    \begin{lstlisting}[language=Go,  breaklines=true, breakatwhitespace=true] 
func producer(ctx context.Context, q queue) {
    for {
        select {
        case q <- 1: // normal enqueue to q with type alias to chan int
        case <-ctx.Done():
            return
        }
    }
}
    \end{lstlisting}
    \item Context can be set up with a timeout ctx := context.WithTimeout(context.Background(), 1*time.Second)
\end{itemize}

\subsection*{T6 Advanced GO Concurrency Patterns}
\textbf{Exit Conditions} \\
\begin{itemize}
    \item Manage different exit conditions for different goroutines and share data across different goroutines
    \item Use a context tree to combine multiple exit conditions into One
    \includegraphics*[width = 7cm, height = 2cm]{contextTree.png}
    \item Sample: 
\begin{lstlisting}[language=Go,  breaklines=true, breakatwhitespace=true]
func main() {
    ctx, cancel := context.WithCancel(context.Background())
    ctx3, cancel := context.WithCancel(ctx)
    defer cancel()

    go func() {
        <-ctx3.Done()
        fmt.Println("Context cancelled")
    }()
...
}
\end{lstlisting}
    \item It is possible for ctx 3 to not be seen as cancelled if ctx (main) is cancelled first. So GO can kill the thread ctx3 is on without printing 
    \item We can use waitgroup or done channel to ensure that main only terminates when all its threads (including ctx3) are done
\end{itemize}

\textbf{Fan-Out, Fan-In} 
\begin{itemize}
    \item Fan-out distributes intense workloads across multiple workers
    \item Fan-in centralises results but this is not necesarily serialised 
    \item We can serialise it using higher-order channels that uses an unbuffered channel, but this also greatly limits concurrency
\end{itemize}

\textbf{Pipelining} \\ 
\begin{itemize}
    \item Useful for resource constrained parallelism while maintaining a separation of constraints
    \item E.g for a 3 stage program that is CPU intensive at stage 2 (bottleneck) and can only run 10 threads concurrently at stage 3 (due to memory constraints), we can pipeline multiple threads at stage 2 and only 10 threads at stage 3 to maximise throughput
\end{itemize}

\pagebreak
% USEFULE FACTS
\subsection*{Classical Synchronisation Problems}

\includegraphics*[width=7cm, height=3cm]{classicalSummary.png}

\subsection*{Comparison between Rust, Go, C++}
\textbf{Ownership} \\ 
\begin{itemize}
    \item C++ has RAII to manage resources, moveable but not copyable reference
\end{itemize}

\textbf{Testing and debugging} \\
\begin{itemize}
    \item C++ cannot catch many race conditions and concurrency bugs at compile time, and requires testing at run time 
    \item Tsan and Asan has verbose output (unlike rust compiler)
    \item Catching concurrent bugs at run time runs into heisenbugs
\end{itemize}

\textbf{Memory Management: GO vs Rust} \\
\begin{itemize}
    \item Go has a garbage collector, Rust has a manual memory management system based on ownership and borrowing - need to be more careful with rust 
    \item Rust's borrow checker does guarante lifetime and exclusive access at compile time
\end{itemize}

\textbf{Rust Concurrency Model} \\
\begin{itemize}
    \item Rust's concurrency is provably correct - the compiler can catch thread safety bugs at compile time. This eliminates basic concurrency bugs
\end{itemize}

\subsection*{Random Trivia}

\textbf{Atomic shared ptr but not atomic unique ptr} \\
\begin{itemize}
    \item std::shared$\_$ptr needs to allocate a control-block where the strong and weak count are kept, so type-erasure of the deleter came at a trivial cost (a simply slightly larger control-block).
    \item Note: Atmoic shared$\_$ptr is not lock free
    \item std::unique$\_$ptr has a zero-overhead policy; using a std::unique$\_$ptr should not incur any overhead compared to using a raw pointer.
    \item But the deleter of a std::unique$\_$ptr may not be zero cost (that is, it may stores some state)
    \item Since there is no guarantee that we can do CAS on the deleter without the use of mutex (which comes with extra overhead), we cannot do CAS on the unique ptr
    \item Another argument is that implementing an atomic unique ptr requires weakening the precondition on the atomic template parameter to avoid deadlocks. And that there is no effective mechanism to test for the weaker property
\end{itemize}

\end{multicols}
\end{document}