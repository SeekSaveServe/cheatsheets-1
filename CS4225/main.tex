\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{multicol}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\setlist{nosep}
\usepackage{subfig}
\usepackage{listings}

% Define Rust language for listings package
\lstdefinelanguage{Rust}{
  morekeywords={let, mut},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}

% for including images
\graphicspath{ {./images/} }


\pdfinfo{
  /Title (CS4225.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Pei Cheng Yi)
  /Subject (CS4225)
  /Keywords (CS4225, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

\newenvironment{tightcenter}{%
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
}{%
  \end{center}
}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}%
\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}
%  convenient absolute value symbol
\newcommand{\abs}[1]{\vert #1 \vert}
%  convenient floor and ceiling
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
%  convenient modulo
\newcommand{\Mod}[1]{\ \mathrm{mod}\ #1}
%  for logical not operator, iff symbol, convenient "if/then"
\renewcommand{\lnot}{\mathord{\sim}}
\let\then\Rightarrow
\let\Then\Rightarrow
%  vectors
\newcommand{\vv}[1]{\boldsymbol{#1}}
\newcommand{\VV}[1]{\overrightarrow{#1}}
%  column vector
\newcommand{\cvv}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}
\newcommand{\code}[1]{\textcolor{myblue}{\texttt{#1}}}
\newcommand\bggreen{\cellcolor{green!10}}

\makeatother
\definecolor{myblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{myblue}}
% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% this changes all items (enumerate and itemize)
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.4cm}
\setlength{\leftmarginiii}{0.5cm}
\setlist[enumerate,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,3]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{4}


% multicol parameters
% These lengths are set only within the two main columns
\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \fbox{%
        \parbox{0.8\linewidth}{\centering \textcolor{black}{
            {\Large\textbf{CS4225}}
            \\ \normalsize{AY23/24 S1}}
            \\ {\footnotesize \textcolor{myblue}{github.com/SeekSaveServe}}
        }%
    }
\end{center}

% LECTURES

\section{Lectures}

\subsection{L1: Introduction}
\textbf{Four Vs of Data Science}
\begin{itemize}
  \item Volume 
  \item Variety
  \item Velocity 
  \item Veracity - uncertainty of data
\end{itemize}

\textbf{Storage Hierarchy}
\begin{itemize}
  \item Volume: Server $le$ Rack $le$ Cluster 
  \item Speed: Server $ge$ Rack $ge$ Cluster 
\end{itemize}

\textbf{Bandwidth vs Latency}
\begin{itemize}
  \item \textbf{Throughput} Actual rate at which data is transmitted across the network over a period of time
  \item \textbf{Bandwidth} Maximum (capacity) amount of data that can be transmitted per unit time 
  \item \textbf{Latency} Time taken for 1 data packet to go from source to destination (or both ways)
  \item Latency does not matter when transmitting a large amount of data 
  \item Bandwith does not matter when transmitting a small amount of data
\end{itemize}

\textbf{Cost of moving data }
\includegraphics*[width=7cm]{data_cost}
\begin{itemize}
  \item Bandwidth drops and latency increases as we move up the data hierarchy 
  \item Disk reads are also much more expensive 
\end{itemize}

\textbf{Big ideas of data processing}
\begin{itemize}
  \item Horizontal scaling is cheaper than vertical scaling 
  \item Move data processing to the machine with the data since data clusters have limited bandwidth
  \item Process data sequentially and avoid random access to reduce total seek time 
  \item Seamless scalability $\rightarrow$ use more machines to reduce time taken to process data
\end{itemize}

\textbf{Challenges}
\begin{itemize}
  \item Machine failures
  \item Synchronisation 
  \item Programming difficulty
\end{itemize}

\subsection*{L2 Map reduce}
\includegraphics*[width=7cm]{map_reduce}
\begin{itemize}
  \item Map: extract something of interest from each. Emits a key value pair 
  \item Shuffle: Shuffle intermediate results by key value pairs 
  \item Reduce: Aggregate intermediate results
  \item Each of these three processes can occur concurently across different machines
\end{itemize}

\includegraphics*[width=7cm]{map_reduce_impl}
\textbf{Map Reduce Implementation}
\begin{enumerate}
  \item \textbf{Submit:} User submits mapreduce program and configuration (e.g. no. of workers) to Master node
  \item \textbf{Schedule:} Master schedules resource for map and reduce tasks (master does not handle actual data)
  \item \textbf{Read:} Input files are separated into splits of 128MB. Each split corresponds to one map task. Each worker executes map tasks 1 ata a time 
  \item \textbf{Map phase:} Each worker iterates over each key,value tuple and applies the map function 
  \item \textbf{Local write: } each worker writes the output of map to intermiediate files on its local disk.These filees are partitioned by key 
  \item \textbf{Remote read:} each reduce worker is responsible for $\geq$ key. For each key, it reads the data it needs from the corresponding partitioon of each mapper's local disk
  \item \textbf{Write: } output of the reduce function is written (usually to a distributed file system such as HDFS)
\end{enumerate}
  

\textbf{Interface}
\begin{itemize}
  \item map(k,v) $\rightarrow$ list(k',v')
  \item reduce 
\end{itemize}



% TODO: L2  

\subsection*{L3: No SQL Overview}

\textbf{NoSQL}
\begin{itemize}
  \item Not Only SQL: can include sql
  \item Stores data in a format other than relational DB 
  \item Sql refers to to relational DBMS, not the querying language - NoSQL can have querying lang too 
  \item Used for large volumes of data and data that does not fit in a structured data (e.g. some has image, some don't)
\end{itemize}

\textbf{Propertiess}
\begin{itemize}
  \item Horizontal Scalable: easy to partition and distribute across machines 
  \item Replicate and distributed over many servers 
  \item Simple call interface 
  \item Often weaker concurrency model than RDBMS 
  \item Efficient use of distributed indexes and RAM 
  \item Flexible schema
\end{itemize}


\textbf{Major NoSQL DB}
\begin{itemize}
  \item Key-value stores
  \begin{itemize}
    \item Stores mapping (associations) bewteen keys and values 
    \item Keys are usually primitives (int,str,raw bytes etc) that can be easily queried 
    \item Values can be primitive or complex; usually cannot be easily queried (lists, JSON, HTML, BLOB)
    \item Eventually consistent
    \item \textbf{Operations}
    \begin{itemize}
      \item Get - fetch value with key 
      \item Put - set value with key 
      \item Multi-Get, multi-put, range queries (must be comparable, e.g. int, str)
    \end{itemize}
    \item \textbf{Suitable for}
    \begin{itemize}
      \item Small continuous read and writes
      \item Storing basic information or no clear schema
      \item Complex queries are rarely required
      \item Improves scalability and efficiency of read and writes 
      \item Eventually consistent, so the data might be stale 
    \end{itemize}
    \item E.g. Storing user sesions, caches, user data that is often processed individually
    \item \textbf{Implementation}
    \begin{itemize}
      \item Non-persistent: Just a big in memory hash table (E.g. redis, memcached) that needs to be regularly backed up to disk
      \item Persistent: data is stored persistently to disk (E.g. RocksDB, Dynamo, Riak)
    \end{itemize}
  \end{itemize}

  \item Wide-column databases - stored sparsely
  \includegraphics*[width=6cm]{wide_col} 
  \begin{itemize}
    \item Rows describe entities
    \item Related groups of columns are grouped as column families (similar to separate tables, except they share the same row)
    \item Sparsity: If a cloumn is not used for a row, it doesn't use space (saves space for sparse data)
  \end{itemize}
  \item Document stores 
  \includegraphics*[width=6cm]{doc_model}
  \begin{itemize}
    \item no schema (flexible schema)
    \item A data base can have multiple collections 
    \item A collection (tables) can have multiple documents (rows)
    \item A document is a JSON-like object with field (columns) and values 
    \item Different documents can have different field and can be nested 
    \item Flexible Schema: accommodates data with different characteristics  
    \item \textbf{Querying}
    \begin{itemize}
      \item Unlike key val stores, doc stores allow querying based on the content 
      \item If the field does not exist on the doc, we just skip it when doing CRUD
    \end{itemize}
  \end{itemize}
  \item Graph databases
  \begin{itemize}
    \item Need to store information about the nodes and edges
    \item Edges: relationship between data (nodes)
    \item Good at modelling and querying complexed relationships between entities
    \item Good for modelling data as graphh problems (traversing relationships, shortest path, social networks etc) 
  \end{itemize}
  \item Vector Databases
  \begin{itemize}
    \item Store vectors (each row is a point in d dimensions)
    \item Usually dense, numerical, and high-dimensional (data with many features)
    \item Allow fast similarity search via locality sensitive hashing (LSH), similar to min-hashing
    \item Scalable, real-time updates, replication
    \item Good for LLM and vision models as they need to be converted to vectors, and search, recommendation, clustering can be easily added 
    \item Good for contetn based similarity matching
    \item E.g. Milvus, Radis, MongoDB, Atlas, Weaviate
  \end{itemize}
\end{itemize}

\textbf{Consistency}
\includegraphics*[width=7cm]{consistency}
\begin{itemize}
  \item Strong: Any reads on all observers immediately read the same result after update (uses locks, higher latency)
  \item Eventual: If the system is working and we wait long enough, eventually all reads will produce the same value (correctness affected)
\end{itemize}

\textbf{BASE}
\begin{itemize}
  \item \textbf{B}asically \textbf{A}vailable - basic writing and reading operations are available most of the time 
  \item \textbf{S}oft state: without guarantees, we only have some probability of knowing the state at any time 
  \item \textbf{E}ventually consistent
  \item Eventual consistency offers better availability at the cost of weaker consistency 
  \item NoSQL allows for weaker consistency guarantees, and can be tuned to be stronger (tunable consistency)
  \item Suitable for statistical queries and social media feed but not suited for financial transactions 
\end{itemize}

\textbf{Duplication /\ Denormalization}
\begin{itemize}
  \item Motivation: Support join statments $\rightarrow$ how do we join 2 tables to form 1 new table 
  \item Some optimizations in SQL may not be possible in NoSQL
  \item \textbf{Denoormalization: } 
  \begin{itemize}
    \item Storage is cheap! Duplicate data to boost efficiency
    \item Tables are designed around potential join queries (pre-create the join tables)
    \item Good if the queries types are fixed 
    \item What if a field is updated? $\rightarrow$ changes need to be propagated to multiple table 
  \end{itemize}
\end{itemize}

\includegraphics*[width=7cm]{nosql_pros_cons}
\begin{itemize}
  \item Depends on: 
  \begin{enumerate}
    \item if denormalization is suitable
    \item importance of consistency 
    \item complexity of queries (joins Vs read/write)
  \end{enumerate}
\end{itemize}

% L5 : TODO

% L6
\section{L6: Sparks Basics I}

\textbf{Hadoop Vs Spark}
\begin{itemize}
  \item Spark stores most of intermediate results in memory, making it faster for iterative processing (spillover to disk still happens if memory runs out)
  \item Hadoop writes intermediate results to local machine / disk. This is not efficient for iterative processing and ML 
  \item Sparks has ease of computability
  \item Spark combines batch processing, streaming, ML, graph processing 
\end{itemize}

\textbf{Spark Architecture}
\begin{itemize}
  \item Driver process: respond to user input and distributes work to executors
  \item Executors: executes code and send result back to the driver
\end{itemize}

\includegraphics*[width=7cm]{spark_arch}

\textbf{Evolution of Spark APIs}

\includegraphics*[width=7cm, height=4cm]{spark_evo}


\textbf{Lineage Approach}
\begin{itemize}
  \item Using replication is expensive since Spark uses memory
  \item A faulty node is replaced and recomputed using the DAG from the lost partition (E.g narrow transformations)
\end{itemize}

\textbf{Resilient Distributed Datasets (RDD)}
\begin{itemize}
  \item Resilient: Fault tolerence through lineage 
  \item Each node executes over 1 partition of data (data parellelism), a RDD is a collection of nodes and the driver
  \item DDs: colleciton of objects distributed over machines
  \item Immutable
  \includegraphics*[width=6cm]{rdd1}
  \item \textbf{Transformations}
  \begin{itemize}
    \item Transform RDDs into RDDs
    \item Lazily evaluated, which allows optimisations to be applied over a series of transformations
    \item E.g.: map, order, groupby, filter, join, select 
  \end{itemize}
  \item \textbf{Actions}
  \begin{itemize}
    \item Triggers spark to compute result from a series of transformations 
    \item \includegraphics*[width=6cm]{rdd2}
    \item Retrieve all RDD to the driver node 
    \item E.g.: count, collect, show, save
  \end{itemize}
  \item Spark actions and transformations are calculated in parallel across distributed workers
  \item RDDs are objects. Completed RDDs are stored in memory and can be flushed out 
  \item Note: transformation work on files in the worker node, not the driver
\end{itemize}

\textbf{Caching}
\begin{itemize}
  \item \textbf{Caching}: sometimes we want to reuse RDDs to avoid recomputation
  \includegraphics*[width=7cm, height=4cm]{rdd4}
  \item cache is also a transformation!
  \item It is lazily done. So it only takes effect after an action
  \item Cache store an RDD to memory of each worker node 
  \item persist()" store RDD to memory or disk or off-heap memory 
  \item RDDs are evicted on a LRU basis so cached RDDs can be evicted
  \includegraphics*[width=7cm]{caching.png}
\end{itemize}


\textbf{DAG}
\begin{itemize}
  \item Represents all RDD objects and order of transformation
  \item RDDs are functional operations
  \item Operations here happens in parallel
\end{itemize}
\includegraphics*[width=7cm]{dag}

\textbf{Narrow and wide dependencies transformation}
\begin{itemize}
  \item Narrow can be linked together 
  \item Wide dependencies are across stage 
  \item Wide: implict synchronisation effect
  \item Narrow: each partition of the parent RDD is used by at most 1 partition of child RDD 
  \item Narrow: map, flatmap, filter, contains 
  \item Wide: partition of parent RDD is used by multiple partitions of the child RDD (other worker nodes)
  \item Wide: reduceByKey, groupBy, orderBy
  \includegraphics*[width=7cm]{dependencies}
  \item Consecutive narrows are grouped as "stages" in DAG
  \item Within stages: spark computes consecutive transformations on the same machine (pipelined, parallelized)
  \item Across stage: data needs to be shuffled and intermediary results needs to be written to disk
  \item Minimize shuffling (across stage)
\end{itemize}

\textbf{Lineage and fault tolerance}
\begin{itemize}
  \item Does not use replication (unlike hadoop) since memory is limited
  \item Lineage: if a worker is down, we replace it, and use DAG to recompute the data in the lost partition. Lost partition will be recomputed from the RDDs
  \item The DAG of each RDD has to be stored 
  \item When flushed, the node can start where it stopped within the wide stage later on 
  \item If the job is passed to a new node, the RDD job starts from the start of the stage
\end{itemize}

\textbf{Dataframe}
\begin{itemize}
  \item column based (applied for each attribute)
  \item Dataframe represents a table of data, similar to tables in sql 
  \item This is a higher level interface that is easier to use 
  \item Implemented with RDDs 
  \item Expression based operation
  \includegraphics*[width=7cm]{dataframe1}
  \item Spark can use sql queries for dataframes which is similar to:
  \includegraphics*[width=7cm]{df2}
  \item Expression based, does not specify the order of functions, hence leaving room for optimisation 
\end{itemize}

\textbf{Datasets}
\begin{itemize}
  \item Type safe version of data frame
  \item Datasets are not available in python and R ssince they are dynamically typed 
  \item Each row is an object of a user defined class
  \includegraphics*[width=7cm]{ds1}
\end{itemize}


\subsection*{L8: Streams}
\textbf{Motivation}
\begin{itemize}
  \item Data arrives overtime (online) via message queue, file stream etc
  \item System cannot store the entire stream, so we have to process the data as they arrive
  \item E.g. search, online activity data, sensor data, financial data
  \item Cannot wait till all the data is recevied to decide
\end{itemize}

\textbf{Fault Tolerance}
\begin{itemize}
  \item Need to be able to store and access intermediate data
  \item Non-stateful stream processing is not accurate 
\end{itemize}

\textbf{Spark stream processing - structued streaming}
\begin{itemize}
  \item Micro Batch model: Divides data from input stream into micro batches 
  \item Each batch is processed in a distributed manner 
  \item Small, deterministic task generates the output to batches
  \item \textbf{Advantages}
  \begin{itemize}
    \item Quick and efficiently recover from failures (process the failed batch again, rollover)
    \item Determinisitc nature: end-to-end exactly once processing is guaranteed
  \end{itemize}
  \item \textbf{Disadvantages: High throughput, high latency}
  \begin{itemize}
    \item Latency of a few seconds - need to wait for all records in the microbatch to be completed 
    \item Application may experience higher delay in other parts of the pipeline 
    \item The latency might be too high for some
  \end{itemize}
  \includegraphics*[width=7cm]{micro_batch}
  \item Treat the table as unbounded, with new rows streaming in
  \item Data flows in incrementally, new "rows" are processed are the result is appended to the output table as new rows as well 
  \includegraphics*[width=7cm]{incremental_exe.png}
  \item \textbf{Defining a structured query}
  \begin{enumerate}
    \item Define input source(s)
    \item Transform data 
    \item Define output sink and output mode 
    \begin{itemize}
      \item output writing details (where and how)
      \item processing details (how to process and recover from failure)
    \end{itemize}
    \item Specify processing details 
    \begin{itemize}
      \item Triggering details: When to trigger the discovery and processing of newly available steaming data 
      \item Check point location: store streaming info for recovery 
    \end{itemize}
    \item start query
  \end{enumerate}
  \includegraphics*[width=7cm]{incremental_exe2.png}
\end{itemize}

\textbf{Data Transformation}
\begin{itemize}
  \item \textbf{Stateless transformation}
  \begin{itemize}
    \item Process each row without info from prev rows 
    \item Projection: select(), explode(), map(), flatmap()
    \item Selection: filter(), where()
  \end{itemize}
  \item \textbf{Stateful transformation}
  \begin{itemize}
    \item E.g. df.groupBy().count()
    \item \^ partial count is stored somewhere and passed to the next batch
    \item It is important to have exact-once even with potential failure and recovery so that the final count is accurate
  \end{itemize}
  \includegraphics*[width=7cm]{spark_stateful.png}
\end{itemize}

\textbf{Stateful streaming aggregation}
\begin{itemize}
  \item \textbf{Aggregation not based on time}
  \item Global: running count = sensorReadings.groupBy().count()
  \item Group: baseline values = sensorReadings.groupBy("sensorID").mean("value")
  \item sum(), mean(), count(), stddev(), countDistinct(), collect\_set(), approx\_count\_distinct()
  \item \textbf{Aggregation based on time}
  \item Groups are based on processing-time window  or event-time window
  \item Processing-time windows may not always contain the same events due to network latency, congestion etc -- result not consistent 
  \item Event-time window is persistent and ensures exact-once semantics -- preferred!
  \item Event-time decouples processing speed from results
  \item sensorReadings.groupBy("sensorId", window("eventTime", "5 minutes")).count()
\end{itemize}


\textbf{Tumbling Window}
\includegraphics*[width=7cm]{tumbling_window.png}
\begin{itemize}
  \item We tag an id to the event based on the event time 
  \item The id refers to the corresponding tumbling window
  \item \textbf{Overlapping Window}
  \includegraphics*[width=7cm]{tumbling_window2.png}
  \begin{itemize}
    \item Consecutvie windows share some data with adjacent windows 
    \item This makes data more continuous and smooth 
    \item Increases data utilization
    \item Reduces edge effect, where the values at the edge of a event window weights less than the events in the centre 
    \item Prevent loss of information, particularly the data at the edge 
    \item A data that arrives will update \>=1 window
  \end{itemize}
\end{itemize}

\textbf{Watermark}
\includegraphics*[width=7cm]{watermark.png}
\begin{itemize}
  \item Water mark: we only track records that are within highest current time - water mark duration: highest current time 
\end{itemize}

\textbf{Performance Tuning}
\begin{itemize}
  \item Cluster resources appropriately to avoid running 24\/7 
  \item Set partitions for shuffling to be lower than batch queries for streaming data, so that the data in each node is large enough for the batch queries 
  \item Setting source rate limits for stability -- prevent incoming stream from breaking spark
  \item Multiple batch,streaming queries, ML can have at once 
\end{itemize}



\textbf{Flink}
\textbf{Overview}
\includegraphics*[width=7cm]{flink.png}
\begin{itemize}
  \item distributed system for stateful parallel data stream
  \item Treats stream as a stream
  \item Achieves microsecond latency
  \item Event driven, message queue and event logs
  \item Also has logical plan and physical plan, similar to spark
\end{itemize}


\textbf{Dataflow model}
\includegraphics*[width=7cm]{dataflow.png}

\textbf{Flink architecture}
\includegraphics*[width=7cm]{flink_overview.png}
\begin{itemize}
  \item Resouce manager is responsible for scheduling tasks to resources
\end{itemize}

\textbf{Task exectution}
\includegraphics*[width=7cm]{task_execution.png}
\begin{itemize}
  \item Task manager manages slots that processes tasks
  \item Task manager can execute tasks from diferent operators and different applications
  \item C $\rightarrow$ B, some network shuffling is done 
\end{itemize}


\textbf{Data transfer in flink}
\includegraphics*[width=7cm]{task_execution.png}
\begin{itemize}
  \item Task of an app is continuously exchanging data 
  \item Task manager takes care ofsending and echanging data 
  \item Network component of task manager buffers the records before sending
  \item Send and receive has their own buffer to reduce network traffic, so send and receive are done async (unlike micro batch)
\end{itemize}
% Tutorials 

\textbf{Event time processing (Flink)}
\begin{itemize}
  \item Every record has an accompanying time stamp
  \includegraphics*[width=7cm]{watermark_flink.png}
  \item \textbf{Watermark (flink)}
  \begin{itemize}
    \item more like a trigger mark (spark triggers after every micro batch, so latency is the size of the micro batch) -- watermark in flink determines how freqeuntly calculations are triggered!
    \item Watermark is represented as special records holding a timestamp
    \item Water mark flows in a stream of regular records
    \item Heuristic watermark: Results trigger after watermark, late records processed again later
    \item Perfect watermark: Late records included, trigger happens after 
  \end{itemize}
\includegraphics*[width=7cm]{watermark_flink2.png}
\item $withLateFiring$ determines when to drop late records
\end{itemize}

\textbf{State Management in Flink}
\includegraphics*[width=7cm]{flink_state.png}
\begin{itemize}
  \item \textbf{Operator State}
  \begin{itemize}
    \item Scoped to an operator task, cannot be accessed by other operators
    \item All records processed by the same parallel task have acess to the same state 
  \end{itemize}
  \item \textbf{Consistent Checkpointing}
  \begin{itemize}
    \item Similar to micro batch checkpoint 
    \item Pause the ingestion of all input streams 
    \item Wait for in-flight data to be completely processed (all tasks processed their inputs)
    \item Like a barrier? Need to finish the whole "microbatch"
    \item Copy and store state to a persistent storage
    \item Need to reset from latest checkpoint -- cannot achieve milisecond delay!
  \end{itemize}
  \item \textbf{Chandy-lamport algorithm}
  \includegraphics*[width=7cm]{checkpoint_flink.png}
  \begin{itemize}
    \item Distributed checkpoints
    \item Decouples checkpoiting from processing, does not pause the entire app, only some tasks pauses 
    \item Sets up a checkpoint barrier, all tasks to perform this barrier in a distributed way 
    \item After receiving the checkpoint message, the source will save their state and broadcast checkpoint barrier to the receiving nodes 
    \item Tasks will buffer the records for barrier alignment (only process once they receive the barrier msg from all sources)
    \item Only the tasks receiving the barrier will buffer
    \item After receiving all checkpoint barriers, the task will save their state
    \item Then the barrier is emitted to the next level (sink nodes)
    \item The jobManager is notified once all the tasks have completed the checkpoint 
  \end{itemize}
\end{itemize}

\textbf{Spark vs Flink}
\includegraphics*[width=7cm]{spark vs flink.png}
% Misc

\end{multicols}
\end{document}