\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{multicol}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\setlist{nosep}
\usepackage{subfig}
\usepackage{listings}

% Define Rust language for listings package
\lstdefinelanguage{Rust}{
  morekeywords={let, mut},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}

% for including images
\graphicspath{ {./images/} }


\pdfinfo{
  /Title (CS4225.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Pei Cheng Yi)
  /Subject (CS4225)
  /Keywords (CS4225, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

\newenvironment{tightcenter}{%
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
}{%
  \end{center}
}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}%
\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}
%  convenient absolute value symbol
\newcommand{\abs}[1]{\vert #1 \vert}
%  convenient floor and ceiling
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
%  convenient modulo
\newcommand{\Mod}[1]{\ \mathrm{mod}\ #1}
%  for logical not operator, iff symbol, convenient "if/then"
\renewcommand{\lnot}{\mathord{\sim}}
\let\then\Rightarrow
\let\Then\Rightarrow
%  vectors
\newcommand{\vv}[1]{\boldsymbol{#1}}
\newcommand{\VV}[1]{\overrightarrow{#1}}
%  column vector
\newcommand{\cvv}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}
\newcommand{\code}[1]{\textcolor{myblue}{\texttt{#1}}}
\newcommand\bggreen{\cellcolor{green!10}}

\makeatother
\definecolor{myblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{myblue}}
% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% this changes all items (enumerate and itemize)
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.4cm}
\setlength{\leftmarginiii}{0.5cm}
\setlist[enumerate,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,3]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{4}


% multicol parameters
% These lengths are set only within the two main columns
\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \fbox{%
        \parbox{0.8\linewidth}{\centering \textcolor{black}{
            {\Large\textbf{CS4225}}
            \\ \normalsize{AY23/24 S1}}
            \\ {\footnotesize \textcolor{myblue}{github.com/SeekSaveServe}}
        }%
    }
\end{center}

% LECTURES

\section{Lectures}

\subsection{L1: Introduction}
\textbf{Four Vs of Data Science}
\begin{itemize}
  \item Volume 
  \item Variety
  \item Velocity 
  \item Veracity - uncertainty of data
\end{itemize}

\textbf{Storage Hierarchy}
\begin{itemize}
  \item Volume: Server $le$ Rack $le$ Cluster 
  \item Speed: Server $ge$ Rack $ge$ Cluster 
\end{itemize}

\textbf{Bandwidth vs Latency}
\begin{itemize}
  \item \textbf{Throughput} Actual rate at which data is transmitted across the network over a period of time
  \item \textbf{Bandwidth} Maximum (capacity) amount of data that can be transmitted per unit time 
  \item \textbf{Latency} Time taken for 1 data packet to go from source to destination (or both ways)
  \item Latency does not matter when transmitting a large amount of data 
  \item Bandwith does not matter when transmitting a small amount of data
\end{itemize}

\textbf{Cost of moving data }
\includegraphics*[width=7cm]{data_cost}
\begin{itemize}
  \item Bandwidth drops and latency increases as we move up the data hierarchy 
  \item Disk reads are also much more expensive 
\end{itemize}

\textbf{Big ideas of data processing}
\begin{itemize}
  \item Horizontal scaling is cheaper than vertical scaling 
  \item Move data processing to the machine with the data since data clusters have limited bandwidth
  \item Process data sequentially and avoid random access to reduce total seek time 
  \item Seamless scalability $\rightarrow$ use more machines to reduce time taken to process data
\end{itemize}

\textbf{Challenges}
\begin{itemize}
  \item Machine failures
  \item Synchronisation 
  \item Programming difficulty
\end{itemize}

\subsection*{L2 Map reduce}
\includegraphics*[width=7cm]{map_reduce}
\begin{itemize}
  \item Map: extract something of interest from each. Emits a key value pair 
  \item Shuffle: Shuffle intermediate results by key value pairs 
  \item Reduce: Aggregate intermediate results
  \item Each of these three processes can occur concurently across different machines
\end{itemize}

\includegraphics*[width=7cm]{map_reduce_impl}
\textbf{Map Reduce Implementation}
\begin{enumerate}
  \item \textbf{Submit:} User submits mapreduce program and configuration (e.g. no. of workers) to Master node
  \item \textbf{Schedule:} Master schedules resource for map and reduce tasks (master does not handle actual data)
  \item \textbf{Read:} Input files are separated into splits of 128MB. Each split corresponds to one map task. Each worker executes map tasks 1 ata a time 
  \item \textbf{Map phase:} Each worker iterates over each key,value tuple and applies the map function 
  \item \textbf{Local write: } each worker writes the output of map to intermiediate files on its local disk.These filees are partitioned by key 
  \item \textbf{Remote read:} each reduce worker is responsible for $\geq$ key. For each key, it reads the data it needs from the corresponding partitioon of each mapper's local disk
  \item \textbf{Write: } output of the reduce function is written (usually to a distributed file system such as HDFS)
\end{enumerate}
  

\textbf{Interface}
\begin{itemize}
  \item map(k,v) $\rightarrow$ list(k',v')
  \item reduce 
\end{itemize}



% TODO: L2  
\subsection*{L3: Map reduce and data mining}
\textbf{Secondary Sort}
\begin{itemize}
  \item (Natural Key, Secondary Key)
  \item Partitioner needs to be customised to partition by the natural key only 
  \item At each reducer, the secondary key (value) will be sorted
\end{itemize}

\textbf{Projection in Map reduce}
\begin{itemize}
  \item Mapper takes in tuples and emit the appropriate key 
  \item No reducer needed 
\end{itemize}

\textbf{Selection}
\begin{itemize}
  \item Mapper takes in a tuple and emit the keys that meet the condition 
  \item No reducer needed 
\end{itemize}


\textbf{Group by aggregation}
\begin{itemize}
  \item E.g. SELECT product id, AVG(price) ... GROUP BY product id 
  \item Framework automatically groups the tuples by the key
  \item Compute average in reducers 
\end{itemize}

\textbf{Relational Joins}
\begin{itemize}
  \item \textbf{Broadcast Join}
  \includegraphics*[width=7cm]{bcast_join.png}
  \begin{itemize}
    \item Requires one of the tables to fit in memory 
    \item All mappers store a copy of the small table 
    \item Iterate over big table to match with small table 
    \item No need reducer
  \end{itemize}
  \item \textbf{Reduce-side common Join}
  \includegraphics*[width=7cm]{common_join.png}
  \begin{itemize}
    \item Don't require a dataset to fit in memory 
    \item Slower than broadcast join
    \item Different mappers operate on each table, and emit records, with key as the variable to join by 
    \item Secondary sort in reducer ensures that keys from X appear before Y
    \item Then hold keys from X and use them to join against keys from Y
    \includegraphics*[width=7cm]{common_reduce.png}
  \end{itemize}
\end{itemize}

\textbf{Finding Similar Docs}
\includegraphics*[width=7cm]{shingling.png}
\begin{itemize}
  \item \textbf{Shingling} Convert documents into shingles (short phrases)
  \item \textbf{Min-Hashing} Convert phrases into signatures of each document while preserving similarity
  \item Documents with the same signature are candidate pairs for finding near duplicates
  \item K-shingle (k-gram) for a doc is a sequence of k tokens that appears in a doc 
\end{itemize}

\textbf{Similarity Metric}
\includegraphics*[width=7cm]{k-shingle.png}
\begin{itemize}
  \item \textbf{Min hash} MinHash gives us a fast approximation to the result using Jacard similarity instead of doing pairwise comparison
  \item Min hash converts large sets to short signatures while preserving similarity
\end{itemize}


\textbf{Min Hashing}
\includegraphics*[width=7cm]{jaccard.png}
\begin{itemize}
  \item Hash column c to h(c), such that the table can fit in memory
  \item sim(c1, c2) is high, then h(c1) == h(c2), if low h(c1) != h(c2)
  \item Shingles:  { (the cat), (cat is), (is glad) }
  \item Step 1: h(“the cat”) = 12, h(“cat is”) = 74, h(“is glad”) = 48
  \item Step 2: Compute the minimum, min(12, 74, 48) = 12
  \item The probability that two documents have the same min hash is the same as their Jaccard similarity
  \item Multiple hash functions are used IRL, and the candidate pair is a pair with a sufficeint number of matching hash values
\end{itemize}

\textbf{MapReduce Implementation}
\includegraphics*[width=7cm]{map_reduce_impl.png}

\textbf{Clustering}
\includegraphics*[width=7cm]{clustering.png}
\begin{itemize}
  \item Separates unlabelled data into groups of similar points 
  \item Clusters have high intra cluster similaritty and low inter cluster similarity
  \item \textbf{Steps}
  \begin{enumerate}
    \item Initialisation: Pick K random points as centres 
    \item Repeat: 
    \begin{itemize}
      \item Assign each point to nearest cluster 
      \item Move each cluster to average of its assigne centres 
    \end{itemize}
    \item stop if no assignment change 
  \end{enumerate}
\end{itemize}

\textbf{Map Reduce V1}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
1: class Mapper
2: method Configure()
3:  c <- LoadClusters()
4: method Map(id i, point p)
5:  n <- NearestClusterID(clusters c, point p)
6:  p <- ExtendPoint(point p)
7:  Emit(clusterid n, point p)

1: class Reducer
2: method Reduce(clusterid n, points [p1, p2, ...])
3:  s <- InitPointSum()
4:  for all point p 2 points do
5:    s <- s + p
6:   m <- ComputeCentroid(point s)
7:  Emit(clusterid n, centroid m)
\end{lstlisting}  
\begin{itemize}
  \item O(nmd) disk IO, inefficient
\end{itemize}

\textbf{Map Reduce V2}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
1: class Mapper
2: method Configure()
3:  c <- LoadClusters()
4:  H <- InitAssociativeArray()

5: method Map(id i, point p)
6:  n <- NearestClusterID(clusters c, point p)
7:  p <- ExtendPoint(point p)
# We emit the hashed array instead of emitting by point
8:  H{n} <- H{n} + p

9: method Close()
10:   for all clusterid n 2 H do
11:     Emit(clusterid n, point H{n})

1: class Reducer
2: method Reduce(clusterid n, points [p1, p2, ...])
3:  s <- InitPointSum()
4:  for all point p 2 points do
5:    s <- s + p
6:  m <- ComputeCentroid(point s)
7: Emit(clusterid n, centroid m)
\end{lstlisting}  
\begin{itemize}
  \item O(kmd) disk IO, inefficient
\end{itemize}

\subsection*{L4: No SQL Overview}

\textbf{NoSQL}
\begin{itemize}
  \item Not Only SQL: can include sql
  \item Stores data in a format other than relational DB 
  \item Sql refers to to relational DBMS, not the querying language - NoSQL can have querying lang too 
  \item Used for large volumes of data and data that does not fit in a structured data (e.g. some has image, some don't)
\end{itemize}

\textbf{Propertiess}
\begin{itemize}
  \item Horizontal Scalable: easy to partition and distribute across machines 
  \item Replicate and distributed over many servers 
  \item Simple call interface 
  \item Often weaker concurrency model than RDBMS 
  \item Efficient use of distributed indexes and RAM 
  \item Flexible schema
\end{itemize}


\textbf{Major NoSQL DB}
\begin{itemize}
  \item Key-value stores
  \begin{itemize}
    \item Stores mapping (associations) bewteen keys and values 
    \item Keys are usually primitives (int,str,raw bytes etc) that can be easily queried 
    \item Values can be primitive or complex; usually cannot be easily queried (lists, JSON, HTML, BLOB)
    \item Eventually consistent
    \item \textbf{Operations}
    \begin{itemize}
      \item Get - fetch value with key 
      \item Put - set value with key 
      \item Multi-Get, multi-put, range queries (must be comparable, e.g. int, str)
    \end{itemize}
    \item \textbf{Suitable for}
    \begin{itemize}
      \item Small continuous read and writes
      \item Storing basic information or no clear schema
      \item Complex queries are rarely required
      \item Improves scalability and efficiency of read and writes 
      \item Eventually consistent, so the data might be stale 
    \end{itemize}
    \item E.g. Storing user sesions, caches, user data that is often processed individually
    \item \textbf{Implementation}
    \begin{itemize}
      \item Non-persistent: Just a big in memory hash table (E.g. redis, memcached) that needs to be regularly backed up to disk
      \item Persistent: data is stored persistently to disk (E.g. RocksDB, Dynamo, Riak)
    \end{itemize}
  \end{itemize}

  \item Wide-column databases - stored sparsely
  \includegraphics*[width=6cm]{wide_col} 
  \begin{itemize}
    \item Rows describe entities
    \item Related groups of columns are grouped as column families (similar to separate tables, except they share the same row)
    \item Sparsity: If a cloumn is not used for a row, it doesn't use space (saves space for sparse data)
  \end{itemize}
  \item Document stores 
  \includegraphics*[width=6cm]{doc_model}
  \begin{itemize}
    \item no schema (flexible schema)
    \item A data base can have multiple collections 
    \item A collection (tables) can have multiple documents (rows)
    \item A document is a JSON-like object with field (columns) and values 
    \item Different documents can have different field and can be nested 
    \item Flexible Schema: accommodates data with different characteristics  
    \item \textbf{Querying}
    \begin{itemize}
      \item Unlike key val stores, doc stores allow querying based on the content 
      \item If the field does not exist on the doc, we just skip it when doing CRUD
    \end{itemize}
  \end{itemize}
  \item Graph databases
  \begin{itemize}
    \item Need to store information about the nodes and edges
    \item Edges: relationship between data (nodes)
    \item Good at modelling and querying complexed relationships between entities
    \item Good for modelling data as graphh problems (traversing relationships, shortest path, social networks etc) 
  \end{itemize}
  \item Vector Databases
  \begin{itemize}
    \item Store vectors (each row is a point in d dimensions)
    \item Usually dense, numerical, and high-dimensional (data with many features)
    \item Allow fast similarity search via locality sensitive hashing (LSH), similar to min-hashing
    \item Scalable, real-time updates, replication
    \item Good for LLM and vision models as they need to be converted to vectors, and search, recommendation, clustering can be easily added 
    \item Good for contetn based similarity matching
    \item E.g. Milvus, Radis, MongoDB, Atlas, Weaviate
  \end{itemize}
\end{itemize}

\textbf{Consistency}
\includegraphics*[width=7cm]{consistency}
\begin{itemize}
  \item Strong: Any reads on all observers immediately read the same result after update (uses locks, higher latency)
  \item Eventual: If the system is working and we wait long enough, eventually all reads will produce the same value (correctness affected)
\end{itemize}

\textbf{BASE}
\begin{itemize}
  \item \textbf{B}asically \textbf{A}vailable - basic writing and reading operations are available most of the time 
  \item \textbf{S}oft state: without guarantees, we only have some probability of knowing the state at any time 
  \item \textbf{E}ventually consistent
  \item Eventual consistency offers better availability at the cost of weaker consistency 
  \item NoSQL allows for weaker consistency guarantees, and can be tuned to be stronger (tunable consistency)
  \item Suitable for statistical queries and social media feed but not suited for financial transactions 
\end{itemize}

\textbf{Duplication /\ Denormalization}
\begin{itemize}
  \item Motivation: Support join statments $\rightarrow$ how do we join 2 tables to form 1 new table 
  \item Some optimizations in SQL may not be possible in NoSQL
  \item \textbf{Denoormalization: } 
  \begin{itemize}
    \item Storage is cheap! Duplicate data to boost efficiency
    \item Tables are designed around potential join queries (pre-create the join tables)
    \item Good if the queries types are fixed 
    \item What if a field is updated? $\rightarrow$ changes need to be propagated to multiple table 
  \end{itemize}
\end{itemize}

\includegraphics*[width=7cm]{nosql_pros_cons}
\begin{itemize}
  \item Depends on: 
  \begin{enumerate}
    \item if denormalization is suitable
    \item importance of consistency 
    \item complexity of queries (joins Vs read/write)
  \end{enumerate}
\end{itemize}

% L5 : TODO
\subsection*{L5: NoSQL and basics of Distributed system}
\textbf{Distributed DB architecture}
\begin{itemize}
  \item Shared Everything: Single Node DBMS
  \item Shared memory: Super Computer 
  \item Shared Disks: Cloud databases 
  \item Shared nothing: NoSQL (mostly)
\end{itemize}

\textbf{Horizontal Partioning}
\begin{itemize}
  \item \textbf{Partition Key}: variable used to decide which node each tuple will be stored on 
  \item Choose partition key based on a column 
  \item Partition key should have high cardinality but also avoid having a single key being too frequent (node not large enough)
  \item One way to do this is by using a composite key
  \item \textbf{Range Partition}: split partition based on range of values, good for ranged queries but can lead to imbalanced sharding
  \item \textbf{Consistent Hashing}: 
  \includegraphics*[width=7cm]{consistent_hash.png}
  \begin{itemize}
    \item Delete: Re-assign tuples clockwise to the next marker 
    \item Replication: replicate and add the tuple in the next few nodes clockwise to the primary node 
    \item Mutplie Markers: Have multiple markes  per node, such that after deletion we don't create a large imbalance
  \end{itemize}
\end{itemize}


\textbf{Query Processing in NoSQL}
\includegraphics*[width=7cm]{architecture_mongo_db.png}
\begin{itemize}
  \item Query is issued to a router and dispatched to the relevant shard with the help of the config server 
  \item The shards will then run their own queries and return the result to the router 
  \item If the query key is based on a key other than the shard key, it is relevant to all shards and will therefore be sent to all shards 
  \item Write: Primary receives write operation and writes it onto the write log, secondaries will replicate this to their local data 
  \item Read: user can specify if they want to read from the primary or secondary. Reading from the secondary decreases latency and spreads the load 
  \item Election: If primary fails, one of the secondary takes over
\end{itemize}

\textbf{Scalability for NoSQL}
\includegraphics*[width=7cm]{scalability for nosql.png}
% L6
\section{L6: Sparks Basics I}

\textbf{Hadoop Vs Spark}
\begin{itemize}
  \item Spark stores most of intermediate results in memory, making it faster for iterative processing (spillover to disk still happens if memory runs out)
  \item Hadoop writes intermediate results to local machine / disk. This is not efficient for iterative processing and ML 
  \item Sparks has ease of computability
  \item Spark combines batch processing, streaming, ML, graph processing 
\end{itemize}

\textbf{Spark Architecture}
\begin{itemize}
  \item Driver process: respond to user input and distributes work to executors
  \item Executors: executes code and send result back to the driver
\end{itemize}

\includegraphics*[width=7cm]{spark_arch}

\textbf{Evolution of Spark APIs}

\includegraphics*[width=7cm, height=4cm]{spark_evo}


\textbf{Lineage Approach}
\begin{itemize}
  \item Using replication is expensive since Spark uses memory
  \item A faulty node is replaced and recomputed using the DAG from the lost partition (E.g narrow transformations)
\end{itemize}

\textbf{Resilient Distributed Datasets (RDD)}
\begin{itemize}
  \item Resilient: Fault tolerence through lineage 
  \item Each node executes over 1 partition of data (data parellelism), a RDD is a collection of nodes and the driver
  \item DDs: colleciton of objects distributed over machines
  \item Immutable
  \includegraphics*[width=6cm]{rdd1}
  \item \textbf{Transformations}
  \begin{itemize}
    \item Transform RDDs into RDDs
    \item Lazily evaluated, which allows optimisations to be applied over a series of transformations
    \item E.g.: map, order, groupby, filter, join, select 
  \end{itemize}
  \item \textbf{Actions}
  \begin{itemize}
    \item Triggers spark to compute result from a series of transformations 
    \item \includegraphics*[width=6cm]{rdd2}
    \item Retrieve all RDD to the driver node 
    \item E.g.: count, collect, show, save
  \end{itemize}
  \item Spark actions and transformations are calculated in parallel across distributed workers
  \item RDDs are objects. Completed RDDs are stored in memory and can be flushed out 
  \item Note: transformation work on files in the worker node, not the driver
\end{itemize}

\textbf{Caching}
\begin{itemize}
  \item \textbf{Caching}: sometimes we want to reuse RDDs to avoid recomputation
  \includegraphics*[width=7cm, height=4cm]{rdd4}
  \item cache is also a transformation!
  \item It is lazily done. So it only takes effect after an action
  \item Cache store an RDD to memory of each worker node 
  \item persist()" store RDD to memory or disk or off-heap memory 
  \item RDDs are evicted on a LRU basis so cached RDDs can be evicted
  \includegraphics*[width=7cm]{caching.png}
\end{itemize}


\textbf{DAG}
\begin{itemize}
  \item Represents all RDD objects and order of transformation
  \item RDDs are functional operations
  \item Operations here happens in parallel
\end{itemize}
\includegraphics*[width=7cm]{dag}

\textbf{Narrow and wide dependencies transformation}
\begin{itemize}
  \item Narrow can be linked together 
  \item Wide dependencies are across stage 
  \item Wide: implict synchronisation effect
  \item Narrow: each partition of the parent RDD is used by at most 1 partition of child RDD 
  \item Narrow: map, flatmap, filter, contains 
  \item Wide: partition of parent RDD is used by multiple partitions of the child RDD (other worker nodes)
  \item Wide: reduceByKey, groupBy, orderBy
  \includegraphics*[width=7cm]{dependencies}
  \item Consecutive narrows are grouped as "stages" in DAG
  \item Within stages: spark computes consecutive transformations on the same machine (pipelined, parallelized)
  \item Across stage: data needs to be shuffled and intermediary results needs to be written to disk
  \item Minimize shuffling (across stage)
\end{itemize}

\textbf{Lineage and fault tolerance}
\begin{itemize}
  \item Does not use replication (unlike hadoop) since memory is limited
  \item Lineage: if a worker is down, we replace it, and use DAG to recompute the data in the lost partition. Lost partition will be recomputed from the RDDs
  \item The DAG of each RDD has to be stored 
  \item When flushed, the node can start where it stopped within the wide stage later on 
  \item If the job is passed to a new node, the RDD job starts from the start of the stage
\end{itemize}

\textbf{Dataframe}
\begin{itemize}
  \item column based (applied for each attribute)
  \item Dataframe represents a table of data, similar to tables in sql 
  \item This is a higher level interface that is easier to use 
  \item Implemented with RDDs 
  \item Expression based operation
  \includegraphics*[width=7cm]{dataframe1}
  \item Spark can use sql queries for dataframes which is similar to:
  \includegraphics*[width=7cm]{df2}
  \item Expression based, does not specify the order of functions, hence leaving room for optimisation 
\end{itemize}

\textbf{Datasets}
\begin{itemize}
  \item Type safe version of data frame
  \item Datasets are not available in python and R ssince they are dynamically typed 
  \item Each row is an object of a user defined class
  \includegraphics*[width=7cm]{ds1}
\end{itemize}

\subsection*{L7: Spark Basics II}
\textbf{Spark Pros}
\includegraphics*[width=7cm]{spark_pros.png}
\begin{itemize}
  \item Speed 
  \item Ease of use (can write sql directly)
  \item Modularity 
  \item Extensibility 
  \item Unified stack for distributed execution
  \item Keeps track of schema and supports opimised sql operations
\end{itemize}

\textbf{RDD Vs. Dataframe}
\includegraphics*[width=7cm]{rdd vs df.png}


\textbf{Catalyst Optimiser}
\begin{enumerate}
  \item Analysis 
  \item Logical Optimisation 
  \item Physical Planning 
  \item Code Generation
\end{enumerate}

\textbf{ML in Spark}
\includegraphics*[width=7cm]{spark_ml.png}
\begin{itemize}
  \item \textbf{Missing data}: Fill in missing value(imputation) with mean, or fitting regression model, or by using dummy variables
  \item imputer = Imputer(inputCols=["a", "b"],outputCols=["out a", "out b"])
  \item model = imputer.fit(df)
  \item \textbf{One Categorical Encoding}: Convert categorical values to binary (is Group1, is not Group1), good if there is no ordinal relationship between the categories
  \item 
\end{itemize}

\textbf{True positive}
\includegraphics*[width=7cm]{tpfp.png}
\begin{itemize}
  \item TN + TP = Accuracy 
  \item $\frac{TP}{TP+FN}$ is sensitivity (actual positives detected)
  \item $\frac{TN}{TN+FP}$ is specificity (actual negatives detected)
\end{itemize}

\textbf{Pipelines}
\begin{itemize}
  \item Better code reuse 
\end{itemize}

\subsection*{L8: Streams}
\textbf{Motivation}
\begin{itemize}
  \item Data arrives overtime (online) via message queue, file stream etc
  \item System cannot store the entire stream, so we have to process the data as they arrive
  \item E.g. search, online activity data, sensor data, financial data
  \item Cannot wait till all the data is recevied to decide
\end{itemize}

\textbf{Fault Tolerance}
\begin{itemize}
  \item Need to be able to store and access intermediate data
  \item Non-stateful stream processing is not accurate 
\end{itemize}

\textbf{Spark stream processing - structued streaming}
\begin{itemize}
  \item Micro Batch model: Divides data from input stream into micro batches 
  \item Each batch is processed in a distributed manner 
  \item Small, deterministic task generates the output to batches
  \item \textbf{Advantages}
  \begin{itemize}
    \item Quick and efficiently recover from failures (process the failed batch again, rollover)
    \item Determinisitc nature: end-to-end exactly once processing is guaranteed
  \end{itemize}
  \item \textbf{Disadvantages: High throughput, high latency}
  \begin{itemize}
    \item Latency of a few seconds - need to wait for all records in the microbatch to be completed 
    \item Application may experience higher delay in other parts of the pipeline 
    \item The latency might be too high for some
  \end{itemize}
  \includegraphics*[width=7cm]{micro_batch}
  \item Treat the table as unbounded, with new rows streaming in
  \item Data flows in incrementally, new "rows" are processed are the result is appended to the output table as new rows as well 
  \includegraphics*[width=7cm]{incremental_exe.png}
  \item \textbf{Defining a structured query}
  \begin{enumerate}
    \item Define input source(s)
    \item Transform data 
    \item Define output sink and output mode 
    \begin{itemize}
      \item output writing details (where and how)
      \item processing details (how to process and recover from failure)
    \end{itemize}
    \item Specify processing details 
    \begin{itemize}
      \item Triggering details: When to trigger the discovery and processing of newly available steaming data 
      \item Check point location: store streaming info for recovery 
    \end{itemize}
    \item start query
  \end{enumerate}
  \includegraphics*[width=7cm]{incremental_exe2.png}
\end{itemize}

\textbf{Data Transformation}
\begin{itemize}
  \item \textbf{Stateless transformation}
  \begin{itemize}
    \item Process each row without info from prev rows 
    \item Projection: select(), explode(), map(), flatmap()
    \item Selection: filter(), where()
  \end{itemize}
  \item \textbf{Stateful transformation}
  \begin{itemize}
    \item E.g. df.groupBy().count()
    \item \^ partial count is stored somewhere and passed to the next batch
    \item It is important to have exact-once even with potential failure and recovery so that the final count is accurate
  \end{itemize}
  \includegraphics*[width=7cm]{spark_stateful.png}
\end{itemize}

\textbf{Stateful streaming aggregation}
\begin{itemize}
  \item \textbf{Aggregation not based on time}
  \item Global: running count = sensorReadings.groupBy().count()
  \item Group: baseline values = sensorReadings.groupBy("sensorID").mean("value")
  \item sum(), mean(), count(), stddev(), countDistinct(), collect\_set(), approx\_count\_distinct()
  \item \textbf{Aggregation based on time}
  \item Groups are based on processing-time window  or event-time window
  \item Processing-time windows may not always contain the same events due to network latency, congestion etc -- result not consistent 
  \item Event-time window is persistent and ensures exact-once semantics -- preferred!
  \item Event-time decouples processing speed from results
  \item sensorReadings.groupBy("sensorId", window("eventTime", "5 minutes")).count()
\end{itemize}


\textbf{Tumbling Window}
\includegraphics*[width=7cm]{tumbling_window.png}
\begin{itemize}
  \item We tag an id to the event based on the event time 
  \item The id refers to the corresponding tumbling window
  \item \textbf{Overlapping Window}
  \includegraphics*[width=7cm]{tumbling_window2.png}
  \begin{itemize}
    \item Consecutvie windows share some data with adjacent windows 
    \item This makes data more continuous and smooth 
    \item Increases data utilization
    \item Reduces edge effect, where the values at the edge of a event window weights less than the events in the centre 
    \item Prevent loss of information, particularly the data at the edge 
    \item A data that arrives will update \>=1 window
  \end{itemize}
\end{itemize}

\textbf{Watermark}
\includegraphics*[width=7cm]{watermark.png}
\begin{itemize}
  \item Water mark: we only track records that are within highest current time - water mark duration: highest current time 
\end{itemize}

\textbf{Performance Tuning}
\begin{itemize}
  \item Cluster resources appropriately to avoid running 24\/7 
  \item Set partitions for shuffling to be lower than batch queries for streaming data, so that the data in each node is large enough for the batch queries 
  \item Setting source rate limits for stability -- prevent incoming stream from breaking spark
  \item Multiple batch,streaming queries, ML can have at once 
\end{itemize}



\textbf{Flink}
\textbf{Overview}
\includegraphics*[width=7cm]{flink.png}
\begin{itemize}
  \item distributed system for stateful parallel data stream
  \item Treats stream as a stream
  \item Achieves microsecond latency
  \item Event driven, message queue and event logs
  \item Also has logical plan and physical plan, similar to spark
\end{itemize}


\textbf{Dataflow model}
\includegraphics*[width=7cm]{dataflow.png}

\textbf{Flink architecture}
\includegraphics*[width=7cm]{flink_overview.png}
\begin{itemize}
  \item Resouce manager is responsible for scheduling tasks to resources
\end{itemize}

\textbf{Task exectution}
\includegraphics*[width=7cm]{task_execution.png}
\begin{itemize}
  \item Task manager manages slots that processes tasks
  \item Task manager can execute tasks from diferent operators and different applications
  \item C $\rightarrow$ B, some network shuffling is done 
\end{itemize}


\textbf{Data transfer in flink}
\includegraphics*[width=7cm]{task_execution.png}
\begin{itemize}
  \item Task of an app is continuously exchanging data 
  \item Task manager takes care ofsending and echanging data 
  \item Network component of task manager buffers the records before sending
  \item Send and receive has their own buffer to reduce network traffic, so send and receive are done async (unlike micro batch)
\end{itemize}
% Tutorials 

\textbf{Event time processing (Flink)}
\begin{itemize}
  \item Every record has an accompanying time stamp
  \includegraphics*[width=7cm]{watermark_flink.png}
  \item \textbf{Watermark (flink)}
  \begin{itemize}
    \item more like a trigger mark (spark triggers after every micro batch, so latency is the size of the micro batch) -- watermark in flink determines how freqeuntly calculations are triggered!
    \item Watermark is represented as special records holding a timestamp
    \item Water mark flows in a stream of regular records
    \item Heuristic watermark: Results trigger after watermark, late records processed again later
    \item Perfect watermark: Late records included, trigger happens after 
  \end{itemize}
\includegraphics*[width=7cm]{watermark_flink2.png}
\item $withLateFiring$ determines when to drop late records
\end{itemize}

\textbf{State Management in Flink}
\includegraphics*[width=7cm]{flink_state.png}
\begin{itemize}
  \item \textbf{Operator State}
  \begin{itemize}
    \item Scoped to an operator task, cannot be accessed by other operators
    \item All records processed by the same parallel task have acess to the same state 
  \end{itemize}
  \item \textbf{Consistent Checkpointing}
  \begin{itemize}
    \item Similar to micro batch checkpoint 
    \item Pause the ingestion of all input streams 
    \item Wait for in-flight data to be completely processed (all tasks processed their inputs)
    \item Like a barrier? Need to finish the whole "microbatch"
    \item Copy and store state to a persistent storage
    \item Need to reset from latest checkpoint -- cannot achieve milisecond delay!
  \end{itemize}
  \item \textbf{Chandy-lamport algorithm}
  \includegraphics*[width=7cm]{checkpoint_flink.png}
  \begin{itemize}
    \item Distributed checkpoints
    \item Decouples checkpoiting from processing, does not pause the entire app, only some tasks pauses 
    \item Sets up a checkpoint barrier, all tasks to perform this barrier in a distributed way 
    \item After receiving the checkpoint message, the source will save their state and broadcast checkpoint barrier to the receiving nodes 
    \item Tasks will buffer the records for barrier alignment (only process once they receive the barrier msg from all sources)
    \item Only the tasks receiving the barrier will buffer
    \item After receiving all checkpoint barriers, the task will save their state
    \item Then the barrier is emitted to the next level (sink nodes)
    \item The jobManager is notified once all the tasks have completed the checkpoint 
  \end{itemize}
\end{itemize}

\textbf{Spark vs Flink}
\includegraphics*[width=7cm]{spark vs flink.png}


\subsection*{L9: Graphs and Page Ranks}
\textbf{Page Rank Algorithm}
\includegraphics*[width=7cm]{page_rank.png}
\begin{itemize}
  \item Web as a directed graph, with each node as a webpage and each edge as a hyperlink
  \item Quantify the importance of web pages on the internet
  \item Use the 'votes' from other webpages to determine the importance of a webpage
  \item The weight of each vote is determined by the importance of the webpage casting the vote
  \item \textbf{Voting Formulation}
  \includegraphics*[width=7cm]{page_rank_formulation.png}
  \begin{itemize}
    \item No unique solution
    \item All solutions are rescalings of each other 
    \item Additional constraint: $r_y + r_a + r_m = 1$
    \item Even with the additional constraint, calculating page rank for a large graph is still difficult
  \end{itemize}
  \item \textbf{Matrix Formulation}
  \includegraphics*[width=7cm]{flow eqn.png}
  \item Suppoe M is i x j, $edge_{j,i}$ is the column j voting for web page i with the weight of the vote being $M_{i,j}$
\end{itemize}

\textbf{Power Iteration Method}
\includegraphics*[width=7cm]{power_iteration.png}
\begin{itemize}
  \item Stop condition: stop when the 2 iterations differ by a small amount, convergence
  \item For some convergence or weird topology, it may not converge 
\end{itemize}

\textbf{Random Walk Formulation}
\includegraphics*[width=7cm]{random_walk.png}
\begin{itemize}
  \item Initially, the surfer has equal probability to go to anywhere on the graph 
  \item At each step, the sufer updates the probabilities depending on the node he is at 
  \item Repeat this till convergence
\end{itemize}

\textbf{Page Rank with teleport}
\begin{itemize}
  \item Motivation: page rank may not converge due to:
  \begin{itemize}
    \item Dead Ends: The surfer will always stay here, and other nodes will have 0 probability. There is convergence.
    \item Spider Traps: Cycle. Rankdom walk cannot escape the cycle. No convergence.
    \item Both will cause leaking out of important values
  \end{itemize}
  \item At each time stamp, the surfer can either 
  \begin{enumerate}
    \item Follow a link (need to factor in the probability for the link too) with probability $\beta$
    \item Jump to some random page with probability $1-\beta$
    \item Common $\beta$ value is 0.8 or 0.9
  \end{enumerate}
  \item If we are at spider trap, we will eventually escape 
  \item Dead end: we will always teleport! 
  \includegraphics*[width=7cm]{teleport_deadend.png}
  \item Updated formulation:
  \includegraphics*[width=7cm]{teleport_deadend.png}
  \begin{itemize}
    \item Chances that someone surfs from connected nodes + chances that someone randomly teleports to the node
  \end{itemize}
\end{itemize}


\textbf{Issues with page rank}
\begin{itemize}
  \item Measures generic popularity based on specific topics 
  \item soln: Topic Specific Page Rank
  \item Uses a single measure of importance 
  \item soln: hubs-and-authorities (other models of importance)
  \item Susceptibe to link spam
  \item soln: Trust Rank artificial link topologies created to boost page rank
\end{itemize}

\textbf{Topic Specific Page Rank}
\includegraphics*[width=7cm]{topic_page_rank.png}
\begin{itemize}
  \item Teleport can only go to a relevant subset (teleport set) of pages relevant to the looker's interest 
  \item S is the set of pages in the relevant set
  \includegraphics*[width=7cm]{topic_page_rank_eg.png}
  \item Node 1 has the highest probability since all the nodes in the teleport set points to it
\end{itemize}

\textbf{Characteristics of Graph Algorithms}
\begin{itemize}
  \item Local computation at each vertex 
  \item Pass message to outdegrees
  \item Algorithms are implemented from the point of view of a single vertex
\end{itemize}

\textbf{Pregel: Computation Model}
\includegraphics*[width=7cm]{compute.png}
\begin{itemize}
  \item Computation conssits of supersteps
  \item In each superstep, the framework invokes user-defined compute() for each vetex in parallel 
  \item \textbf{Compute()} at vertex v and superstep s
  \begin{itemize}
    \item Read message sent to v at s-1
    \item Send messages to neighbors at s + 1 
    \item Read or write value of v and teh value of its outgoing edges (or add remove edges)
  \end{itemize}
  \item Termination: Computation halts if all vertices are inactive 
  \item Can be woken up 
\end{itemize}

\textbf{Pregel: Implementation}
\includegraphics*[width=7cm]{pregel_impl.png}
\begin{itemize}
  \item Master and Worker Architecture
  \item Vertices are  hash partitioned and assigned to workers (edge cut) 
  \item Worker maintains its the state of its portion in memory 
  \item Computation happens in memory
  \item At every superstep, each worker loops through its vertices and calls compute 
  \item Messages from vertices can be sent to the same or different worker (buffered locally and sent as a batch to reduce network traffic)
  \item \textbf{Fault Tolerance} 
  \begin{itemize}
    \item Checkpointing to persistent storage 
    \item Failure detected through heartbeats 
    \item Corrupt workers are reassigned and reloaded from checkpoints
  \end{itemize}
\end{itemize}

\textbf{Pregel: Page Rank}
\includegraphics*[width=7cm]{pregel_pagerank.png}
\includegraphics*[width=7cm]{pregel_pagerank2.png}

\textbf{Pregel: Spark}
\includegraphics*[width=7cm]{spark_pregel.png}
\includegraphics*[width=7cm]{spark_pregel2.png}

\subsection*{L10: Delta Lake}

\textbf{Data warehouse}
\begin{itemize}
  \item Central relational repository of itnegrated, historical data from multiple data sources
  \item Reliable, high quality, strong ACID guarantees 
  \item However, expensive to scale out and do not support non-sql based analytics
\end{itemize}

\textbf{Data lake}
\includegraphics*[width=7cm]{datalake_choices.png}
\begin{itemize}
  \item Distributed, Cost effective central repository to store data at scale
  \item Support many formats
  \item Decouples storage and compute systems 
  \item \textbf{Pros}
  \begin{itemize}
    \item Flexibility on choosing storage
    \item Cheap 
  \end{itemize}
\item \textbf{Cons}
\begin{itemize}
  \item No ACID guarantees
  \item Difficult to maintain 
  \item Easy to ingest data but expensive to transform data to deliver insights 
  \item Data quality issues 
\end{itemize}
\end{itemize}

\textbf{Data lakehouse}
\includegraphics*[width=7cm]{data_lakehouse.png}
\begin{itemize}
  \item Combines data lake and warehouse 
  \item ACID + low cost + flexibility
  \item Good for business insights and machine learning 
\end{itemize}

\textbf{Delta Lake}
\begin{itemize}
  \item Metadata, caching, indexing layer on top of a data lake storage 
  \item Serves:
  \begin{itemize}
    \item ACID 
    \item FULL Data Maniuplation Language (DML) support 
    \item Audit history 
    \item Unification of batch and streaming into one processing model
    \item Schema enforcement and evolution 
    \item Rich metadat and scaling
  \end{itemize}
\end{itemize}

\textbf{Delat Lake Format}
\includegraphics*[width=7cm]{delta_log.png}
\includegraphics*[width=7cm]{parquet1.png}
\includegraphics*[width=7cm]{parquet2.png}
\begin{itemize}
  \item Standard parquet file with additional metadata 
  \item Uses delta log as a single source of truth and enables concurrent read and writes and atomic transactions and commits 
  \item After 10 JSON files (logs), the logs are check pointed to reduce the number of logs read and support fault tolerence
\end{itemize}


% Misc

\end{multicols}
\end{document}