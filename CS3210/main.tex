\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{ifthen}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage[document]{ragged2e}
\usepackage{listings}
\setlist{nosep}
\usepackage{subfig}
\usepackage{listings}

% Define Rust language for listings package
\lstdefinelanguage{Rust}{
  morekeywords={let, mut},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}

% for including images
\graphicspath{ {./images/} }


\pdfinfo{
  /Title (CS3210.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Pei Cheng Yi)
  /Subject (CS3210)
  /Keywords (CS3210, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

\newenvironment{tightcenter}{%
  \setlength\topsep{0pt}
  \setlength\parskip{0pt}
  \begin{center}
}{%
  \end{center}
}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}%
\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}
%  convenient absolute value symbol
\newcommand{\abs}[1]{\vert #1 \vert}
%  convenient floor and ceiling
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
%  convenient modulo
\newcommand{\Mod}[1]{\ \mathrm{mod}\ #1}
%  for logical not operator, iff symbol, convenient "if/then"
\renewcommand{\lnot}{\mathord{\sim}}
\let\then\Rightarrow
\let\Then\Rightarrow
%  vectors
\newcommand{\vv}[1]{\boldsymbol{#1}}
\newcommand{\VV}[1]{\overrightarrow{#1}}
%  column vector
\newcommand{\cvv}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}
\newcommand{\code}[1]{\textcolor{myblue}{\texttt{#1}}}
\newcommand\bggreen{\cellcolor{green!10}}

\makeatother
\definecolor{myblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{myblue}}
% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% this changes all items (enumerate and itemize)
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.4cm}
\setlength{\leftmarginiii}{0.5cm}
\setlist[enumerate,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,3]{leftmargin=3mm,labelindent=1mm,labelsep=1mm}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------
 
\begin{document}
\raggedright
\footnotesize
\begin{multicols}{4}


% multicol parameters
% These lengths are set only within the two main columns
\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \fbox{%
        \parbox{0.8\linewidth}{\centering \textcolor{black}{
            {\Large\textbf{CS3210}}
            \\ \normalsize{AY23/24 Sem 1}}
            \\ {\footnotesize \textcolor{myblue}{github.com/SeekSaveServe}}
        }%
    }
\end{center}

% LECTURES

\section{Lectures}
\section{Introduction}
\subsection{L0 and L1}
\textbf{Program Parallelization} \\
\textcolor{blue}{\textbf{Decomposition}}: Decompose a sequential algorithm into tasks (programmer)\\
\begin{itemize}
    \item Granularity of tasks are important
    \item Tasks have dependencies (data or control) between each other which defines the execution order
\end{itemize}
\textcolor{red}{\textbf{Scheduling}}: Assign tasks to processes (programmer / compiler)\\
\textcolor{green}{\textbf{Mapping}} - Map processes to cores (OS)\\

\textbf{Von Neumann Computation Model} instruction and data are stored in memory, and processors computes. \\
\textbf{Memory Wall} disparity between memory speed and processor speed ($\le$ 1 ns VS $\ge$ 100 ns)\\   
\textbf{Processing unit} refers to a core that can execute a kernel thread \\
\textbf{Interconnect} busses betwen different components in the machine \\
\textbf{Node} Machine in a distributed system\\

\textbf{Why Parallel} \\
\textbf{Primary Reasons}
\begin{itemize}
    \item [1] OVercome limits of serial computing
    \item [2] Solve larger problems
    \item [3] Save (wall-clock) time
\end{itemize}
\textbf{Other Reasons}
\begin{itemize}
    \item Take advantage of non-local resources 
    \item Cost/energy saving - use multiple cheaper computing resourcees 
    \item Overcome memory constraints
\end{itemize}

\textbf{Computational Model Attributes} \\
\begin{itemize}
    \item \textbf{Operation mechanism} Primitive units of computation or basic actions of the computer on a specific Architecture 
    \item \textbf{Data Mechanism} How we access and store data in address space 
    \item \textbf{Control Mechanism} How primtive units of computation are scheduled
    \item \textbf{Communication Mechanism} Modes and patterns of exchanging information between parallel tasks (e.g message passing, shared memory)
    \item \textbf{Synchronization Mechanism} ensures to ensure needed information arrives at the right time
\end{itemize}

\textbf{Dependencies and Coordination}
\begin{itemize}
    \item Dependencies among tasks impose constraints on scheduling 
    \item Memory organizations: Shared-memory (threads), distributed-memory (processes) 
    \item Coordination (synchronisation) imposes additional overheads
\end{itemize}

\textbf{Two algorithms}
\includegraphics*[width=7cm]{l1_1.png}
\begin{itemize}
    \item Core 0 is active throughout the execution
    \item Some cores are idle
    \item This is a lot better than having all cores idle while the master core is executing
\end{itemize}


\textbf{Parallel Performance} 
\begin{itemize}
    \item Execution time Vs Throughput
    \item Parallel execution time = computation time + parallelization overheads 
    \item Overheads: Distribution of work(tasks) to porocesses, information exchange, synchronisation, idle time, etc
\end{itemize}

\section{Background on Parallelism}
\subsection*{L2: Processes and Threads}
\textbf{Process}
\begin{itemize}
    \item Identified by PID 
    \item Program counter, global data (open files, network connections), stack or heap, current values of the registers (GPRs and Special)
    \item These information are abstracted in the PCB, and each proecss can be viewed as having exclusive access to tis address space 
    \item Explicit communication is needed
    \item \textbf{Disadvantage}
    \begin{enumerate}
        \item High overhead of system calls
        \item Potential re-allocation of data-structures
        \item Communication goes through OS (system calls) and context switch is costly
    \end{enumerate}
\end{itemize}

\includegraphics*[width=7cm]{memory_space.png}

\textbf{Multi tasking}
\begin{itemize}
    \item Overhead: Context switching (PCB change) is needed and states of suspended process must be saved 
    \item Time slicing: Pseudo-parallelism
    \item Child processes can use parent's data
\end{itemize}

\textbf{Inter-process communication (IPC)}
\begin{itemize}
    \item Shared memory: need to protect access with locks 
    \item Message passing: Blocking, unblocking, Synchronous, unsynchronous
\end{itemize}

\includegraphics*[width=7cm, height=3.8cm]{except_interrupt}

\textbf{Threads}
\begin{itemize}
    \item A process may have multiple indepedent control flows called threads 
    \item Each thread has its own stack and registers (PC, SP, registers), but share the same address space 
    \item Shared memory model and Shared memory architecture
    \item Faster thread generation- no copy of  address space
    \item Different process can be assigned to run on different cores of a multicore processor
    \item \textbf{User threads}
    \begin{itemize}
        \item Managed by library 
        \item Context siwtch is fast, OS not involved
        \item \textbf{Disadvantage}
        \begin{enumerate}
            \item OS cannot map different threads of the same process to different resources $\Rightarrow$ No parallelism
            \item OS cannot switch to another thread if one thread blocks
        \end{enumerate} 
    \end{itemize}
    \item \textbf{Kernel threads}
    \item OS is aware of the threads and can manage accordingly
    \item Efficient in a multicore system
    \item Potential synchornisation issues
\end{itemize}

\textbf{Many to one mapping}
\begin{itemize}
    \item All user-level threads mapped to one process. 
    \item Efficiency depends on threading library
\end{itemize}

\textbf{One to one mapping}
\begin{itemize}
    \item Each user-level thread is mapped to one kernel thread
    \item OS schedules 
\end{itemize}

\textbf{Many to many mapping}
\begin{itemize}
    \item Many user-level threads mapped to many kernel threads
    \item Library threads has overheads, and kernel threads has overheads
    \item At different points in time, different user threads are mapped to different kernel threads
    \item Number of threads must be suitable to the degree of parallism and the resources available
\end{itemize}

\textbf{Locks}
\begin{itemize}
    \item Spinlock: busy wait 
    \item Blocking: mutex 
    \item Using more locks increasese the number of context switches 
    \item DO NOT wait in the critical section
\end{itemize}

\textbf{Semaphores}
\begin{itemize}
    \item Essentially shared global variables 
    \item Can be potentially accessed anywhere in program 
    \item No connection between semaphone and the data being protected 
\end{itemize}

\textbf{Barrier}
\begin{itemize}
    \item All threads must reach the barrier before any thread can proceed
\end{itemize}

\textbf{Deadlock}
\begin{itemize}
    \item Deadlock exists among a set of processes if every process is waiting for an event that can be caused only by another process in the set
    \item \textbf{iff these conds are met}
    \begin{enumerate}
        \item Mutual exclusion-at least one resource is not shareable 
        \item Hold and wait - at least one process holding a resource and waiting for another 
        \item No preemption - crticial section cannot be aborted externally 
        \item Circular wait
    \end{enumerate}
    \item \textbf{Dealing with deadlock}
    \item Ignore it, prevent it, avoid it by controlling resource allocation, detection and recovery by breaking cycles
\end{itemize}

\textbf{Starvation}
\begin{itemize}
    \item Side effect of the scheduling algorithm. Lower priority processes might starve
\end{itemize}

\textbf{Livelock}
\begin{itemize}
    \item Active acquire release but no useful work done 
\end{itemize}
 
% Taken from my CS3211 notes
\textbf{Producer-Consumer Problem} \\
\begin{itemize}
    \item Specifications:
    \begin{itemize}
        \item Producers put in a shared bounded buffer if not full, consumers read from it if not empty
    \end{itemize}
    \item Solution:\\
    \includegraphics*[width = 7cm, height = 3cm]{producerConsumer.png} \\ 
    \item Concurrent read, exclusive write. Categorical starvation of writer is possible\\ 
    \includegraphics*[width = 7cm, height = 3cm]{readerwritter1.png} \\
    \item Light switch: Abstracts out the shared lock for the reader 
    \includegraphics*[width = 3cm, height = 3cm]{lightswitch.png} \\
    \item Starvation free solution (block out readers): \\
    \includegraphics*[width = 7cm, height = 3cm]{turnstile.png} \\
    \item Prioritise Writer: \\
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
readSwitch = lightswitch();
writeSwitch = lightswitch();
noReaders = semaphore(1);
noWriters = semaphore(1);

reader() {
/* Waiting for writers to be done */ 
wait(noReaders);
/* Writers cannot enter */
    readswitch.lock(noWriters);
signal(noReaders);
    # critical section
readSwitch.unlock(noWriters);
}

writer() {
/* Immediately acquires no readers so writers have priority */
writeSwitch.lock(noReaders);
    wait(noWriters);
    # critical section
    signal(noWriters);
writeSwitch.unlock(noReaders);
}
\end{lstlisting}
\item This is implemented in C++ as a \textbf{shared\_lock} and \textbf{unique\_lock}
\item GO has something similar: readLock and writeLock
\end{itemize}

\textbf{Barrier} \\
\begin{itemize}
    \item All threads must stop at a common point before proceeding, can be reusable (barrier) or single use (latch)
    \item std::barrier, std::latch in C++
    \item E.g. std::barrier arrivalPoint(size) ... arrivalPoint.arrive$\_$and$\_$wait()
    \item sync.WaitGroup in GO is a latch, we can use 2 of them to make a barrier
    \item C++ implementation
    \begin{itemize}
        \item The naive version fails because context switch can happen right before counter == N, which causes multiple threads (that were context switched out after counter ++) to signal the switch (another way to fail is to have 1 thread lap everyone else between the first barrier unlocks and second barrier unlocks)
        \item The solution is to add a second turnstile (initialised as 1) to guard the turnstile1.signal, such that only one thread can signal it
        \item But using mutex to increment turnstile1 one-by-one is slow 
        \item So we use a counting semaphore instead so we can raise the barrier by 1 thread!  \\ 
        \includegraphics*[width = 7cm, height = 3cm]{barrier.png}
    \end{itemize}
\end{itemize}

\textbf{Dining Philosophers} \\
\begin{itemize}
    \item Specifications: N philosophers, N chopsticks
    \item Deadlock: All pick up left simultaneously
    \item Livelock: Put down left if right cannot be acquired
    \item Slap a mutex: Becomes sequential 
    \item Scoped Lock(left, right): Acquire multiple mutexes in a deadlock free manner (deadlock avoidance), but as we have seen in CS3223, deadlock avoidance can lead to livelock
    \item GO's Mutex Free Solution: Use odd-even ring communication, odd numbered philosophers pick up left first, even numbered philosophers pick up right first
    \item This is the same as the right hander argument 
    \item Tanenbaum's solution: 
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
#define N 5
#define LEFT ((i+N-1)% N)
#define RIGHT ((i+1) % N)
#define THINKING 0
#define HUNGRY 1
#define EATING 2
int state[N];
Semaphore mutex = 1;
Semaphore s[N];

void philosopher( int i ){
    while (TRUE){
        Think( );
        takeChpStcks( i );
        Eat( );
        putChpStcks( i );
    }
}
void safeToEat(i)
{
    if( (state[i] == HUNGRY) && 
    (state[LEFT] != EATING) && 
    (state[RIGHT] != EATING) ) {
    state[i] = EATING;
    signal(s[i]); 
    }
}

void takeChpSticks( i )
{
    wait(mutex);
    state[i] = HUNGRY;
    safeToEat(i);
    signal(mutex);
    wait(s[i]);
}

void putChpSticks(i)
{
    wait(mutex);
    state[i] = THINKING;
    safeToEat(LEFT);
    safeToEat(RIGHT);
    signal(mutex);
}
    \end{lstlisting}
    \item Limited seats: Use a semaphore(N-1) to limit the number of philosophers that can eat at the same time \\ 
    \includegraphics*[width=7cm, height=3cm]{godiningphilo.png}
\end{itemize}

\textbf{Barber Shop} \\
\begin{itemize}
    \item Barbershop consists of a waiting room with n chairs and the barber chair 
    \item If there are no customers to be served, the barber goes to sleep
    \item If the barber is busy, but waiting room is available, customer seats on one of the chairs 
    \item If barber is sleeping, customer wakes him up 
    \item If all chairs are occupied, customer leaves
    \includegraphics*[width=7cm, height=3cm]{barber.png}
    \item line 31-32 are important since we need customer and barber need to agree that the haircut is done
    \item GO implementation:
    \includegraphics*[width=7cm, height=3cm]{gobarber.png}
\end{itemize}



\section*{Architecture}
\subsection*{L3: Processor and memory organization}

\textbf{Single Processor Parallelism}
\begin{itemize}
    \item Bit level - we work with word (multiple bits), data parallelism
    \item Instruction level (from same thread)
    \begin{enumerate}
        \item Pipelining - parallelism across time
        \begin{itemize}
            \item Multiple instructions to occupy different stages in the same clock cycle - assuming no control or data dependencis
            \item \textbf{Disadvanatges}
            \begin{enumerate}
                \item Independence 
                \item Bubbles - idle stages
                \item Data and control flow hazard
            \end{enumerate}
            \item Wrong speculation of if-else branches can lead to wasted cycles
            \item Synchronisation - need to preserve read-after-write 
            \item no more benefit to improving ILP now

        \end{itemize}
        \item Superscalar - parallelism across space
        \includegraphics*[width=7cm]{superscalar}
        \begin{itemize}
            \item Duplicate pipelines and allow multiple instructions to pass through the same stage
            \item Scheduling tough - which ones to execute together?
            \item E.g Multiple ALUs
            \begin{enumerate}
                \item Static - compiler decides
                \item Dynamic - hardware decides
            \end{enumerate}
        \end{itemize}
    \end{enumerate}
    \item Thread level
    \begin{itemize}
        \item Motivated by the limitation of ILP 
        \item SMT: Duplicate hardware context (PC, registers etc)
        \item By convention, SMT is limited to 2 threads to reduce overhead and memory contention 
        \item Logical cores: hyperthreads
    \end{itemize}
\end{itemize}

\textbf{Processor level parallism}
\begin{itemize}
    \item Add more cores to processors to enable \textbf{multiple execution flows}
    \item Each core can be hyperthreaded
    \item Shared Memory 
    \item Distributed Memory 
\end{itemize}

\includegraphics*[width=7cm]{coarse}

\textbf{Flynn's Taxonomy}
\begin{itemize}
    \item Describes parallel architecture based on instruction stream (execution flow - PC) and data stream
\end{itemize}

\textbf{Single Instruction Single Data}
\begin{itemize}
    \item Single stream of instructions with each working on a single data 
    \item Not to be confused with SIMD from parallel patterns
\end{itemize}

\textbf{Single Instruction Multiple Data}
\begin{itemize}
    \item Single stream of instructions with each working on multiple data 
    \item Exploit data parallelism (vector processor)
    \item Same instruction broadcasted to all ALUs
    \item AVX: intrinsic functions operatie on vectors of 4 64 bit values 
\end{itemize}

\textbf{Multiple Instruction Single Data}
\begin{itemize}
    \item Multiple instructions operating with a single data 
\end{itemize}

\textbf{Multiple Instructions Multiple Data}
\begin{itemize}
    \item Each PU fetches its  own instructions 
    \item Each PU operates its own data 
    \item  
\end{itemize}
\includegraphics*[width=7cm]{mimd}

\textbf{Hierarchical designs}
\includegraphics*[width=7cm]{hierarchical_design}
\begin{itemize}
    \item Each core can have a separate L1 cache and shares the L2 cache
    \item All cores share common external memory 
\end{itemize}

\textbf{Pipelined design}
\includegraphics*[width=7cm]{pipelined_design}
\begin{itemize}
    \item Multiple packets being processed in a pipelined fashion 
    \item Cores connected linearly, shares the same cache, memory 
    \item Useful if the same computation has to be applied to a long sequence of data elements
\end{itemize}

\textbf{Network-based design}
\includegraphics*[width=7cm]{network_design}
\begin{itemize}
    \item Cores and their local memory and memories are connected via an interconnection network
\end{itemize}

\textbf{Why cache}
\begin{itemize}
    \item cache provides high bandwidth data transfer to CPU and reduce latency in data access 
    \item Memory latency: Amount of time for a memory request from a procesor to be serviced 
    \item Bandwitdth: Rate at which the memory system can provide data to a processor 
    \item A stall happens when the next instruction depends on previous instructions
    \item Bandwidth and latency affects stalls, since instructions (sw, lw) needs to wait for the memory system to become available
\end{itemize}

\textbf{Performant parallel programs}
\begin{itemize}
    \item Try not to overload the memory system with too many requests
    \item Share data across threads (inter-thread cooperation)
    \item Reuse data fetched previously (temporal locality)
    \item \textbf{Favor additional arithmatic over load /\ store}
\end{itemize}

\includegraphics*[width=7cm]{parallel_memory}


\textbf{Cache coherence}
\begin{itemize}
    \item Multiple copies of data exist on different caches 
    \item Local updates should not be seen by other processes 
    \item Maintained by additional insructions 
    \item Instructions that mess up cache coherence hence presents severe overheads
\end{itemize}

\textbf{Memory consistency}
\begin{itemize}
    \item Memory consistency depends on the PL and architecture 
    \item A seq consistent architecture makes a PL with seq const memory model run faster since fewer instructions are needed to ensure memory consistency
\end{itemize}


\textbf{Distributed Memory }
\includegraphics*[width=7cm]{distributed_memory}
\begin{itemize}
    \item Each node is an independent unit with processor and memory 
    \item Memory in each node is private 
    \item Nodes communicate through a network 
\end{itemize}

\textbf{Shared memory}
\includegraphics*[width=7cm]{i7_shared}
\begin{itemize}
    \item Parallel programmes share memory through controller /\ provider
    \item Cache coherence and memory consistency is ensured 
\end{itemize}


\includegraphics*[width=7cm]{uma}
\textbf{Uniform  Memory Access}
\begin{itemize}
    \item Latency of accessing main memory is the same for processors 
    \item Suitable for samll number of processors. Contention over memory can be high for large number of processes 
\end{itemize}

\includegraphics*[width=7cm]{numa}
\textbf{Non-uniform Memroy Access}
\begin{itemize}
    \item Physically distributed memory of all processing elements are combined to form a global shared memory
    \item Local memory access has lower latency
    \item Reduce contention since each processor tend to access local memory 
    \item Adding more processes does not increase contention as much as UMA 
    \item Data consistency is easier too
\end{itemize}

\textbf{Cache Coherent NUMA (CCNUMA)}
\begin{itemize}
    \item EAch node has cache to reduce contention
\end{itemize}

\includegraphics*[width=7cm]{coma}
\textbf{Cache only Memory Architecture (COMA)}
\begin{itemize}
    \item Each memory blocks works as cache memory. This means that no fixed space stores data permanently and cache block with data can be moved around dynamically.
    \item Data migrates dynamically to keep data as close as possible to the processors
    \item Cache coherence is harder since data may not just be copied, they can also be shifted around.
\end{itemize}


\subsection*{L7: Cache coherence and memory consistency}

\textbf{Cache properties}
\begin{itemize}
    \item Larger cache reduces cache miss but increases access time 
    \item Block size (cache line): data is transferred between main memory and cache in blocks of fixed size 
    \item Larger block size -- greater spatial locality 
    \item Smaller block size -- shorter replacement 
\end{itemize}

\textbf{Case Study: Matrix Multiplication}
\begin{itemize}
    \item Size of matrix: A 256KB cache can only store a matrix of floats of size (178*178) * 8 Bytes (float size)
\end{itemize}

\textbf{Write Policy}
\begin{itemize}
    \item \textbf{Write through}
    \begin{itemize}
        \item Write access is immediately transferred to memory
        \item Advantage: always get the newest value of a bloc
        \item Disadvanatge: slow down due to many memory access (use a buffer!)
    \end{itemize}
    \item \textbf{Write-back}
    \includegraphics*[width=5cm]{write_back_cache.png}
    \begin{itemize}
        \item Write is only performed in the cache, write to main memory is only performed when the cache is replaced (dirty bit)
        \item Advantage: fewer write operations 
        \item Disadvantage: memory may contain invalid entries  
    \end{itemize}
\end{itemize}

\textbf{Cache coherence}
\begin{itemize}
    \item Problem: Multiple copies of the data exists on different cache lines, stale data may exist
    \item \textbf{Coherence}
    \begin{itemize}
        \item Each processing unit should have a consistent view of the memory through its local cache 
        \item All processing units should agree on the order of read writes to the same memory space 
        \item Property 1: Program Order Property 
        \begin{itemize}
            \item Programme should observe the effects of writes in the order of the programme
        \end{itemize}
        \item Property  2: Write propagation
        \begin{itemize}
            \item Writes become visible to other processing units eventually
        \end{itemize}
        \item Property 3: Write serialization
        \begin{itemize}
            \item Given: 
            \begin{enumerate}
                \item write $v_1$ to x 
                \item write $v_2$ to x 
            \end{enumerate}
            \item programme should never read x as $v_2$ and then as $v_1$
            \item All writes to a location are seen in the same order by all execution units, eventually
        \end{itemize}
    \end{itemize}
\end{itemize}

\textbf{Tracking cache line sharing status}
\begin{itemize}
    \item \textbf{Snopping based}
    \begin{itemize}
        \item No centralised directory 
        \item Each cache keeps track of the sharing status 
        \item Cache monitors and snoop on the bus to keep the cache line updated 
        \item Used in architectures with a bus
        \item Write Propagation: All the processing units on the bus can observe changes made by every other bus 
        \item Write serialization: Bus transactions are visible to the processing units in the same order 
        \item Granularity: cache block
    \end{itemize}
    \item \textbf{Directory based}
    \begin{itemize}
        \item Sharing status is kept in a central directory 
        \item Commonly used in a NUMA architecture
    \end{itemize}
    \item \textbf{Implications}
    \begin{itemize}
        \item Increased in overhead: increased memory latency, reduced cache hit rate 
        \item Cache ping-pong: the effect where a cache line is transferred between multiple cores as a result of true / false sharing
        \item False sharing: different threads have data that is not shared in the program, but this data gets mapped to the same cache line  
        \item False sharing makes cache ping pong difficult to detect, since the code ensures that memory are not shared but they happened to be mapped to the same cache line
        \includegraphics*[width=6cm]{false_sharing.png}
    \end{itemize}
\end{itemize}

\textbf{Memory Consistency Models}
\begin{itemize}
    \item Coherence ensures that processing units agree on the order of writes on the SAME memory location, and that all writes to shared memory will eventually propagate 
    \item Consistency ensures that processing units agree on the order of writes on DIFFERENT memory locations 
    \item Under the consistency rules, the instructions can be reordered to hide latencies
    \item \textbf{4 types of memory operations orderings}
    \begin{itemize}
        \item must commit -- the results are visible
        \item W $\rightarrow$ R: write to X must commit before the subeqeuqnt read of Y
        \item R $\rightarrow$ W: read of X must commit before the subsequent write of Y
        \item R $\rightarrow$ R: read of X must commit before the subsequent read of Y
        \item W $\rightarrow$ W: write to X must commit before the subsequent write of Y
    \end{itemize}
\end{itemize}

\textbf{Sequential Consistency}
\begin{itemize}
    \item Every processing unit isues their memory operations in programme order 
    \item Global results of all memory operation on every memory address appear in the same sequential order to evry processing unit
    \item All 4 memory operation orderings are observed
    \item Poor performance
    \item Examples: 
    \includegraphics*[width=7cm]{memory_model_eg.png}
    \begin{itemize}
        \item \textbf{Once 1 core sees an interleaving, the same interleaving will be observed by other cores}
        \item Possible interleavings 
        \begin{enumerate}
            \item (1)-(3)-(5)-(2)-(4)-(6) 
            \item (1)-(2)-(3)-(4)-(5)-(6) 
        \end{enumerate} 
        \item Impossible output: 011001
        \item To produce 0110, we need something like (1)-(3)-(2)-(4). But after this it is not possible to produce another 0 since the last read statements happens after all the write statements
    \end{itemize}
\end{itemize}

\textbf{Relaxed memory consistency}
\begin{itemize}
    \item Relax if data dependencies allow 
    \item \textbf{Data dependency: if two ops access the SAME memory location}
    \begin{itemize}
        \item R $\rightarrow$ W
        \item W $\rightarrow$ W
        \item W $\rightarrow$ R 
    \end{itemize}
\end{itemize}

\textbf{Relaxed Consistency: Write-to-read (WR)}
\begin{itemize}
    \item Allows a read on processing unit P to be reorderd wrt the previous write operations on different memory locations 
    \item Data dependencies must be observed, but it is only wrt the same memory location
    \item Data dependencies cannot be chained
    \item Different models depends on the timing of return
    \item \textbf{Total Store Ordering}
    \begin{itemize}
        \item Processing units can \textbf{move its own reads} infront of its own writes 
        \item \textbf{Write Atomicity is observed}: Reads by other processing units cannot return new values of address A untill the write to A is observed by all PUs 
    \end{itemize}
    \item \textbf{Processor Consistency}
    \begin{itemize}
        \item \textbf{Write atomicity is not observed}: write can be read by some processing units before they are read by other processing units
        \item \textbf{Write serialization is observed}: writes to the same memory location are seen in the same order by all processing units
    \end{itemize}
\end{itemize}

\includegraphics*[width=1.5cm]{mem_order.png}

\textbf{Relaxed Consistency: Write-to-Write (WW)}
\begin{itemize}
    \item Writes can bypass earlier writes (to different locations) in write buffer 
    \item Allows write misses to overwrite to hide latency
    \item Can only reorder within the same processing unit 
    \item \textbf{Partial Store Order}
    \begin{itemize}
        \item Relax W $\rightarrow$ R similar to TSO 
        \item Relax W $\rightarrow$ W
    \end{itemize}
    \item \textbf{Example 1}
    \includegraphics*[width=7cm]{mem_model_eg2.png}
    \begin{itemize}
        \item Only PSO can observe A=0, B=1 since only it reorders WW
    \end{itemize}

    \item \textbf{Example 2}
    \includegraphics*[width=7cm]{mem_model_eg1.png}
    \begin{itemize}
        \item TSO and SC cannot observe A=0 due to write atomicity 
        \item PC can observe A=0, since observing B=1 does not mena that it has observed A=1 (not write wtomicity)
        \item PSO cannot observe (0,0) since it still observes write atomicity
    \end{itemize}
\end{itemize}

\includegraphics*[width=7cm]{mem_model_summary.png}


\subsection*{L11: Interconnection networks}

\textbf{Torus}
\includegraphics*[width=7cm]{torus.png}
\begin{itemize}
    \item Wraps left to right, top to bottom 
    \item All PEs have four links
\end{itemize}


\textbf{Shear Sorting Algo}
\includegraphics*[width=7cm]{shear_sort.png}
\begin{itemize}
    \item Arrange PEs into a 2D mesh 
    \item Sort into a "snake"
    \includegraphics*[width=2cm]{snake.png}
    \item $\sqrt{N}$ width and length
    \item Keep doing row sort and column sort till it is sorted in snake order!
    \item for N numbers, $log_2N + 1$ phases
\end{itemize}

\textbf{Topology}
\includegraphics*[width=7cm]{interconnect.png}
\begin{itemize}
    \item \textbf{Direct Interconnection}
    \begin{itemize}
        \item Static, point-to-point 
        \item Endpoints are of the same type (core, memory)
    \end{itemize}
    \item \textbf{Indirect Interconnection}
    \begin{itemize}
        \item Dynamic 
        \item Interconnect formed by switches
    \end{itemize}
\end{itemize}

\textbf{Topology Metric}
\begin{itemize}
    \item \textbf{Diameter}: maximum distance between any pair of nodes 
    \begin{itemize}
        \item Small diameter ensures small distances for message transmission
    \end{itemize}
    \item \textbf{Degree}: number of direct neighbors 
    \begin{itemize}
        \item Small node degree reduces the node hardware overhead
    \end{itemize}
    \item \textbf{Bisection width}: minimum number of edges that must be removed for the network to have 2 equal parts 
    \begin{itemize}
        \item Capacity of a network when transmitting message simultaneously
    \end{itemize}
    \item \textbf{Connectivity} 
    \begin{itemize}
        \item \textbf{Node connectivity}: minimum number of nodes that fail to disconnect the network
        \item Determines robustness of network
        \item \textbf{Edge connectivity}: minimum number of edges that fail to disconnect the network
        \item Determine the number of independent paths between any pair of nodes
    \end{itemize}
\end{itemize}


\textbf{Cube Connected Cycles}
\includegraphics*[width=7cm]{ccc.png}
\begin{itemize}
    \item Replace a node with a cycle of k-nodes 
    \item Total nodes: $2*2^k$ nodes 
    \item X=node index in hypercube 
    \item Y=position in the cycle 
    \item Node (X,Y) is connected to: 
    \begin{itemize}
        \item Cycle buddies:
        \item (X,(Y+1) mod k)
        \item (X,(Y-1) mod k)
        \item Link from the corresponding dimension y in the hypercube
        \item (X xor $2^{y}$, Y)
    \end{itemize}
\end{itemize}

\textbf{Indirect Interconnect}
\begin{itemize}
  \item To reduce hardware costs by sharing switches and links 
  \item Switches share links between nodes and can be configured dynamically 
  \item Cost: number of switches and links 
  \item Concurrent connection (bandwidth)
  \textbf{Bus Network}
  \includegraphics*[width=7cm]{bus_network.png}
  \begin{itemize}
    \item Only 1 pair can communicate at a time
    \item Bus used to coordinate 
    \item Used for small number of processes
  \end{itemize}
\end{itemize}
    
\textbf{Crossbar network}
\includegraphics*[width=7cm]{cross_bar.png}
\begin{itemize}
    \item Switche: straight or direction changing 
    \item Hardware costly since there's n*m switches
    \item Hard to scale since cost increases exponentially 
    \item For small number of processes
\end{itemize}


\textbf{Multistage Interconnection Network (MIN)}
\includegraphics*[width=7cm]{min.png}
\begin{itemize}
    \item Several intermediate switches connecting wires between neighbouring switches
    \item Goal: obtain a small distance for arbitrary small input and output devices
\end{itemize}

\textbf{Omega Network}
\includegraphics*[width=7cm]{omega_network.png}
\begin{itemize}
    \item One unique path for every input to output 
    \item n * n network has log n stages
    \item n / 2 switches per stage 
    \item Switch position: $(\alpha, i)$ where a is the position of a switch within a stage and i is the stage number 
\end{itemize}

\textbf{Omega Network Vs Crossbar}
\begin{itemize}
    \item 16 processors and 16 memory nodes 
    \item Cross bar: 16 * 16 = 256
    \item Omega: n=16, using 2*2 switches, n/2  * logn = 32 switches
\end{itemize}

\textbf{Butterfly network}
\includegraphics*[width=7cm]{butterfly.png}

\textbf{Baseline network}
\includegraphics*[width=7cm]{baseline.png}


\section*{Parallel Computation Models}
\subsection*{L4: Shared-memory programming models}
\textbf{Parallelism}
\begin{itemize}
    \item Average number of units of work that can be performed in parallel per unit time. 
    \item E.g. MIPS, MFLOPS 
    \item Limitation: Program dependencies - data, control 
    \item Runtime delays - memory contention, communication overheads, thread overhead, synchronisation
    \item We cannot reorder them however we like
    \item Work = Task + dependencies (limitations)
\end{itemize}

\textbf{Data parallelism}
\begin{itemize}
    \item If iterations are \textbf{independent}, they can be executed in arbitrary order on multiple cores
    \item Patition data among processing units, each doing similar work 
    \item Commonly expressed as a loop, if the iterations are independent and can be executed in arbitrary order
    \item E.g. SIMD computers
    \item \textbf{OpenMP - matrix multiplication} 
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
// parallelize result = a x b
// each thread works on one itreration of the outer-most loop 
// vars (a, b ,result) are shared
#pragma omp parallel for num_thread(8)
    shared(a, b, result) private(i, j ,k)
    ...
        \end{lstlisting}
    \item \textbf{Same as}
    \begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
for (i=0; i < size; i ++) 
    for (j=0; j < size; j ++) 
        for (k=0; k < size, k ++) 
            result[element][i][j] += a.element[i][k] * b.element[k][j]
    \end{lstlisting}
    \item \textbf{Single Program Multiple Data (SPMD)}
    \begin{itemize}
        \item Same programme may behave differently based on the data
        \item Good if 
        \item E.g. Scalar product of $x.y$ on p processing units 
    \end{itemize}
\includegraphics*[width=7cm]{spmd}
\end{itemize}


\textbf{Task parallelism}
\begin{itemize}
    \item Partition the tasks among the processing units
    \item indepedent program tasks/\ parts can be executed in parallel
    \item Granularity: statement, loop, function 
    \item More complexed than data parallelism $\rightarrow$ needs to schedule, map, take care of dependencies ...
    \item \textbf{Decomposition}
    \begin{itemize}
        \item The room for parallelism in a task depends on how the task is decomposed
    \end{itemize}
\end{itemize}

\includegraphics*[width=7cm]{decomp_a}
\includegraphics*[width=7cm]{decomp_b}

\textbf{Task dependence graph}
\begin{itemize}
    \item DAG: node=tasks, value=expected execution time, edge=condtrol dependency
    \item Bad for one process to take disproportionately more data $\rightarrow$ idle time   
    \item Critical path length: maximum slowest completion time 
    \item Degree of concurrency=total work/critical path length
\end{itemize}
\includegraphics*[width=7cm]{task_dependence}

\includegraphics*[width=7cm]{types_parallelism}

\textbf{Coordination: Shared memory}
\begin{itemize}
    \item Protect access to shared address space, mutex.
    \item Needs hardware support to implement efficiency. NUMA makes it easier but it is still costly to scale due to contention (any processor can load/\ store to any address)
    \item Can be done without a shared memory system (NUMA, UMA)
    \item Any type of coordination can be used in any hardware via software
\end{itemize}

\textbf{Coordination: Data-parallel}
\begin{itemize}
    \item SIMD, vector processors 
    \item Traditional: Map a function onto a large collection of data 
    \item Side effect free execution
    \item Modern: Data-parallel languages do not enforce this structure 
    \item SPMD model used in CUDA, OpenCL, ISPC instead
\end{itemize}

\textbf{Coordination: Message passing}
\begin{itemize}
    \item Tasks operate within their own private address space and communicate by explicitly sending /\ receiving messages 
    \item E.g. MPI, GO 
    \item Hardware does not implement system wide loads and stores, can connect commodity systems toegther to form large parallel machines 
    \item Many many computers, not a very big one
    \item Compatible with distributed memory systems 
\end{itemize}

\textbf{Coordination and hardware}
\begin{itemize}
    \item Shared memory: UMA, NUMA. Copies of messages and sent /\ received from library buffers
    \item Message passing: distributed systems, clusters, supercomputers
    \item Any abstraction can be implemented with any hardware but it will be more costly
    \item Shared address space on incompatible hardware
    \begin{itemize}
        \item Write: Send message to all cores to invalidate value 
        \item Read: page fault handler issues appropriate network requests
    \end{itemize}
\end{itemize}

\includegraphics*[width=7cm]{coordination_model}



\includegraphics*[width=7cm]{foster}
\textbf{Foster's Design methodology}
\begin{enumerate}
    \item Partitioning 
    \begin{itemize}
        \item Divide computation and data into independent pieces to discover maximum parallelism 
        \item Two approaches: 
        \begin{enumerate}
            \item Domain decomposition: divide data into smaller, equal pieces. Associate computation with data.
            \item E.g. 24 tasks with 3 grids each $\rightarrow$ 6 tasks with 12 grids each 
            \item Functional decomposition: Divide computation into piece. Associate data with computation.
            \item E.g. Climate model $\rightarrow$ Atmospheric model, hydrology model ...
        \end{enumerate}
        \item Rule of thumb:
        \begin{itemize}
            \item 10x more primitive tasks than cores in target computer 
            \item Minimize redundant computations and redundant data storage 
            \item Primitive data should be of roughly the same size 
            \item Number of tasks an increasing function of problem size
        \end{itemize}
    \end{itemize}
    \item Communication (coordination)
    \begin{itemize}
        \item Dependencies between tasks necessitates communication 
        \item Overlap computation and communication such that when some tasks are communicating, others are computing (improve utilisation)
        \item \textbf{Local Communication}
        \begin{itemize}
            \item Tasks needs data from a small number of other tasks (neighbors)
            \item Use channel 
            \item E.g. 2-D finite state computation (requires 5 points to compute next state)
        \end{itemize}
        \item \textbf{Global Communication}
        \begin{itemize}
            \item Significant number of tasks contribute to perform a computation
            \item Do not create channels early on in the execution
            \item E.g. Unoptimised sum N numbers
            \begin{itemize}
                \item Does not distribute computation and communication - \textbf{centralised} 
                \item Does not allow overlap of computation and communication - \textbf{Sequential}
            \end{itemize}
        \end{itemize}
        \includegraphics*[width=7cm]{unoptimised_sum}
        \item Rule of thumb:
        \begin{itemize}
            \item Communication operation balanced among tasks 
            \item Each task communicates with only a small group of neighbors 
            \item Tasks can communicate in parallel 
            \item Overlap computation with communication 
        \end{itemize}
    \end{itemize}
    \item Agglomeration
    \begin{itemize}
        \item Combine tasks into larger tasks s.t \textbf{tasks $\ge$ cores}
        \item Goals:
        \begin{itemize}
            \item Improve performance by reducing cost of task creation and communication 
            \item Maintain scalability of program 
            \item Simplify programming 
        \end{itemize}
        \item E.g. Granular: One task per grid 
        \begin{itemize}
            \item 8*8=64 tasks 
            \item 64 * 4 (neighbors) * 2(send/\ receive)=512 data transfers 
        \end{itemize}
        \item Coarse: 16 grid per task 
        \begin{itemize}
            \item 2*2=4 tasks 
            \item 4*4*2=32 data transfers 
            \item larger messages
        \end{itemize}
        \item Rule of thumb:
        \begin{itemize}
            \item Increases locality of parallel programmes (more neighbors read)
            \item Number of tasks increases with problem size 
            \item Number of tasks suitable for likely target increases (10xnumCores)
            \item Trade-off between agglomeration and code modification should be resonable (man hour)
        \end{itemize}
    \end{itemize}
    \includegraphics*[width=7cm]{agglomorate.png}
    \item Mapping
    \begin{itemize}
        \item Assignment of tasks to execution units 
        \item Goals:
        \begin{itemize}
            \item Maximise processor utilisation: place tasks of different cores
            \item Minimise inter-process communication:Place tasks that communicate often on the same core to increase locality 
        \end{itemize}
        \item Can be performed by user (distributed memory systems) or OS (centralised multiprocessor) 
        \item Rule of thumb:
        \begin{itemize}
            \item Finding optimal mapping is NP hard in general (set cover) 
            \item Consider designs based on one task per core and multiple tasks per core 
            \item Evaluate static and dynamic task allocation 
            \begin{itemize}
                \item Dynamic: allocator should not be performance bottleneck 
                \item Static: task:core $\ge$ 10:1
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \includegraphics*[width=7cm]{mapping} 
    (same amount of work on each processing unit and high locality)
\end{enumerate}

\textbf{Automatic Parallelization}
\begin{itemize}
    \item Compilers perform decomposition and scheduling 
    \item Drawbacks:
    \begin{itemize}
        \item Dependence analysis is difficult for pointer-based computation or indirect addressing 
        \item Execution time of function calls or loops with unknown bounds is difficult to predict at compile time
    \end{itemize}
\end{itemize}

\textbf{Functional programming languages}
\begin{itemize}
    \item Describe the computations of a program as the evaluation of mathematical functions without side effects 
    \item Advantage: New language constructs are not necessary to handle a parallel execution 
    \item Challenge: Extract the parallelism at the right level of recursion
\end{itemize}


\textbf{Parallel Programming Patterns}
\begin{itemize}
    \item Pattens are not mutually exclusive, use the best match 
\end{itemize}

\textbf{Fork Join}
\begin{itemize}
    \item Children run in parallel but are independent 
    \item Children execute the same or different program 
    \item Children join the parent at different points 
    \item Good for loop parallelism (independent for loops)
    \item \textbf{Implementation: } Processes, threads etc
\end{itemize}

\includegraphics*[width=7cm]{db_fj}

\textbf{Parbegin - Parend}
\begin{itemize}
    \item most relaxed, code is structured into sequential segments and parallel segments
    \item Programmer specifies a sequence of statements to be executed in parallel 
    \item A set of threads is created and the statement of the construct are assigned to these threads 
    \item All the forks are done at the same time and all the joins are done at the same time 
    \item Statements after parbegin and parend are only executed after all threads joins (barrier)
    \item \textbf{Implementation: } OpenMP or compiler directives 
    \item E.g Matrix multiplication using openMD
\end{itemize}

\textbf{SIMD (not the Architecture)}
\begin{itemize}
    \item Single instructions are executed synchronously by diferent threads on different data 
    \item Similar to parbegin-parend but al threads execute the same instruction at the same time (synchronous)
    \item Parallel but synchronous
    \item \textbf{Implementation: } AVX /\ SSE instruction on intel processor
\end{itemize}
\includegraphics*[width=7cm]{simd}

\textbf{SPMD}
\begin{itemize}
    \item Same program executed on different cores but operate o different data  
    \item Different threads might execute on different instructions of the same program due to control flow (ifs) and speed of cores
    \item Similar to parbegin-parend but there is no implicit synchronization (lack of barrier)
    \item E.g. programs on GPGPU
\end{itemize}

\textbf{Master-Worker}
\begin{itemize}
    \item Single program controls the execution of the program 
    \item Master executes main function, assigns work to worker threads
    \item Initialisation, output and Coordination is done by master
    \item Worker waits for instruction 
    \item \textbf{Benefit:} Good for simple and homogeneous worker threads and a master thread to organize them
\end{itemize}

\includegraphics*[width=5cm]{main.png}
\includegraphics*[width=5cm]{master}
\includegraphics*[width=5cm]{worker}


\textbf{Task Pool}
\begin{itemize}
    \item Common data structure for threads to retrieve tasks 
    \item Number of threads is fixed 
    \item Threads are statically created by main 
    \item Work is not pre-allocated. Instead  worker retrieves new tasks from pool 
    \item Thread can generate new tasks to put in pool and coordination is not done by master (difference from master-worker)
    \item May run into producer consumer issues when accessing the pool 
    \item Execution is completed when the pool is empty AND each thread has terminated the processing of its last task 
    \item \textbf{Benefits:} 
    \begin{enumerate}
        \item Adaptive can generate tasks dynamically, good for irregular applications
        \item Overhead for thread creation is independent from execution 
    \end{enumerate}
    \item \textbf{Disadvanatges}
    \begin{enumerate}
        \item For fine grained tasks, the overhead of retrieving and insertion becomes significant
    \end{enumerate}
\end{itemize}
\includegraphics*[width=7cm]{java_pool}

\textbf{Producer Consumer}
\includegraphics*[width=7cm]{producer_consumer}
\begin{itemize}
    \item Producer produces data which are used as input by consumer threads 
    \item Synchronisation is needed to ensure correct coodination between producer and consumer threads 
\end{itemize}
\includegraphics*[width=7cm]{producer_consumer_code}


\textbf{Pipelining}
\begin{itemize}
    \item Data is aprtitioned into a stream that flows throuh pipeline stages synchronuously 
    \item Each stage (threads) can be processed in parallel (functional parallel stream)
\end{itemize} 
\includegraphics*[width=7cm]{pipeline_pattern}

\subsection*{L6: Data parallel models (GPGPU)}

\textbf{Shader GPU}
\begin{itemize}
    \item Hard to transfer data between GPU and CPU
    \item No scatter: threads cannot write to arbitrary or multiple mem locations
    \item No communication between fragments 
    \item Coarse thread synchronisation
    \item Example of data parallelism: fast processors for performing the same computation on large collection of data
\end{itemize}

\textbf{FLOPs performance on GPGPU}
\begin{itemize}
    \item Best performance with single precision FLOPs
    \item 2 processors need to work to perform double precision
\end{itemize}

\textbf{GPU Architecture}
\includegraphics*[width=7cm]{gpu_archi}
\begin{itemize}
    \item Multiple Streaming Multiprocessors (SMs) - Memory, cache, connecting interface (PCI)
    \item SM consists of multiple compute cores 
    \begin{itemize}
        \item Memories(register, L1 cache, shared memory)
        \item Logic for thread and instruction management
    \end{itemize}
\end{itemize}


\textbf{CUDA programming model}
\begin{itemize}
    \item Compute Unifified Device Architecture
    \item Simple extension to standard C 
    \item Mature software stack (high-level to low level)
    \item User launches batches fo threads on the GPUFully general load /\ store memory model (CRCW)
    \item Scales with non-NVIDIA GPUs too
    \item Transparently scales to hundreds of cores and thousands of parallel threads 
    \item Programmer focus on parallel algorithms
    \item Enable heterogeneous systems (CPU + GPU)
\end{itemize}


% BUG
\textbf{CUDA layers}
\includegraphics*[width=7cm]{cuda-layers}

\textbf{CUDA kernels and threads}
\begin{itemize}
    \item Device=GPU
    \item Host=CPU
    \item Kernel=function that runs on the device
    \item Parallel portions execute on decice as kernels, and multiple are allowed in CUDA hardware 
    \item CUDA threads are extermely light wait with minimal creation overhead and instant context switches 
    \item The key is to divide work to thousands of threads
\end{itemize}
 
\textbf{Arrays of parallel threads}
\begin{itemize}
    \item A CUDA kernel is executed by an array of threads 
    \item All threads run the same code (SPMD)
    \item Each thread has an ID that is used to compute memory addresses and make control decisions 
\end{itemize}

\textbf{Thread cooperation}
\begin{itemize}
    \item Threads in the array need not be completely independent 
    \item Shares results to save computation 
    \item Share memory accesses which reduces bandwidth 
    \item \textbf{Scalable Cooperation}
    \begin{itemize}
        \item Divide monolithic thread awray into multiple blocks
        \item In a block: shared memory, atomic operations and barrier synchronisation 
        \item Threads in different blocks cannot cooperate 
    \end{itemize}
    \item Enables programs to transparently scale to any number of proceses 
\end{itemize}


 \textbf{Thread Execution Mapping to Architecture}
 \begin{itemize}
    \item SIMT execution model
    \item Multiprocessors, creates, manages, schedules and execute threads in SIMT Warps (32)
    \item Threads in a warp starts at the same program address 
    \item Threads have individual programme counter and state 
    \item A block is always split into warps in the same way 
    \item Having divergent control flow will cause the programme to stall 
 \end{itemize}

 \textbf{CUDA Memory model}
 \includegraphics*[width=7cm]{cuda_mem_model.png}
  \begin{itemize}
     \item Kernels are launched in grids 
     \item A block executes on one SM (streaming multiprocessor)
     \item A block cannot be migrated, but several blocks can reside in one SM 
     \item Register file and shared memory are partitioned among all resident thread blocks 
  \end{itemize}

  \textbf{Cuda memory space}
  \begin{itemize}
    \item Data must be explicitly transferred from CPU to device
    \item Shared memory is the cache, and is therefore not cached 
    \item Global, local memory are cached and needs to be warmed up 
    \item Constant memory is useful for uniformly-accessed read-only data 
    \item Spatial data is useful for coherent random-access read-only data (cached too)
  \end{itemize}

\textbf{Coalesced access}
\includegraphics*[width=7cm]{cuda_mem_model.png}
\begin{itemize}
    \item Simultaneous access to global memory by threads in a warp is coalesced to transactions of 32 bytes 
    \item Reduce disk I/Os
\end{itemize}

\textbf{Shared Memory}
\begin{itemize}
    \item Higher bandwidth and lowr latency than local or gloobal 
    \item Divided into equally-sized banks 
    \item Addresses from different banks can be accessed simultaneously
    \item Bank conflict: two threads access two different addresses in the same memory bank -- has to be serialised 
    \item Bank broadcast: (threads accessing the same address in a bank) one reading thread broadcast the result to the conflicting threads so they all get the info
\end{itemize}

\textbf{Strided access}
\includegraphics*[width=7cm]{strided access.png}
\begin{itemize}
    \item Threads within a warp access memory with a stride size of x 
    \item This increases the number of bank conflicts by x times! 
    \item Half of the elements in the transactions are not used and represent wasted bandwidth
\end{itemize}

\textbf{Optimisation in Cuda - goals}
\begin{enumerate}
    \item Maximum memory band width by coalescing memory access
    \item Maximise parallel execution by maximising data parallelism and increase hardware utilisation (SIMD!)
    \item Maximum instruction throughput by avoiding different execution paths within the same warp
\end{enumerate}

\textbf{Memory Optimizations}
\begin{itemize}
    \item Minimize data transfer between host and device
    \item Ensure global memory are coalesced whenever possible
    \item Minimize global memory accesses by using shared memory 
    \item Minimize bank conflicts in shared memory accesses (e.g. adding padding words between every 32 wordss)
\end{itemize}

\textbf{Data transfer between host and device}
\begin{itemize}
    \item Peak bandwidth between device and GPU is higher than between host and device 
    \item Hence data transfer between host and device should be minimized 
    \item E.g. running kernel on GPU without any performance benefits over CPU 
    \item Batch small transfers into one larger transfer 
    \item Use paged-locked or pinned memory transfer (not cached) -- eliminates a step in memory transfer 
    \item Page pinned: locked in RAM, cannot be moved to Disk. Both CPU and GPU can access them. Overuse can cause performance issues as it cannot be swapped out of RAM.
\end{itemize}

 
\textbf{Concurrent data transfers and executions}
\includegraphics*[width=7cm]{cuda_async.png}
\begin{itemize}
    \item Overlap asynchronous transfers with computation 
    \item $cudaMemcpyAsync()$ instead of $cudaMemcpy()$, and do CPU computation while data transfers 
    \item Use different streams to acheive concurrent copy and execute
\end{itemize}


\textbf{Execution Configuration}
\begin{itemize}
    \item Improve occupancy
    \begin{itemize}
        \item Number of warps should \> number of processors 
        \item So every processor has 1 warp to execute 
        \item High occupancy hides memory latency and when a block synchronises 
    \end{itemize}
    \item Threads per block should be multiples of warp size 
    \begin{itemize}
        \item If one warp blocks, the other can execute. Better coalesced access
        \item Use smaller thread blocks to reduce chances of bank conflict
    \end{itemize}
    \item Limitation on block size 
    \begin{itemize}
        \item Limited by registers and shared resource 
        \item The kernel prevents launch if the block allocates more thread resources than available
        \item This ensures that at least one block can execute 
    \end{itemize}
    \item Multiple contexts 
    \begin{itemize}
        \item If multiple CUDA apps access the same GPU concurrently, there are likely multiple contexts
    \end{itemize}
\end{itemize}

\textbf{Maximise instruction output}
\begin{itemize}
    \item Use single precision floats where possible 
    \item Replace integer division and modulo operations with bitwise operations 
    \item Use signed loop counters 
\end{itemize}

\textbf{Control Flow}
\begin{itemize}
    \item Reduce divergent warps caused by control flow instructions 
    \item Reduce the number of instructions where possible
\end{itemize}

\subsection*{L9: Parallel Programming Models - II}
\textbf{Data distribution for 1D array} 
\includegraphics*[width=7cm]{1d_data_distribution.png}
\begin{itemize}
    \item \textbf{Block wise distribution}
    \begin{itemize}
        \item Preserves locality of data 
        \item Difficult to apply if hunk size is unknown
    \end{itemize}
    \item \textbf{Cyclic data distribution}
    \begin{itemize}
        \item Chunks are of more equal size 
    \end{itemize}
\end{itemize}

\textbf{Data distribution for 2D arrays}
\begin{itemize}
    \item Combination of blockwise and cyclic distribution in one or both dimensions
    \item One-dimension distribution: By column dimension
    \includegraphics*[width=7cm]{col.png}
    \item One-dimension distribution: Block-cyclic 
    \includegraphics*[width=7cm]{block_cyc.png}
    \begin{itemize}
        \item Form blocks of size b, then perform cyclic round robin allocation
        \item Closer to even distribution 
        \item Set block size to task size to improve locality
        \item Some PUs might have more data 
    \end{itemize}
\end{itemize}

\textbf{2D Checkerboard}
\includegraphics*[width=7cm]{checkerboard.png}
\begin{itemize}
    \item Blockwise: elements split among both dimensions 
    \item Cyclic: based on procesor mesh. The communication delay between p1 to p4 will be longer
    \item Block-cyclic: elements split into b1 x b2 size blocks then cyclic assignment to processors
\end{itemize}
 
\textbf{Example: Heat Transfer Simulation}
\begin{itemize}
    \item N * N metal plate with p processors 
    \item P less than N 
    \item Idea 1: Include border 
    \includegraphics*[width=7cm]{heat_plate1.png}
    \begin{itemize}
        \item Improves locality 
        \item But more shared memory, more communication
    \end{itemize}
    \item Idea 2: Checkerboard, row-wise 
    \includegraphics*[width=7cm]{heat_plate2.png}
    \begin{itemize}
        \item Minimizes communication: only talks to the 4 neighbours
        \item Connection is only 1 step away 
    \end{itemize}
    \item Idea 3: Row-wise Block Cyclic 
    \includegraphics*[width=7cm]{heat_plate3.png}
    \begin{itemize}
        \item Low granularity, poor locality: task size is huge, and each block may not fit into the cache line 
        \item Checker board is better as task size can be defined to fit into cache line
    \end{itemize}
\end{itemize}

\textbf{Information Exchange}
\begin{itemize}
    \item \textbf{Shared Address - Shared memory}
    \begin{itemize}
        \item Assumes global memory access 
        \item Need synchronisation 
    \end{itemize}
    \item \textbf{Distributed Space - Communication Operation}
    \begin{itemize}
        \item Assumes disjoint memory model
    \end{itemize}
\end{itemize}

\textbf{Message Exchange Protocol}
\includegraphics*[width=7cm]{buffered_non_buffered.png}
\begin{itemize}
    \item \textbf{Blocking Send}
    \begin{itemize}
        \item Send operation blocks till input buffer is safe to be reused 
    \end{itemize}
    \item \textbf{Non-buffered Blocking Send}
    \begin{itemize}
        \item Operation blocks until the matching receive is performed 
        \item Considerable idling timing due to mismatch between send and receive 
        \item Possible to deadlock 
        \item Can happen with buffered blocking send once the buffer is full
    \end{itemize}
    \item \textbf{Buffered Blocking Send}
    \begin{itemize}
        \item Reduces idling overhead but increases copying overhead
        \item Sender returns after data is copied into buffer 
    \end{itemize}
    \item Deadlock! 
    \includegraphics*[width=7cm]{deadlock.png}

    \item \textbf{Non blocking send}
    \begin{itemize}
        \item Returns before it is semantically safe 
        \item Allows programmers to overlap operations and hide communication overhead
    \end{itemize}
\end{itemize}

\textbf{Sync and async}
\includegraphics*[width=7cm]{async_sync.png}
\begin{itemize}
    \item \textbf{Sync}
    \includegraphics*[width=7cm]{sync.png}
    \begin{itemize}
        \item Send completes after matching receive and source data sent 
        \item Receive completes after matching send and data transfer completes 
    \end{itemize}
    \item \textbf{Async}
    \includegraphics*[width=7cm]{async.png}
    \begin{itemize}
        \item Send completes after input buffer maybe reused
    \end{itemize}
\end{itemize}



\textbf{Send and Receive MPI}
\includegraphics*[width=7cm]{mpi_block_nonblock.png}
\includegraphics*[width=7cm]{mpi_buffer.png}

\textbf{Logical Ring}
\includegraphics*[width=7cm]{deadlock_free_logical_ring.png}
\begin{itemize}
    \item Trace the logic from 0 to 3
    \item There will be no deadlocks since there are implicit barriers due to the blocking calls
\end{itemize}

\textbf{Process Group}
\begin{itemize}
    \item Process group: ordered set of processes, each with unique rank 
    \item A process can be in multiple groups
\end{itemize}

\textbf{Communicators}
\begin{itemize}
    \item Communication domain for a group of processes 
    \item \textbf{Intra communicators}
    \begin{itemize}
        \item Support the execution of arbitrary collective communication on a single group
        \item E.g. MPI\_COMM\_WORLD
    \end{itemize}
    \item \textbf{Inter communicators}
    \begin{itemize}
        \item Supports point to point communication opeartions between process groups 
    \end{itemize}
\end{itemize}

\textbf{Process Group Ops}
\includegraphics*[width=7cm]{process_grp_op.png}

\textbf{Communicator Ops}
\includegraphics*[width=7cm]{communication.png}

\textbf{Virtual Top}
\includegraphics*[width=7cm]{virtual_top_op.png}

\textbf{Virtual Topology Operations}
\includegraphics*[width=7cm]{virtual_top_op.png}


\textbf{Scatter, gather}
\includegraphics*[width=7cm]{scatter_gather.png}

\textbf{Single Broadcast}
\includegraphics*[width=7cm]{single_broadcast.png}

\textbf{Multi Broadcast}
\includegraphics*[width=7cm]{multi_broadcst.png}
\begin{itemize}
    \item Each processor sends the same data bloc to every other processor
    \item Data blocks are collected in rank order
\end{itemize}

\textbf{Single Accumulate}
\includegraphics*[width=7cm]{single_acc.png}
\begin{itemize}
    \item Reduction operation is applied by element to the data blocks 
    \item Result is collected at root
\end{itemize}

\textbf{Multi Accumulate}
\includegraphics*[width=7cm]{multi_acc.png}
\begin{itemize}
    \item Each processor provides for every other processor a potentially different data block 
    \item Data block for the same receiver are combined with a given reduction operation
\end{itemize}

\textbf{Total exchange}
\begin{itemize}
    \item Each process executes a scatter operation (sends different message to each processor)
    \item Stepwise to multi-broadcast, that sends the same mmessage to all processors
\end{itemize}

\textbf{Stepwise and Duality}
\includegraphics*[width=7cm]{collective_comm.png}
\begin{itemize}
    \item Duality: The same spanning tree can be used for both operations 
    \includegraphics*[width=7cm]{dualit.png}
    \item Stepwise: Operations that are stepwise specializations 
    \item E.g.: Total exchange and multi-broadcast
\end{itemize}

\section*{Performance and Scalability of Parallel Programs}
\subsection*{L5: Performance of parallel systems}
\textbf{Two Views}
\begin{itemize}
    \item Response Time (user): duration of a program is reduced  (start - end time)
    \item Throughput (computer manager): more work to be done in the same time (jobs per second)
\end{itemize}


\textbf{Performance Factors}
\begin{enumerate}
    \item Programming Model: how well the programmer can code (like good language, API etc)
    \item Computational Mode: How well the given program runs in the given architecture
    \item Architectural Model: interconnnection network, memory organization, execution mode,sync or async processing
\end{enumerate}

\textbf{Response time in sequential programs}
\begin{itemize}
    \item Wall-clock time 
    \item Comprise of 
    \begin{itemize}
        \item User CPU time: time CPU spends executing program 
        \begin{itemize}
            \item Know that read and write cycles take different time 
        \end{itemize}
        \item System CPU time: time CPU spends on system instructions. Depends on OS.
        \item Waiting time: IO waiting time and execution of ther programs due to time sharing. Depends on the load of the system.
    \end{itemize}
\end{itemize}

\textbf{User CPU time}
\begin{itemize}
    \item \textbf{$Time_{user}(A)=N_{cycle} * Time_{cycle}$}
    \item $N_{cycle}$
    \begin{itemize}
        \item Depends on translation of program statements by the compiler into instructions
        \item For a program with n instructions:
        \item $N_{cycle} = \sum_{i=1}^{n} CPI_i * n_i(A)$
        \item $n_i(A)$: number of times instruction i is executed in program A
        \item Depends on architecture of the computer system  and compiler
        \item $CPI_i$: average number of cycles per instruction i
    \end{itemize}  
    \item \textbf{Refinement with memory access}
    \item $Time_{cycle}$: Execution time for each instruction, $\frac{1}{clock rate}$
    \item  $Time_{user}(A)=(N_{instr}(A) + N_{mm\_cycle}(A)) * Time_{cycle}$
    \item $N_{mm\_cycle}=N_{read\_cycle} + N_{write\_cycle}$
    \item $N_{read\_wite\_cycle}=N_{read\_ op}*R_{miss}*N_{miss\_cycles}$
    \item \textbf{Miss rates}
    \includegraphics*[width=6cm]{miss_rate}
\end{itemize}


\textbf{Memory access flow}
\includegraphics*[width=7cm]{read_write}

\textbf{Throughput}
\begin{itemize}
    \item \textbf{MIPS} $\frac{N_{instr}}{Time_{user}*10^6}$ OR $\frac{clock_{freq}}{CPI*10^6}$
    \item Only considers the number of instructions 
    \item Can be easily manipulated by making the instructions smaller so it takes more to run the same programme 
    \item \textbf{MFLOPS} $\frac{N_{flops}}{Time_{user}*10^6}$
    \item Does not differentiate between the different types of floating pt ops 
\end{itemize}

\textbf{Misc}
\begin{itemize}
    \item Higher clock freq != shorter execution time, since we do not capture CPI 
\end{itemize}


\textbf{Speed up}
\includegraphics*[width=7cm]{t_p}
\begin{itemize}
    \item \textbf{Cost}: $C_{p}(n)=p*T_{p}(n)$, where $C_{p}$ measures the total work performed by all processors 
    \item A parallel programme is cost optimal if it executes the same total numebr of operations as the fastest sequential program 
    \item \textbf{Speed up}: $S_p(n)=\frac{T_{best\_seq(n)}}{T_p(n)}$
    \item Theoretically $S_p$ $\le$ p
    \item Practically, sublinear can occur when the parallel working task fits within the cache but the seq one cannot 
    \item \textbf{Efficiency}
    \includegraphics*[width=6cm]{efficiency}
    \item $T_*(n)$ refers to the best sequential
    \item In the ideal case $T_p=p$, and $E(p)$ = 1
\end{itemize}

\textbf{Scalability}
\begin{itemize}
    \item How size of problem and size of parallel computer interact 
    \item Problem size small: Parallelism overheads dominates benefits 
    \item Problem size large: working set cannot fit on machine, cannot start
\end{itemize}

\textbf{Amadahl's Law}
\begin{itemize}
    \item Speedup of the parallel execution is limited by the sequential fraction $f= \frac{t_{sequential}}{t_{total}}$
    \item Manufacturers are discouraged from making large parallel computers
    \item Effort diverted to reducing sequential section
\end{itemize}
\includegraphics*[width=7cm]{amdahls.png}

\textbf{Rebuttal to Amdahl's}
\begin{itemize}
    \item $f$ is not always constant 
    \item In a good parallel programme, $\lim_{n\rightarrow \infty}(n)=0$
    \item Hence $S_{p}=p$
\end{itemize}

\textbf{Gufstafson's law}
\includegraphics*[width=7cm]{guslaw.png}
\begin{itemize}
    \item In some programmes, $f$ decreases when the problem size increases (the parallel parts increases more)
    \item Then $S_p=p$ given a large enough problem size
\end{itemize}

\textbf{Perfromance Measure of Communication}
\includegraphics*[width=7cm]{communication.png}


\textbf{Latency of sending a m sized msg}
\includegraphics*[width=7cm, height=6cm]{latency.png}

\textbf{Finding Possible Bottlenecks}
\begin{itemize}
    \item Instruction-rate limit: Add more non-memory instructions and check if execution time increases linearly with math operations count 
    \item Memory bottleneck: remove most non-memory operations, did the time change proportionately?
    \item Locality of data access: change all arrays to access A[0]
    \item Sync overhead: remove all atomic operations or locks (might change control flow, so may not work)
\end{itemize}

\subsection*{L8: performance instrumentation}
\textbf{Terminology}
\begin{itemize}
    \item Latency: Time waiting to be serviced 
    \item Response time: Time for an operation to complete
    \item Throughput: Rate of operation/data performed 
    \item Utilization: Proportion of time where the resource is busy
    \item Saturation: Degree to which a resourc has queued work that it cannot service 
    \item Bottleneck: Service thta limits system performance 
\end{itemize}

\textbf{Resource analysis - system administrator}
\begin{itemize}
    \item Focus on system resource: CPU, memory, disks, network interface, buses, interconnects 
\end{itemize}


\textbf{Workload analysis - app dev}
\begin{itemize}
    \item Examines the worklaod and how the system is responding
    \item Requests, latencies, completion \& error 
\end{itemize}


\textbf{Anti-methodologies}
\begin{itemize}
    \item No deliberate methodology
    \item \textbf{Street light}
    \begin{itemize}
        \item Look for obvious issues that can be found online or by chance 
    \end{itemize}
    \item \textbf{Drunk man}
    \begin{itemize}
        \item Tune things at random until problem goes away
        \item Tune the wrong software
    \end{itemize}
\end{itemize}

\textbf{Problem statement method}
\begin{itemize}
    \item Ask questions abou the problem to better understand it
\end{itemize}

\textbf{USE Method}
\begin{itemize}
    \item Uilization: Busy time 
    \item Saturation: Queue length or time 
    \item Errors: easy to interpret
    \item Start with questions and then find the tools 
\end{itemize}

\textbf{Monitoring}
\begin{itemize}
    \item Records performance statistics overtime to find patterns 
    \item Good for: capacity planning, quantifying growth, showing peak usage 
\end{itemize}


\textbf{Performance analysis in 60 seconds}
\includegraphics*[width=7cm]{perf_anal.png}

\textbf{Tools method}
\begin{itemize}
    \item List available performance tools 
    \item List the useful metrices they provide 
    \item List ways to interpret metrices
\end{itemize}

% TODO: actually finish L8

\section*{New Trends}
\subsection*{L12: Energy efficient computing}


\pagebreak
% Tutorials
\section*{Tutorials}

\subsection*{Tutorial 1}

\textbf{Hybrid memory (Shared + Distributed)}
\begin{itemize}
    \item Hybrid: Each core has its own memory (cache) that needs to communicate to the shared memory to stay updated 
    \item Not hybrid: Everyone just sees one shared memory
\end{itemize}

\textbf{xs-4114 vs i7-7700}

\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Processor Type} & \textbf{Cores} & \textbf{Threads} \\
\hline
Xeon Silver (xs) & 10 & 20 \\ 
\hline
Core i7 (i7) & 4 & 8 \\
\hline

\end{tabular}
\label{table:processor_comparison}
    


\subsection*{Tutorial 2}
\textbf{Task Dependence Graph}
\includegraphics*[width=7cm]{task_dependence_graph.png}
\begin{itemize}
    \item parallel: X and Y are executed in parallel 
    \item parend: all tasks must complete before moving on 
\end{itemize}

\textbf{Average CPI}
\begin{itemize}
    \item Not all instructions are created equal! Having fewer instruction != faster programme if translation of instructions takes more clock cycles
\end{itemize}

\textbf{Calculaing MIPS}
\begin{enumerate}
    \item Calculate time taken for the programme
    \begin{itemize}
        \item $\frac{instructions_i * cycles_i}{clock rate}$
        \item Ghz = $10^9$ Hz
    \end{itemize}
    \item Find the total number of million instructions 
    \begin{itemize}
        \item $\frac{total inst.}{10^6}$
    \end{itemize}
    \item divide 2 by 1
\end{enumerate}

\textbf{Amdahl's vs Gustafson's}
\begin{itemize}
    \item Amdahl's: If problem is fixed sized \textbf{OR} there's a constant sequential fraction with increasing problem size, then the speedup is limited by the sequential fraction
    \item Gustafson's: If the problem size can bve varied \textbf{AND} the sequential fraction does not scale much with problem size, then we can solve larger problems with more speed up
\end{itemize}

\textbf{Flynn's Taxonomy}
\begin{itemize}
    \item MIMD is a superset to SIMD!
\end{itemize}

\textbf{Programming Models}
\begin{itemize}
    \item \textbf{Distributed memory} requires explicitly communication between processess
    \item \textbf{Master-worker}
    \begin{itemize}
        \item Good for relatively simple and homogeneous worker threads and a master thread to organize them
        \item Similar to the idea of SIMD 
    \end{itemize}
    \item \textbf{Task Pool}
    \begin{itemize}
        \item Good for heterogeneous tasks or those that finish at different times 
        \item Good when you are unsure of finish timing
    \end{itemize}
    \item \textbf{Fork-join / Parbegin parend}
    \begin{itemize}
        \item Fork join can be complicated to write (synchronisation, waiting etc)
    \end{itemize}
    \item \textbf{Pipelining}
    \begin{itemize}
        \item Better for task parallelism. Bad for data parallelism where there is only one task 
        \item Best when each stage takes similar time / are even in workload
        \item If parbegin-parend, use OpenMP!
    \end{itemize}
    \item \textbf{Producer Consumer}
    \begin{itemize}
        \item Good when there is a pair of producer / consumer 
        \item Need to have something that is produced and consumed!
        \item If producers/consumers have uneven workload, then we need more producer/consumer to offset the difference
    \end{itemize}
\end{itemize}

\subsection*{Tutorial 3}

\textbf{Relaxed Orders}
\begin{itemize}
    \item \textbf{Data Dependencies (while loops) DOES NOT COUNT}
    \item TSO: Relax W $\rightarrow$ R on the same processorf
    \item PC: Relax W $\rightarrow$ R on the same processor, processes see writes at different times
    \item PSO: Re;ax WR on the same processor, relax WW on the same processor if no data dependency
\end{itemize}


\textbf{Drawing Dependencies}
\includegraphics*[width=7cm]{drawing_dependencies.png}
\begin{itemize}
    \item To determine the final value in a SC execution, draw a line from the last write to the read
\end{itemize}


\textbf{Number of Warps and Blocks}
\begin{itemize}
    \item \#of registers per kernel=\#of registers per thread 
    \item \#of blocks=\#of threads on device/\#of registers per block 
    \item \#of warps=\#of threads per block/warp size
\end{itemize}

\textbf{CUDA mat mul}
\includegraphics*[width=7cm]{cuda_t3_1.png}
\includegraphics*[width=7cm]{cuda_t3_2.png}
\begin{itemize}
    \item 1 thread process 1 element in the output array 
    \item Use managed to let CUDA figure out when to copy the matrices
\end{itemize}


\subsection*{Tutorial 5}

\textbf{Dining Philosophers Vs Logical Ring}
\begin{itemize}
    \item Without the odd even soln, a deadlock can occur since send does not guarantee that the system buffer is large enough
    \item Logical ring takes num message * 2 number of communications to complete 
    \item Dining Philosopher can take many more steps! 
    \item Logical ring looks like a pipeline but also comes with synchronisation issues since its not just unidirectional
\end{itemize} 

\textbf{Writing logical ring using non-blocking send recv}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
int rank, p, size = 8;
int left, right ;
char send_buffer1[8], recv_buffer1[8];
char send_buffer2[8], recv_buffer2[8];
gethostname(send_buffer1, size) //repeat for buffer2

MPI_Comm_rank(MPI_COMM_WORLD, &rank)
MPI_Comm_size(MPI_COMM_WORLD, &p)
left = (rank - 1 + p) % p ;
right = (rank + 1) % p ;
MPI_Request reqs[4]; MPI_Status stats[4];
MPI_ISend (send_buffer1, size, MPI_CHAR, left, TAG_LEFT , MPI_COMM_WORLD, &reqs[0]);
MPI_IRecv (recv_buffer1, size, MPI_CHAR, right, TAG_LEFT , MPI_COMM_WORLD, &reqs[1]);
MPI_ISend (send_buffer2, size, MPI_CHAR, right , TAG_RIGHT, MPI_COMM_WORLD, &reqs[2]);
MPI_IRecv (recv_buffer2, size, MPI_CHAR, left , TAG_RIGHT , MPI_COMM_WORLD, &reqs[3]);
MPI_Waitall(4, reqs, stats);
...
\end{lstlisting}

\textbf{Wimpy and Brawny cores}
\begin{itemize}
    \item Single threaded performance: Brawny 
    \item Multi threaded performance: Wimpy
    \item Always good to use a mix of both!
\end{itemize}


\subsection*{Tutorial 6}
\textbf{Omega network - 8 * 8}
\includegraphics*[width=7cm]{omega2.png}

\textbf{XOR Tag Routing}
\includegraphics*[width=7cm]{xor_routing.png}
% Labs
\section*{Labs}

\subsection*{Lab 2: OpenMP}
\textbf{Work Sharing Constructs}
\begin{itemize}
    \item Get thread id: $omp\_get\_thread\_num()$
    \item Get num threads: $omp\_get\_num\_threads()$
    \item To share work across multiple threads, we need to use work-sharing constructs 
    \includegraphics*[width=7cm]{work_sharing.png}
    \item Static assigns work in a circular manner if $iter\_size$ is larger than the number of threads 
    \item Dynamic will assign based on the given chunk size or iter size but in a random order 
    \begin{itemize}
        \item Pros: Appropriate when the iteration require different computational cots and the iterations are not balanced 
        \item Cons: Scheduling has higher over head than static scheduling because it distributes iterations during run time 
    \end{itemize}
\end{itemize}

\textbf{Nesting Work-Sharing constructs}
\includegraphics*[width=7cm]{nested.png}
\begin{itemize}
    \item Nesting of work sharing constructs is not allowed 
\end{itemize}

\textbf{Sections}
\includegraphics*[width=5cm]{section.png}
\begin{itemize}
    \item Each section will be assigned to any available thread, one ata a time
    \item single section (pragma omp single) will be executed by only one thread, decided at run time 
    \item master section (pragma omp master) will be executed by only the master thread
\end{itemize}

\subsection*{Lab 3: CUDA}
\textbf{Basics}
\begin{itemize}
    \item Complex cores like CPU has low latency 
    \item Many simple cores like GPU has high throughput
    \item one SM runs one thread block and executes multiple warps of threads in parallel
\end{itemize}

\textbf{Synchronisation Constructs}
\begin{itemize}
    \item \textbf{pragma omp barrier}: Synchronizes all threads 
    \item \textbf{pragma omp master}: Only master thread executes
    \item \textbf{pragma omp critical}: Can only be executed by 1 thread at a time 
    \includegraphics*[width=5cm]{critical.png}
    \item \textbf{pragma omp atomic}: Only one thread can execute the atomic statement at a time
\end{itemize}

\textbf{Volta (CC7) Vs Pascal(CC6)}
\includegraphics*[width=7cm]{volta.png}
\includegraphics*[width=7cm]{pascal.png}


\textbf{CUDA Memory Types}
\includegraphics*[width=7cm]{cuda_mem_types.png}
\begin{itemize}
    \item program scope = both host and device
    \item Prefers register \> local \> shared \> global
    \item \textbf{Global memory}
    \begin{itemize}
        \item $cudaError\_t cudaMalloc ( void ** devPtr , size\_t size )$
        \item Visible to all blocks
    \end{itemize}
    \includegraphics*[width=7cm]{unified1.png}
    \item \textbf{Shared memory}
    \begin{itemize}
        \item $ \_\_shared\_\_ $
        \item Only resides in device, hence faster 
        \item Only visible to those in the same thread block
    \end{itemize}
    \item \textbf{Unified memory}
    \begin{itemize}
        \item Defines a common memory addressing space, allowing both CPU and GPU to access it as if it is in their memory space 
        \item $cudaMallocManaged$ and $\_\_managed\_\_$
        \item Page-locked memory (locked in the RAM)! GPU can access directly without CPU intervention.
    \end{itemize}
    \includegraphics*[width=7cm]{unified2.png}
\end{itemize}

\textbf{Synchronisation in CUDA}
\begin{itemize}
    \item CUDA provides synchronising primitives
    \item $atomicAdd(\&counter, 1);$
    \item \textbf{Barrier in CUDA}
    \begin{itemize}
        \item $\_\_syncthreads()$ synchronises threads in the same block untill all of them have reached this point
        \item Threads from other blocks are not synchronised
        \item $volatile$ keyword: hints to the compiler to not optimise load and store operations to prevent stale version of the var from being read
        \item Volatile varaibles may be modified asynchronously by other threads
    \end{itemize}
\end{itemize}

\textbf{Cuda Malloc}
\begin{itemize}
    \item $cudaMalloc(void **pointer, size\_t nbytes)$ is called in host. Since host cannot touch the shared memory, the memory is allocated to global
    \item $cudaMemset(void *pointer, int value, size\_t count);$
    \item $cudaFree(void* pointer)$
\end{itemize}

\textbf{CUDA Example Codes}
\begin{itemize}
    \item \textbf{Adding two arrays}
    \includegraphics*[width=7cm]{cuda_basic.png}
    \includegraphics*[width=7cm]{cuda_basic_allocate.png}
    \includegraphics*[width=7cm]{cuda_basic_kernel.png}
    \item \textbf{Matrix multiplication}
    \includegraphics*[width=7cm]{cuda_matrix_mult_1.png}
    \includegraphics*[width=7cm]{cuda_matrix_mult_2.png}
    \includegraphics*[width=7cm]{cuda_matrix_mult_3.png}
\end{itemize}

\subsection*{Lab 4 and 5: MPI}

\textbf{MPI Blocking Communication}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
    int MPI_Send ( const void * buf , int count , MPI_Datatype datatype ,
    int dest , int tag , MPI_Comm comm )

    int MPI_Recv ( void * buf , int count , MPI_Datatype datatype ,
int source , int tag , MPI_Comm comm , MPI_Status * status )
    \end{lstlisting}

\includegraphics*[width=7cm]{MPI_BLOCKING.png}


\textbf{Non Blocking Communication}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
    int MPI_Isend ( const void * buf , int count , MPI_Datatype datatype ,
    int dest , int tag , MPI_Comm comm , MPI_Request * request )

    int MPI_Irecv ( void * buf , int count , MPI_Datatype datatype ,
    int source , int tag , MPI_Comm comm , MPI_Request * request )

    int MPI_Test ( MPI_Request * request , int * flag , MPI_Status * status )


    int MPI_Wait ( MPI_Request * request , MPI_Status * status )

    \end{lstlisting}
    \begin{itemize}
        \item other variations of test and wait: MPI\_Test , MPI\_Testall , MPI\_Testany , MPI\_Testsome
        MPI\_Wait , MPI\_Waitany , MPI\_Waitsome
        \item MPI does not guarantee fairness, starvation can still happen
    \end{itemize}


\textbf{Collective Communication}

\textbf{Barrier}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
int MPI_Barrier ( MPI_Comm comm )

# Example 
if (rank == master_node_rank) {
    // Master node
    for (int i = 0; i < num_workers * ITERATIONS; i++) {
        MPI_Barrier(MPI_COMM_WORLD);
        MPI_Recv(&number, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        double print_delay = MPI_Wtime() - loop_start_time_s;
        printf("Master node received number %d at time %.5f sec\n", number, print_delay);
    }
} else {
    // Workers
    for (int i = 0; i < num_workers  * ITERATIONS; i++) {
    // Only 1 process will be able to send
    MPI_Barrier(MPI_COMM_WORLD);
    if (rank == (i % num_workers)) {
        // Only this particular worker should send in this iteration
        MPI_Send(&rank, 1, MPI_INT, master_node_rank, 0, MPI_COMM_WORLD);
    }
    // Random sleep to vary the workers
    useconds_t sleepTime = (useconds_t)(((rand() % 5) + 1) * 100);
    usleep(sleepTime);
    }
}
    \end{lstlisting}


\textbf{Data movement Operations}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
/* MPI_Bcast - broadcasts ( sends ) a message from the process with rank
root to all other processes in the group */
int MPI_Bcast ( void * buffer , int count , MPI_Datatype datatype ,
int root , MPI_Comm comm )
--------------------------------------
/* MPI_Scatter - sends data from one process to all processes in
a communicator */
int MPI_Scatter ( const void * sendbuf , int sendcount ,
MPI_Datatype sendtype , void * recvbuf ,
int recvcount , MPI_Datatype recvtype ,
int root , MPI_Comm comm )
--------------------------------------
/* MPI_Gather - gathers data from a group of processes into one root
process */
int MPI_Gather ( const void * sendbuf , int sendcount ,
MPI_Datatype sendtype , void * recvbuf ,
int recvcount , MPI_Datatype recvtype ,
int root , MPI_Comm comm )
--------------------------------------
/* MPI_Allgather - gathers data from a group of processes into every
process of that group */
int MPI_Allgather ( const void * sendbuf , int sendcount ,
MPI_Datatype sendtype , void * recvbuf , int recvcount ,
MPI_Datatype recvtype , MPI_Comm comm )
--------------------------------------
/* MPI_Alltoall - each process in a group performs a
scatter operation , sending a distinct message to all the
processes in the group in order by their rank */
int MPI_Alltoall ( const void * sendbuf , int sendcount ,
MPI_Datatype sendtype , void * recvbuf , int recvcount ,
MPI_Datatype recvtype , MPI_Comm comm )
    \end{lstlisting}

\textbf{Collective Computation}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
--------------------------------------
/* MPI_Reduce - reduces values on all processes within a group ; the
reduction operation must be one of the following :
MPI_MAX maximum | MPI_MIN minimum | MPI_SUM sum | MPI_PROD product |
MPI_LAND logical AND | MPI_BAND bit - wise AND | MPI_LOR logical OR |
MPI_BOR bit - wise OR | MPI_LXOR logical XOR | MPI_BXOR bit - wise XOR |
MPI_MAXLOC max value and location | MPI_MINLOC min value and location
*/
int MPI_Reduce ( const void * sendbuf , void * recvbuf , int count ,
MPI_Datatype datatype , MPI_Op op , int root , MPI_Comm comm )
--------------------------------------
/* MPI_Allreduce - applies a reduction operation and places the
result in all processes in the communicator
( this is equivalent to an MPI_Reduce followed by an MPI_Bcast ) */
int MPI_Allreduce ( const void * sendbuf , void * recvbuf , int count ,
MPI_Datatype datatype , MPI_Op op , MPI_Comm comm )

#Example
source = 0;
sendcount = 1;
recvcount = 1;

// Generate a random value for each process
srand(rank);
int localval = rand() % 10;
printf("Rank %d generated value %d\n", rank, localval);

int sum = 0;
MPI_Allreduce(&localval, &sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
MPI_Finalize();

--------------------------------------
/* MPI_Reduce_scatter - first performs an element - wise reduction on a
vector across all processes in the group , then splits the result
vector into disjoint segments to distribute across the processes
( this is equivalent to an MPI_Reduce followed by an MPI_scatter ) */
int MPI_Reduce_scatter ( const void * sendbuf , void * recvbuf ,
const int recvcounts [] , MPI_Datatype datatype ,
MPI_Op op , MPI_Comm comm )
    \end{lstlisting}

\textbf{Managing Communicators}
\begin{itemize}
    \item A communicator contains a set of process MPI GROUP with an asscoiated context 
\end{itemize}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
--------------------------------------
/* MPI_Comm_group - returns the group associated with a communicator */
int MPI_Comm_group ( MPI_Comm comm , MPI_Group * group )
--------------------------------------
/* MPI_Group_incl - produces a group by reordering an existing group and
taking only listed members */
int MPI_Group_incl ( MPI_Group group , int n , const int ranks [] ,
MPI_Group * newgroup )
--------------------------------------
/* MPI_Comm_create - creates a new communicator with a group of
processes */
int MPI_Comm_create ( MPI_Comm comm , MPI_Group group , MPI_Comm * newcomm )
--------------------------------------
/* MPI_Group_rank - returns the rank of the calling process in the given
group */
int MPI_Group_rank ( MPI_Group group , int * rank )
--------------------------------------
/* MPI_Comm_rank - returns the rank of the calling process in the given
communicator */
int MPI_Comm_rank ( MPI_Comm comm , int * rank )

#Example
ranks1[4]={0,1,2,3}, ranks2[4]={4,5,6,7};
// Variables to hold the group of orig_group and new_group
// What is held in new_group is not necessarily the same as what is 
MPI_Group  orig_group, new_group;
MPI_Comm   new_comm;

MPI_Init(&argc,&argv);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &numtasks);

sendbuf = rank;

/* Extract the original group handle */
MPI_Comm_group(MPI_COMM_WORLD, &orig_group);

/* Divide tasks into two distinct groups based upon rank */
if (rank < NPROCS/2) {
    MPI_Group_incl(orig_group, NPROCS/2, ranks1, &new_group);
} else {
    MPI_Group_incl(orig_group, NPROCS/2, ranks2, &new_group);
}

/* Create new new communicator and then perform collective communications */
MPI_Comm_create(MPI_COMM_WORLD, new_group, &new_comm);
MPI_Allreduce(&sendbuf, &recvbuf, 1, MPI_INT, MPI_SUM, new_comm);

MPI_Group_rank (new_group, &new_rank);
printf("rank= %d newrank= %d recvbuf= %d\n",rank,new_rank,recvbuf);

MPI_Finalize();

        \end{lstlisting}

\textbf{Inter group communication}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
int MPI_Intercomm_create ( MPI_Comm local_comm , int local_leader ,
MPI_Comm peer_comm , int remote_leader , int tag ,
MPI_Comm * newintercomm )
\end{lstlisting}

\begin{itemize}
    \item All inter group construcrtors are grouping and requires the local and remote groups to be disjoint to avoid deadlock 
    \item The two groups communicate through Ring leaders 
    \item MPI Barrier is a one to all operation. The one calling process in a sub group waits for all other processes to enter the barrier before continuing
\end{itemize}

\textbf{Cartesian Virtual Topology}
\begin{lstlisting}[language=C++, breaklines=true, breakatwhitespace=true]
--------------------------------------
/* MPI_Cart_create - makes a new communicator to which Cartesian
topology information has been attached */
int MPI_Cart_create ( MPI_Comm comm_old , int ndims , const int dims [] ,
const int periods [] , int reorder ,
MPI_Comm * comm_cart )
--------------------------------------
/* MPI_Cart_coords - determines process coordinates in the Cartesian
topology , given its rank in the group */
int MPI_Cart_coords ( MPI_Comm comm , int rank , int maxdims , int coords [])
--------------------------------------
/* MPI_Cart_shift - returns the shifted source and destination ranks ,
given a shift direction and amount */
int MPI_Cart_shift ( MPI_Comm comm , int direction , int disp ,
int * rank_source , int * rank_dest )


#Example
#define SIZE 16

#define UP    0
#define DOWN  1
#define LEFT  2
#define RIGHT 3

int main(int argc, char *argv[])
{
int numtasks, rank, source, dest, outbuf, i, tag=1, 
    inbuf[4]={
    MPI_PROC_NULL,
    MPI_PROC_NULL,
    MPI_PROC_NULL,MPI_PROC_NULL,
    }, 
    // Row * Columns
    nbrs[4], dims[2]={4,4}, 
    periods[2]={0,0}, reorder=0, coords[2];

MPI_Request reqs[8];
MPI_Status stats[8];
MPI_Comm cartcomm;

MPI_Init(&argc,&argv);
MPI_Comm_size(MPI_COMM_WORLD, &numtasks);

if (numtasks == SIZE) {
    
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cartcomm);
    MPI_Comm_rank(cartcomm, &rank);
    MPI_Cart_coords(cartcomm, rank, 2, coords);
    MPI_Cart_shift(cartcomm, 0, 1, &nbrs[UP], &nbrs[DOWN]);
    MPI_Cart_shift(cartcomm, 1, 1, &nbrs[LEFT], &nbrs[RIGHT]);

    outbuf = rank;

    // for all 4 directions, send its rank to its neighbours
    // and update the rank information
    for (i=0; i<4; i++)	{
        dest = nbrs[i];
        source = nbrs[i];
        MPI_Isend(&outbuf, 1, MPI_INT, dest, tag, MPI_COMM_WORLD, &reqs[i]);
        MPI_Irecv(&inbuf[i], 1, MPI_INT, source, tag, MPI_COMM_WORLD, &reqs[i+4]);
    }

    MPI_Waitall(8, reqs, stats);
    
    for (i = 0; i < SIZE; i++) {
        MPI_Barrier(MPI_COMM_WORLD);
        if (rank == i) {
            printf("rank= %d coords= %d %d  neighbors(u,d,l,r)= %d %d %d %d\n",
                rank,coords[0],coords[1],nbrs[UP],nbrs[DOWN],nbrs[LEFT], 
                nbrs[RIGHT]);
            printf("rank= %d                 inbuf(u,d,l,r)= %d %d %d %d\n\n",
                rank,inbuf[UP],inbuf[DOWN],inbuf[LEFT],inbuf[RIGHT]);
        }
        // Prevent overflowing the previous barrier
        MPI_Barrier(MPI_COMM_WORLD);
    }
} else {
    printf("Must specify %d processes. Terminating.\n",SIZE);
}

MPI_Finalize();
}
    \end{lstlisting}
    

    \end{multicols}
    


% Things i dont want to be in multi col
\pagebreak

\centering
\includegraphics*[width=14cm]{many_interconnects.png}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{Network \( G \) with \( n \) nodes} & \textbf{Degree \( g(G) \)} & \textbf{Diameter \( \delta(G) \)} & \textbf{Edge-connectivity \( ec(G) \)} & \textbf{Bisection Bandwidth \( B(G) \)} \\
    \hline
    Complete graph & \( n - 1 \) & 1 & \( n - 1 \) & \( (\binom{n}{2})^2 \) \\
    \hline
    Linear array & 2 & \( n - 1 \) & 1 & 1 \\
    \hline
    Ring & 2 & \( \left\lfloor \frac{n}{2} \right\rfloor \) & 2 & 2 \\
    \hline
    \( d \)-dimensional mesh \( (n = r^d) \) & \( 2d \) & \( d(\sqrt[d]{n} - 1) \) & \( d \) & \( n^{\frac{d-1}{d}}\) \\
    \hline
    \( d \)-dimensional torus \( (n = r^d) \) & \( 2d \) & \( d \left\lfloor \frac{\sqrt[d]{n}}{2} \right\rfloor \) & \( 2d \) & \( \ 2n^{\frac{d-1}{d}} \) \\
    \hline
    \( k \)-dimensional hypercube \( (n = 2^k) \) & \( \log n \) & \( \log n \) & \( \log n \) & \( \frac{n}{2} \) \\
    \hline
    \( k \)-dimensional CCC-network \( (n = k2^k \text{ for } k \geq 3) \) & 3 & \( 2k - 1 + \left\lfloor \frac{k}{2} \right\rfloor \) & 3 & \( \frac{n}{2k} \) \\
    \hline
    Complete binary tree \( (n = 2^k - 1) \) & 3 & \( 2 \log \frac{n+1}{2} \) & 1 & 1 \\
    \hline
    \( k \)-ary \( d \)-cube \( (n = k^d) \) & \( 2d \) & \( d \left\lfloor \frac{k}{2} \right\rfloor \) & \( 2d \) & \( 2k^{d-1} \) \\
    \hline
    \end{tabular}
    \caption{Network Characteristics}
    \label{table:network_characteristics}
    \end{table}
    

\end{document}